{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f506feee-98f0-4106-a53a-ccbff09b3ce3",
   "metadata": {},
   "source": [
    "# Jigsaw - Agile Community Rules Classification\n",
    "### https://www.kaggle.com/competitions/jigsaw-agile-community-rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9062a4b-55ec-493c-a31e-f03a3489a374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20520, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>rule</th>\n",
       "      <th>formatted_rule</th>\n",
       "      <th>positive_example_1</th>\n",
       "      <th>negative_example_1</th>\n",
       "      <th>positive_example_2</th>\n",
       "      <th>negative_example_2</th>\n",
       "      <th>test_comment</th>\n",
       "      <th>violates_rule</th>\n",
       "      <th>raw_response</th>\n",
       "      <th>example_comments_used</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Futurology</td>\n",
       "      <td>\"No Personal Attacks or Incivility:Do not enga...</td>\n",
       "      <td>\"No Personal Attacks or Incivility:Do not enga...</td>\n",
       "      <td>Only a complete moron who clearly failed high ...</td>\n",
       "      <td>I respectfully disagree with your timeline for...</td>\n",
       "      <td>You're either willfully ignorant or deliberate...</td>\n",
       "      <td>That's an interesting perspective on quantum c...</td>\n",
       "      <td>Anyone who believes in the technological singu...</td>\n",
       "      <td>\"Yes\"</td>\n",
       "      <td>Formatted Rule: \"No Personal Attacks or Incivi...</td>\n",
       "      <td>\\nExample Reddit Comments from /r/worldnews:\\n...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subreddit                                               rule  \\\n",
       "0  Futurology  \"No Personal Attacks or Incivility:Do not enga...   \n",
       "\n",
       "                                      formatted_rule  \\\n",
       "0  \"No Personal Attacks or Incivility:Do not enga...   \n",
       "\n",
       "                                  positive_example_1  \\\n",
       "0  Only a complete moron who clearly failed high ...   \n",
       "\n",
       "                                  negative_example_1  \\\n",
       "0  I respectfully disagree with your timeline for...   \n",
       "\n",
       "                                  positive_example_2  \\\n",
       "0  You're either willfully ignorant or deliberate...   \n",
       "\n",
       "                                  negative_example_2  \\\n",
       "0  That's an interesting perspective on quantum c...   \n",
       "\n",
       "                                        test_comment violates_rule  \\\n",
       "0  Anyone who believes in the technological singu...         \"Yes\"   \n",
       "\n",
       "                                        raw_response  \\\n",
       "0  Formatted Rule: \"No Personal Attacks or Incivi...   \n",
       "\n",
       "                               example_comments_used  error  \n",
       "0  \\nExample Reddit Comments from /r/worldnews:\\n...    NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "base_path = \"./data/final/\"\n",
    "df_train = pd.read_csv(f\"{base_path}df_train_ds_fgkr_01.csv\")\n",
    "print(df_train.shape)\n",
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23d55a85-519e-49cf-9559-7058bd7c45f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Assuming your dataframe is called 'df'\n",
    "# # Fill NaN values with empty strings first\n",
    "# df[['add1', 'add2', 'add3', 'add4']] = df[['add1', 'add2', 'add3', 'add4']].fillna('')\n",
    "# # Or if you want to replace the rule_text column entirely:\n",
    "# df['rule_text'] = df['rule_text'] + ' ' + df['add1'] + ' ' + df['add2'] + ' ' + df['add3'] + ' ' + df['add4']\n",
    "# # Clean up extra spaces\n",
    "# df['rule_text'] = df['rule_text'].str.strip().str.replace(r'\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f280b2d-293e-4338-8935-3d9d3312e75e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 367 CSV files:\n",
      "  - formatted_rules_batch_238.csv\n",
      "  - formatted_rules_batch_11.csv\n",
      "  - formatted_rules_batch_60.csv\n",
      "  - formatted_rules_batch_251.csv\n",
      "  - formatted_rules_batch_114.csv\n",
      "  - formatted_rules_batch_76.csv\n",
      "  - formatted_rules_batch_344.csv\n",
      "  - formatted_rules_batch_358.csv\n",
      "  - formatted_rules_batch_64.csv\n",
      "  - formatted_rules_batch_324.csv\n",
      "  - formatted_rules_batch_103.csv\n",
      "  - formatted_rules_batch_293.csv\n",
      "  - formatted_rules_batch_288.csv\n",
      "  - formatted_rules_batch_121.csv\n",
      "  - formatted_rules_batch_201.csv\n",
      "  - formatted_rules_batch_199.csv\n",
      "  - formatted_rules_batch_271.csv\n",
      "  - formatted_rules_batch_27.csv\n",
      "  - formatted_rules_batch_342.csv\n",
      "  - formatted_rules_batch_57.csv\n",
      "  - formatted_rules_batch_16.csv\n",
      "  - formatted_rules_batch_25.csv\n",
      "  - formatted_rules_batch_193.csv\n",
      "  - formatted_rules_batch_6.csv\n",
      "  - formatted_rules_batch_209.csv\n",
      "  - formatted_rules_batch_341.csv\n",
      "  - formatted_rules_batch_216.csv\n",
      "  - formatted_rules_batch_296.csv\n",
      "  - formatted_rules_batch_30.csv\n",
      "  - formatted_rules_batch_228.csv\n",
      "  - formatted_rules_batch_4.csv\n",
      "  - formatted_rules_batch_109.csv\n",
      "  - formatted_rules_batch_310.csv\n",
      "  - formatted_rules_batch_336.csv\n",
      "  - formatted_rules_batch_46.csv\n",
      "  - formatted_rules_batch_333.csv\n",
      "  - formatted_rules_batch_367.csv\n",
      "  - formatted_rules_batch_282.csv\n",
      "  - formatted_rules_batch_285.csv\n",
      "  - formatted_rules_batch_171.csv\n",
      "  - formatted_rules_batch_8.csv\n",
      "  - formatted_rules_batch_258.csv\n",
      "  - formatted_rules_batch_190.csv\n",
      "  - formatted_rules_batch_61.csv\n",
      "  - formatted_rules_batch_143.csv\n",
      "  - formatted_rules_batch_306.csv\n",
      "  - formatted_rules_batch_197.csv\n",
      "  - formatted_rules_batch_340.csv\n",
      "  - formatted_rules_batch_139.csv\n",
      "  - formatted_rules_batch_141.csv\n",
      "  - formatted_rules_batch_96.csv\n",
      "  - formatted_rules_batch_302.csv\n",
      "  - formatted_rules_batch_318.csv\n",
      "  - formatted_rules_batch_331.csv\n",
      "  - formatted_rules_batch_118.csv\n",
      "  - formatted_rules_batch_206.csv\n",
      "  - formatted_rules_batch_59.csv\n",
      "  - formatted_rules_batch_236.csv\n",
      "  - formatted_rules_batch_126.csv\n",
      "  - formatted_rules_batch_354.csv\n",
      "  - formatted_rules_batch_273.csv\n",
      "  - formatted_rules_batch_113.csv\n",
      "  - formatted_rules_batch_294.csv\n",
      "  - formatted_rules_batch_249.csv\n",
      "  - formatted_rules_batch_166.csv\n",
      "  - formatted_rules_batch_309.csv\n",
      "  - formatted_rules_batch_194.csv\n",
      "  - formatted_rules_batch_95.csv\n",
      "  - formatted_rules_batch_180.csv\n",
      "  - formatted_rules_batch_207.csv\n",
      "  - formatted_rules_batch_174.csv\n",
      "  - formatted_rules_batch_359.csv\n",
      "  - formatted_rules_batch_133.csv\n",
      "  - formatted_rules_batch_215.csv\n",
      "  - formatted_rules_batch_158.csv\n",
      "  - formatted_rules_batch_239.csv\n",
      "  - formatted_rules_batch_350.csv\n",
      "  - formatted_rules_batch_20.csv\n",
      "  - formatted_rules_batch_17.csv\n",
      "  - formatted_rules_batch_188.csv\n",
      "  - formatted_rules_batch_167.csv\n",
      "  - formatted_rules_batch_307.csv\n",
      "  - formatted_rules_batch_67.csv\n",
      "  - formatted_rules_batch_138.csv\n",
      "  - formatted_rules_batch_135.csv\n",
      "  - formatted_rules_batch_364.csv\n",
      "  - formatted_rules_batch_173.csv\n",
      "  - formatted_rules_batch_134.csv\n",
      "  - formatted_rules_batch_326.csv\n",
      "  - formatted_rules_batch_357.csv\n",
      "  - formatted_rules_batch_366.csv\n",
      "  - formatted_rules_batch_182.csv\n",
      "  - formatted_rules_batch_279.csv\n",
      "  - formatted_rules_batch_33.csv\n",
      "  - formatted_rules_batch_298.csv\n",
      "  - formatted_rules_batch_22.csv\n",
      "  - formatted_rules_batch_244.csv\n",
      "  - formatted_rules_batch_311.csv\n",
      "  - formatted_rules_batch_51.csv\n",
      "  - formatted_rules_batch_77.csv\n",
      "  - formatted_rules_batch_142.csv\n",
      "  - formatted_rules_batch_144.csv\n",
      "  - formatted_rules_batch_84.csv\n",
      "  - formatted_rules_batch_223.csv\n",
      "  - formatted_rules_batch_15.csv\n",
      "  - formatted_rules_batch_78.csv\n",
      "  - formatted_rules_batch_316.csv\n",
      "  - formatted_rules_batch_178.csv\n",
      "  - formatted_rules_batch_54.csv\n",
      "  - formatted_rules_batch_202.csv\n",
      "  - formatted_rules_batch_90.csv\n",
      "  - formatted_rules_batch_186.csv\n",
      "  - formatted_rules_batch_72.csv\n",
      "  - formatted_rules_batch_243.csv\n",
      "  - formatted_rules_batch_2.csv\n",
      "  - formatted_rules_batch_91.csv\n",
      "  - formatted_rules_batch_9.csv\n",
      "  - formatted_rules_batch_356.csv\n",
      "  - formatted_rules_batch_203.csv\n",
      "  - formatted_rules_batch_230.csv\n",
      "  - formatted_rules_batch_58.csv\n",
      "  - formatted_rules_batch_119.csv\n",
      "  - formatted_rules_batch_208.csv\n",
      "  - formatted_rules_batch_317.csv\n",
      "  - formatted_rules_batch_127.csv\n",
      "  - formatted_rules_batch_98.csv\n",
      "  - formatted_rules_batch_227.csv\n",
      "  - formatted_rules_batch_140.csv\n",
      "  - formatted_rules_batch_86.csv\n",
      "  - formatted_rules_batch_362.csv\n",
      "  - formatted_rules_batch_233.csv\n",
      "  - formatted_rules_batch_330.csv\n",
      "  - formatted_rules_batch_260.csv\n",
      "  - formatted_rules_batch_88.csv\n",
      "  - formatted_rules_batch_274.csv\n",
      "  - formatted_rules_batch_343.csv\n",
      "  - formatted_rules_batch_268.csv\n",
      "  - formatted_rules_batch_240.csv\n",
      "  - formatted_rules_batch_164.csv\n",
      "  - formatted_rules_batch_14.csv\n",
      "  - formatted_rules_batch_176.csv\n",
      "  - formatted_rules_batch_323.csv\n",
      "  - formatted_rules_batch_128.csv\n",
      "  - formatted_rules_batch_301.csv\n",
      "  - formatted_rules_batch_222.csv\n",
      "  - formatted_rules_batch_284.csv\n",
      "  - formatted_rules_batch_92.csv\n",
      "  - formatted_rules_batch_35.csv\n",
      "  - formatted_rules_batch_275.csv\n",
      "  - formatted_rules_batch_221.csv\n",
      "  - formatted_rules_batch_327.csv\n",
      "  - formatted_rules_batch_276.csv\n",
      "  - formatted_rules_batch_304.csv\n",
      "  - formatted_rules_batch_105.csv\n",
      "  - formatted_rules_batch_38.csv\n",
      "  - formatted_rules_batch_210.csv\n",
      "  - formatted_rules_batch_360.csv\n",
      "  - formatted_rules_batch_365.csv\n",
      "  - formatted_rules_batch_325.csv\n",
      "  - formatted_rules_batch_286.csv\n",
      "  - formatted_rules_batch_269.csv\n",
      "  - formatted_rules_batch_150.csv\n",
      "  - formatted_rules_batch_291.csv\n",
      "  - formatted_rules_batch_129.csv\n",
      "  - formatted_rules_batch_32.csv\n",
      "  - formatted_rules_batch_157.csv\n",
      "  - formatted_rules_batch_265.csv\n",
      "  - formatted_rules_batch_49.csv\n",
      "  - formatted_rules_batch_299.csv\n",
      "  - formatted_rules_batch_136.csv\n",
      "  - formatted_rules_batch_280.csv\n",
      "  - formatted_rules_batch_152.csv\n",
      "  - formatted_rules_batch_295.csv\n",
      "  - formatted_rules_batch_5.csv\n",
      "  - formatted_rules_batch_68.csv\n",
      "  - formatted_rules_batch_250.csv\n",
      "  - formatted_rules_batch_175.csv\n",
      "  - formatted_rules_batch_205.csv\n",
      "  - formatted_rules_batch_308.csv\n",
      "  - formatted_rules_batch_165.csv\n",
      "  - formatted_rules_batch_63.csv\n",
      "  - formatted_rules_batch_40.csv\n",
      "  - formatted_rules_batch_130.csv\n",
      "  - formatted_rules_batch_50.csv\n",
      "  - formatted_rules_batch_289.csv\n",
      "  - formatted_rules_batch_337.csv\n",
      "  - formatted_rules_batch_292.csv\n",
      "  - formatted_rules_batch_47.csv\n",
      "  - formatted_rules_batch_361.csv\n",
      "  - formatted_rules_batch_94.csv\n",
      "  - formatted_rules_batch_257.csv\n",
      "  - formatted_rules_batch_37.csv\n",
      "  - formatted_rules_batch_89.csv\n",
      "  - formatted_rules_batch_7.csv\n",
      "  - formatted_rules_batch_99.csv\n",
      "  - formatted_rules_batch_160.csv\n",
      "  - formatted_rules_batch_315.csv\n",
      "  - formatted_rules_batch_153.csv\n",
      "  - formatted_rules_batch_335.csv\n",
      "  - formatted_rules_batch_248.csv\n",
      "  - formatted_rules_batch_156.csv\n",
      "  - formatted_rules_batch_45.csv\n",
      "  - formatted_rules_batch_172.csv\n",
      "  - formatted_rules_batch_41.csv\n",
      "  - formatted_rules_batch_195.csv\n",
      "  - formatted_rules_batch_85.csv\n",
      "  - formatted_rules_batch_177.csv\n",
      "  - formatted_rules_batch_69.csv\n",
      "  - formatted_rules_batch_71.csv\n",
      "  - formatted_rules_batch_229.csv\n",
      "  - formatted_rules_batch_267.csv\n",
      "  - formatted_rules_batch_204.csv\n",
      "  - formatted_rules_batch_283.csv\n",
      "  - formatted_rules_batch_235.csv\n",
      "  - formatted_rules_batch_300.csv\n",
      "  - formatted_rules_batch_1.csv\n",
      "  - formatted_rules_batch_23.csv\n",
      "  - formatted_rules_batch_123.csv\n",
      "  - formatted_rules_batch_149.csv\n",
      "  - formatted_rules_batch_225.csv\n",
      "  - formatted_rules_batch_241.csv\n",
      "  - formatted_rules_batch_185.csv\n",
      "  - formatted_rules_batch_10.csv\n",
      "  - formatted_rules_batch_270.csv\n",
      "  - formatted_rules_batch_75.csv\n",
      "  - formatted_rules_batch_34.csv\n",
      "  - formatted_rules_batch_351.csv\n",
      "  - formatted_rules_batch_124.csv\n",
      "  - formatted_rules_batch_242.csv\n",
      "  - formatted_rules_batch_44.csv\n",
      "  - formatted_rules_batch_36.csv\n",
      "  - formatted_rules_batch_43.csv\n",
      "  - formatted_rules_batch_52.csv\n",
      "  - formatted_rules_batch_218.csv\n",
      "  - formatted_rules_batch_220.csv\n",
      "  - formatted_rules_batch_281.csv\n",
      "  - formatted_rules_batch_237.csv\n",
      "  - formatted_rules_batch_3.csv\n",
      "  - formatted_rules_batch_13.csv\n",
      "  - formatted_rules_batch_355.csv\n",
      "  - formatted_rules_batch_170.csv\n",
      "  - formatted_rules_batch_161.csv\n",
      "  - formatted_rules_batch_348.csv\n",
      "  - formatted_rules_batch_192.csv\n",
      "  - formatted_rules_batch_87.csv\n",
      "  - formatted_rules_batch_120.csv\n",
      "  - formatted_rules_batch_24.csv\n",
      "  - formatted_rules_batch_12.csv\n",
      "  - formatted_rules_batch_319.csv\n",
      "  - formatted_rules_batch_297.csv\n",
      "  - formatted_rules_batch_217.csv\n",
      "  - formatted_rules_batch_232.csv\n",
      "  - formatted_rules_batch_328.csv\n",
      "  - formatted_rules_batch_259.csv\n",
      "  - formatted_rules_batch_183.csv\n",
      "  - formatted_rules_batch_42.csv\n",
      "  - formatted_rules_batch_313.csv\n",
      "  - formatted_rules_batch_352.csv\n",
      "  - formatted_rules_batch_290.csv\n",
      "  - formatted_rules_batch_189.csv\n",
      "  - formatted_rules_batch_347.csv\n",
      "  - formatted_rules_batch_214.csv\n",
      "  - formatted_rules_batch_353.csv\n",
      "  - formatted_rules_batch_108.csv\n",
      "  - formatted_rules_batch_224.csv\n",
      "  - formatted_rules_batch_151.csv\n",
      "  - formatted_rules_batch_106.csv\n",
      "  - formatted_rules_batch_56.csv\n",
      "  - formatted_rules_batch_287.csv\n",
      "  - formatted_rules_batch_334.csv\n",
      "  - formatted_rules_batch_101.csv\n",
      "  - formatted_rules_batch_31.csv\n",
      "  - formatted_rules_batch_211.csv\n",
      "  - formatted_rules_batch_246.csv\n",
      "  - formatted_rules_batch_234.csv\n",
      "  - formatted_rules_batch_231.csv\n",
      "  - formatted_rules_batch_116.csv\n",
      "  - formatted_rules_batch_66.csv\n",
      "  - formatted_rules_batch_19.csv\n",
      "  - formatted_rules_batch_349.csv\n",
      "  - formatted_rules_batch_162.csv\n",
      "  - formatted_rules_batch_55.csv\n",
      "  - formatted_rules_batch_277.csv\n",
      "  - formatted_rules_batch_155.csv\n",
      "  - formatted_rules_batch_111.csv\n",
      "  - formatted_rules_batch_255.csv\n",
      "  - formatted_rules_batch_82.csv\n",
      "  - formatted_rules_batch_48.csv\n",
      "  - formatted_rules_batch_131.csv\n",
      "  - formatted_rules_batch_339.csv\n",
      "  - formatted_rules_batch_252.csv\n",
      "  - formatted_rules_batch_272.csv\n",
      "  - formatted_rules_batch_198.csv\n",
      "  - formatted_rules_batch_320.csv\n",
      "  - formatted_rules_batch_262.csv\n",
      "  - formatted_rules_batch_145.csv\n",
      "  - formatted_rules_batch_263.csv\n",
      "  - formatted_rules_batch_104.csv\n",
      "  - formatted_rules_batch_137.csv\n",
      "  - formatted_rules_batch_226.csv\n",
      "  - formatted_rules_batch_93.csv\n",
      "  - formatted_rules_batch_74.csv\n",
      "  - formatted_rules_batch_112.csv\n",
      "  - formatted_rules_batch_132.csv\n",
      "  - formatted_rules_batch_65.csv\n",
      "  - formatted_rules_batch_73.csv\n",
      "  - formatted_rules_batch_117.csv\n",
      "  - formatted_rules_batch_278.csv\n",
      "  - formatted_rules_batch_212.csv\n",
      "  - formatted_rules_batch_179.csv\n",
      "  - formatted_rules_batch_345.csv\n",
      "  - formatted_rules_batch_79.csv\n",
      "  - formatted_rules_batch_363.csv\n",
      "  - formatted_rules_batch_200.csv\n",
      "  - formatted_rules_batch_122.csv\n",
      "  - formatted_rules_batch_169.csv\n",
      "  - formatted_rules_batch_26.csv\n",
      "  - formatted_rules_batch_253.csv\n",
      "  - formatted_rules_batch_181.csv\n",
      "  - formatted_rules_batch_81.csv\n",
      "  - formatted_rules_batch_254.csv\n",
      "  - formatted_rules_batch_256.csv\n",
      "  - formatted_rules_batch_168.csv\n",
      "  - formatted_rules_batch_102.csv\n",
      "  - formatted_rules_batch_21.csv\n",
      "  - formatted_rules_batch_332.csv\n",
      "  - formatted_rules_batch_163.csv\n",
      "  - formatted_rules_batch_125.csv\n",
      "  - formatted_rules_batch_53.csv\n",
      "  - formatted_rules_batch_184.csv\n",
      "  - formatted_rules_batch_305.csv\n",
      "  - formatted_rules_batch_29.csv\n",
      "  - formatted_rules_batch_196.csv\n",
      "  - formatted_rules_batch_62.csv\n",
      "  - formatted_rules_batch_264.csv\n",
      "  - formatted_rules_batch_18.csv\n",
      "  - formatted_rules_batch_97.csv\n",
      "  - formatted_rules_batch_322.csv\n",
      "  - formatted_rules_batch_28.csv\n",
      "  - formatted_rules_batch_261.csv\n",
      "  - formatted_rules_batch_70.csv\n",
      "  - formatted_rules_batch_245.csv\n",
      "  - formatted_rules_batch_110.csv\n",
      "  - formatted_rules_batch_266.csv\n",
      "  - formatted_rules_batch_100.csv\n",
      "  - formatted_rules_batch_107.csv\n",
      "  - formatted_rules_batch_247.csv\n",
      "  - formatted_rules_batch_303.csv\n",
      "  - formatted_rules_batch_187.csv\n",
      "  - formatted_rules_batch_219.csv\n",
      "  - formatted_rules_batch_146.csv\n",
      "  - formatted_rules_batch_39.csv\n",
      "  - formatted_rules_batch_115.csv\n",
      "  - formatted_rules_batch_338.csv\n",
      "  - formatted_rules_batch_159.csv\n",
      "  - formatted_rules_batch_346.csv\n",
      "  - formatted_rules_batch_154.csv\n",
      "  - formatted_rules_batch_314.csv\n",
      "  - formatted_rules_batch_321.csv\n",
      "  - formatted_rules_batch_80.csv\n",
      "  - formatted_rules_batch_83.csv\n",
      "  - formatted_rules_batch_312.csv\n",
      "  - formatted_rules_batch_148.csv\n",
      "  - formatted_rules_batch_213.csv\n",
      "  - formatted_rules_batch_191.csv\n",
      "  - formatted_rules_batch_329.csv\n",
      "  - formatted_rules_batch_147.csv\n",
      "Combined shape: (36616, 10)\n",
      "  Subreddit     Original_Rule_Name  \\\n",
      "0    Kochen  Keine Affiliate-Links   \n",
      "1    Kochen    Videos auf r/Kochen   \n",
      "\n",
      "                           Original_Rule_Description Formatted_Rule_Name  \\\n",
      "0  Wenn ihr auf Produkte, die mit dem Kochen zu t...                 NaN   \n",
      "1  Wenn ihr einen Youtube Kanal im Rezept verlink...                 NaN   \n",
      "\n",
      "  Formatted_Rule_Description Final_Formatted_Rule Clarity_Tag Clarity_Reason  \\\n",
      "0                        NaN                  NaN         NaN            NaN   \n",
      "1                        NaN                  NaN         NaN            NaN   \n",
      "\n",
      "  Raw_Response                                              Error  \n",
      "0          NaN  429 You exceeded your current quota, please ch...  \n",
      "1          NaN  429 You exceeded your current quota, please ch...  \n",
      "Saved combined data to: ./data/synthetic_generation/formatted_rules_batch_8/popular_subreddit_rules_formatted.csv\n",
      "(12546, 10)\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import glob\n",
    "# import os\n",
    "\n",
    "# # Your path\n",
    "# base_path = \"./data/synthetic_generation/formatted_rules_batch_8/\"\n",
    "\n",
    "# # Find all CSV files in the directory\n",
    "# csv_files = glob.glob(f\"{base_path}*.csv\")\n",
    "# print(f\"Found {len(csv_files)} CSV files:\")\n",
    "# for file in csv_files:\n",
    "#     print(f\"  - {os.path.basename(file)}\")\n",
    "\n",
    "# # Load and combine all CSV files\n",
    "# dfs = [pd.read_csv(file) for file in csv_files]\n",
    "# df_test = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# print(f\"Combined shape: {df_test.shape}\")\n",
    "# print(df_test.head(2))\n",
    "\n",
    "# df_test.columns=['subreddit', 'Original_Rule_Name', 'Original_Rule_Description',\n",
    "#        'Formatted_Rule_Name', 'Formatted_Rule_Description',\n",
    "#        'formatted_rule', 'clarity_tag', 'Clarity_Reason', 'Raw_Response',\n",
    "#        'Error']\n",
    "\n",
    "# df_test = df_test[df_test['formatted_rule'].notna()]\n",
    "\n",
    "# # Save the combined data\n",
    "# output_file = f\"{base_path}popular_subreddit_rules_formatted.csv\"\n",
    "# df_test.to_csv(output_file, index=False)\n",
    "# print(f\"Saved combined data to: {output_file}\")\n",
    "# print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb46e22-1db1-40c2-a286-0ca42a9f6388",
   "metadata": {},
   "source": [
    "### combine ibatch generation into train/test single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ff8d0c-e965-4941-beb6-2f6a61e42668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 CSV files:\n",
      "Combined shape: (4968, 12)\n",
      "  Subreddit                                          Rule Name  \\\n",
      "0       wow  No Real-World Politics: Discussion of real-wor...   \n",
      "\n",
      "                                      Formatted Rule  \\\n",
      "0  No Real-World Politics: Discussion of real-wor...   \n",
      "\n",
      "                                  Positive Example 1  \\\n",
      "0  \"What do you guys think about the new tax bill...   \n",
      "\n",
      "                                  Negative Example 1  \\\n",
      "0  \"I can't believe how much money Blizzard is ma...   \n",
      "\n",
      "                                  Positive Example 2  \\\n",
      "0  \"Does anyone else think the recent election re...   \n",
      "\n",
      "                                  Negative Example 2  \\\n",
      "0  \"Anyone else excited for the next WoW patch? I...   \n",
      "\n",
      "                                        Test Comment Violates Rule  \\\n",
      "0  \"I heard that the new government is looking in...          True   \n",
      "\n",
      "                                        Raw Response  \\\n",
      "0  Formatted Rule: No Real-World Politics: Discus...   \n",
      "\n",
      "                               Example Comments Used  Error  \n",
      "0  \\nExample Reddit Comments from /r/personalfina...    NaN  \n",
      "after dropna: (4959, 8)\n",
      "Saved combined data to: ./data/synthetic_generation/batch_kr_1_train.csv\n",
      "Final shape: (4959, 8)\n"
     ]
    }
   ],
   "source": [
    "## combine ibatch generation into train/test single file\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Your path\n",
    "batch=\"batch_kr_1\"\n",
    "base_path = f\"./data/synthetic_generation/{batch}/\"\n",
    "output_path=\"./data/synthetic_generation/\"\n",
    "\n",
    "# Find all CSV files in the directory\n",
    "csv_files = glob.glob(f\"{base_path}*.csv\")\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "# for file in csv_files:\n",
    "#     print(f\"  - {os.path.basename(file)}\")\n",
    "\n",
    "# Load and combine all CSV files\n",
    "dfs = [pd.read_csv(file) for file in csv_files]\n",
    "df_test = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(f\"Combined shape: {df_test.shape}\")\n",
    "print(df_test.head(1))\n",
    "\n",
    "\n",
    "#print(df_test.columns)\n",
    "### commet this part for batch-1,2,3\n",
    "req_cols=['Subreddit', 'Rule Name', 'Formatted Rule',\n",
    "       'Positive Example 1', 'Negative Example 1', 'Positive Example 2',\n",
    "       'Negative Example 2', 'Test Comment', 'Violates Rule', 'Raw Response',\n",
    "       'Error']\n",
    "df_test=df_test[req_cols]\n",
    "\n",
    "df_test.columns=['subreddit', 'Rule Name', 'rule', 'positive_example_1', 'negative_example_1', 'positive_example_2',\n",
    "       'negative_example_2', 'test_comment', 'violates_rule', 'Raw Response',\n",
    "        'Error']\n",
    "\n",
    "req_cols=['subreddit', 'rule', 'positive_example_1', 'negative_example_1', 'positive_example_2',\n",
    "       'negative_example_2', 'test_comment', 'violates_rule']\n",
    "\n",
    "df_test=df_test[req_cols]\n",
    "df_test=df_test.dropna()\n",
    "print(\"after dropna:\",df_test.shape)\n",
    "\n",
    "# Save the combined data\n",
    "output_file = f\"{output_path}{batch}_train.csv\"\n",
    "df_test.to_csv(output_file, index=False)\n",
    "print(f\"Saved combined data to: {output_file}\")\n",
    "print(\"Final shape:\",df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b74a4d35-ab12-4882-9dbb-bcda40e3a6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12546, 10)\n",
      "(24616, 10)\n",
      "Combined shape: (24616, 10)\n",
      "           subreddit                                 Original_Rule_Name  \\\n",
      "0  tearsofthekingdom  ðŸ’§ | Content MUST relate to The Legend of Zelda...   \n",
      "1  tearsofthekingdom    âœ¨| Be original & follow post quality standards.   \n",
      "\n",
      "                           Original_Rule_Description  \\\n",
      "0  ----\\n\\n#####ðŸŒ¿ Submissions in **r/TearsOfTheKi...   \n",
      "1  ----\\n\\n#####**ðŸ”ï¸ Aim to submit unique content...   \n",
      "\n",
      "               Formatted_Rule_Name  \\\n",
      "0        TotK Related Content Only   \n",
      "1  Original & High-Quality Content   \n",
      "\n",
      "                          Formatted_Rule_Description  \\\n",
      "0  Content must be directly related to Tears of t...   \n",
      "1  Submit unique content; avoid repetitive trends...   \n",
      "\n",
      "                                      formatted_rule clarity_tag  \\\n",
      "0  TotK Related Content Only: Content must be dir...       Clear   \n",
      "1  Original & High-Quality Content: Submit unique...       Clear   \n",
      "\n",
      "                   Clarity_Reason  \\\n",
      "0  Rule is clear and well-defined   \n",
      "1  Rule is clear and well-defined   \n",
      "\n",
      "                                        Raw_Response Error  \n",
      "0  Formatted Rule Name: TotK Related Content Only...   NaN  \n",
      "1  Formatted Rule Name: Original & High-Quality C...   NaN  \n",
      "Saved combined data to: ./data/popular_subreddit_rules_formatted_2.csv\n",
      "(6019, 10)\n"
     ]
    }
   ],
   "source": [
    "# df_1=pd.read_csv(\"/home/vino/ML_Projects/Jigsaw-ACRC-Kaggle/data/synthetic_generation/popular_subreddit_rules_formatted_1.csv\")\n",
    "# print(df_1.shape)\n",
    "# df_2=pd.read_csv(\"/home/vino/ML_Projects/Jigsaw-ACRC-Kaggle/data/synthetic_generation/popular_subreddit_rules_formatted_2.csv\")\n",
    "# df_2.columns=[\n",
    "#     'subreddit', 'Original_Rule_Name', 'Original_Rule_Description',\n",
    "#     'Formatted_Rule_Name', 'Formatted_Rule_Description',\n",
    "#     'formatted_rule', 'clarity_tag', 'Clarity_Reason', 'Raw_Response',\n",
    "#     'Error'\n",
    "# ]\n",
    "# print(df_2.shape)\n",
    "# #df_test = pd.concat([df_1, df_2], ignore_index=True)\n",
    "# df_test=df_2\n",
    "# print(f\"Combined shape: {df_test.shape}\")\n",
    "# print(df_test.head(2))\n",
    "\n",
    "# # Rename columns\n",
    "# df_test.columns = [\n",
    "#     'subreddit', 'Original_Rule_Name', 'Original_Rule_Description',\n",
    "#     'Formatted_Rule_Name', 'Formatted_Rule_Description',\n",
    "#     'formatted_rule', 'clarity_tag', 'Clarity_Reason', 'Raw_Response',\n",
    "#     'Error'\n",
    "# ]\n",
    "\n",
    "# # Keep only rows where 'formatted_rule' is not NaN\n",
    "# df_test = df_test[df_test['formatted_rule'].notna()]\n",
    "\n",
    "# # Save the combined data\n",
    "# output_file = f\"{base_path}popular_subreddit_rules_formatted_2.csv\"\n",
    "# df_test.to_csv(output_file, index=False)\n",
    "\n",
    "# print(f\"Saved combined data to: {output_file}\")\n",
    "# print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84fa478d-867d-4c8a-b1eb-b77e1178c2af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Subreddit', 'Original_Rule_Name', 'Original_Rule_Description',\n",
       "       'Formatted_Rule_Name', 'Formatted_Rule_Description',\n",
       "       'Final_Formatted_Rule', 'Clarity_Tag', 'Clarity_Reason', 'Raw_Response',\n",
       "       'Error'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc13ff46-b400-4638-9608-aa4335f6d7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Subreddit', 'Original_Rule_Name', 'Original_Rule_Description',\n",
       "       'Formatted_Rule_Name', 'Formatted_Rule_Description',\n",
       "       'Final_Formatted_Rule', 'Clarity_Tag', 'Clarity_Reason', 'Raw_Response',\n",
       "       'Error'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b6914-69d0-4e68-8990-adfebf3322ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns = [\n",
    "    'subreddit', 'Original_Rule_Name', 'Original_Rule_Description',\n",
    "    'Formatted_Rule_Name', 'Formatted_Rule_Description',\n",
    "    'formatted_rule', 'clarity_tag', 'Clarity_Reason', 'Raw_Response',\n",
    "    'Error'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5931aebe-426f-43ad-9818-4f4ef15e0c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26899, 8)\n"
     ]
    }
   ],
   "source": [
    "base_path=\"./data/final/\"\n",
    "df_train = pd.read_csv(f\"{base_path}df_train.csv\")\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e51a64be-682b-48eb-8721-7506b966f7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26633,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"test_comment\"].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd0ea71-1d39-4f72-ae4e-c8114ed6d0c2",
   "metadata": {},
   "source": [
    "### combine individual train files into single final train file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21f7c5c6-7add-4735-bc9e-98fbbcddebab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated 1 train files: ['./data/final/df_train_ds_fgkr_01.csv']\n",
      "Concatenated 1 test files: ['./data/final/df_test_cr_12.csv']\n",
      "Train shape: (20520, 12)\n",
      "Test shape: (2000, 8)\n",
      "Index(['subreddit', 'rule', 'formatted_rule', 'positive_example_1',\n",
      "       'negative_example_1', 'positive_example_2', 'negative_example_2',\n",
      "       'test_comment', 'violates_rule', 'raw_response',\n",
      "       'example_comments_used', 'error'],\n",
      "      dtype='object')\n",
      "Dropped 2 rows from train due to invalid 'violates_rule'\n",
      "Dropped 0 rows from test due to invalid 'violates_rule'\n",
      "subreddit: 0 rows would be dropped\n",
      "rule: 0 rows would be dropped\n",
      "positive_example_1: 0 rows would be dropped\n",
      "negative_example_1: 0 rows would be dropped\n",
      "positive_example_2: 0 rows would be dropped\n",
      "negative_example_2: 0 rows would be dropped\n",
      "test_comment: 0 rows would be dropped\n",
      "violates_rule: 0 rows would be dropped\n",
      "Using path: ./data/final/\n",
      "\n",
      " After dropping:\n",
      "Train shape: (20518, 8)\n",
      "Test shape: (2000, 8)\n",
      "\n",
      " After checking True/False:\n",
      "Train shape: (20518, 8)\n",
      "Test shape: (2000, 8)\n"
     ]
    }
   ],
   "source": [
    "### combine individual train files into single final train file\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "WITHOUT_KAGGLE=True\n",
    "\n",
    "# Check if running on Kaggle\n",
    "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "    # Running on Kaggle\n",
    "    base_path = \"/kaggle/input/jigsaw-agile-community-rules/\"\n",
    "    df_train = pd.read_csv(f\"{base_path}train.csv\")\n",
    "    df_test = pd.read_csv(f\"{base_path}test.csv\")\n",
    "else:\n",
    "    # Running locally\n",
    "    base_path = \"./data/final/\"\n",
    "    \n",
    "    # Find all train files\n",
    "    train_files = glob.glob(f\"{base_path}df_train_ds_fgkr_01.csv\")\n",
    "    if WITHOUT_KAGGLE and './data/synthetic_generation/batch_0_train.csv' in train_files:\n",
    "        train_files.remove('./data/synthetic_generation/batch_0_train.csv')\n",
    "        \n",
    "    if train_files:\n",
    "        train_dfs = [pd.read_csv(file) for file in train_files]\n",
    "        df_train = pd.concat(train_dfs, ignore_index=True)\n",
    "        print(f\"Concatenated {len(train_files)} train files: {train_files}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No train files found in {base_path}\")\n",
    "    \n",
    "    # Find all test files\n",
    "    test_files = glob.glob(f\"{base_path}df_test*.csv\")\n",
    "    if test_files:\n",
    "        test_dfs = [pd.read_csv(file) for file in test_files]\n",
    "        df_test = pd.concat(test_dfs, ignore_index=True)\n",
    "        print(f\"Concatenated {len(test_files)} test files: {test_files}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No test files found in {base_path}\")\n",
    "\n",
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Test shape: {df_test.shape}\")\n",
    "\n",
    "print(df_train.columns)\n",
    "        \n",
    "req_cols=['subreddit', 'rule', 'positive_example_1', 'negative_example_1', 'positive_example_2',\n",
    "       'negative_example_2', 'test_comment', 'violates_rule']\n",
    "\n",
    "df_train=df_train[req_cols]\n",
    "df_test=df_test[req_cols]\n",
    "\n",
    "# Normalize \"True\"/\"False\" -> \"Yes\"/\"No\" and drop anything else\n",
    "for name, df in [(\"train\", df_train), (\"test\", df_test)]:\n",
    "    df[\"violates_rule\"] = (\n",
    "        df[\"violates_rule\"]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.replace(r'^[\"\\']|[\"\\']$', '', regex=True)  # Remove leading/trailing quotes\n",
    "        .str.strip()  # Strip again after removing quotes\n",
    "        .map({\"True\": \"Yes\", \"False\": \"No\", \"Yes\": \"Yes\", \"No\": \"No\"})\n",
    "    )\n",
    "    before = len(df)\n",
    "    df.dropna(subset=[\"violates_rule\"], inplace=True)  # drop rows with NaN (anything not Yes/No/True/False)\n",
    "    after = len(df)\n",
    "    print(f\"Dropped {before - after} rows from {name} due to invalid 'violates_rule'\")\n",
    "        \n",
    "for col in req_cols:\n",
    "    dropped_rows = df_train[df_train[col].isna()].shape[0]\n",
    "    print(f\"{col}: {dropped_rows} rows would be dropped\")\n",
    "    \n",
    "df_train = df_train[req_cols].dropna()\n",
    "df_test = df_test[req_cols].dropna()\n",
    "\n",
    "print(f\"Using path: {base_path}\")\n",
    "print(\"\\n After dropping:\")\n",
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Test shape: {df_test.shape}\")\n",
    "df_train.head(2)\n",
    "\n",
    "df_train[\"violates_rule\"] = df_train[\"violates_rule\"].astype(str)\n",
    "df_test[\"violates_rule\"] = df_test[\"violates_rule\"].astype(str)\n",
    "\n",
    "valid_values = {\"Yes\", \"No\"}\n",
    "df_train = df_train[df_train[\"violates_rule\"].isin(valid_values)]\n",
    "df_test  = df_test[df_test[\"violates_rule\"].isin(valid_values)]\n",
    "print(\"\\n After checking True/False:\")\n",
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Test shape: {df_test.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if WITHOUT_KAGGLE:\n",
    "#     df_train.to_csv(\"df_train_without_kaggle.csv\",index=False)\n",
    "# else:\n",
    "#     df_train.to_csv(\"df_train.csv\",index=False)\n",
    "    \n",
    "# df_test.to_csv(\"df_test.csv\",index=False)\n",
    "\n",
    "# print(df_train[\"violates_rule\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2e34598-bb66-4abe-86dc-d762060c87a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>rule</th>\n",
       "      <th>positive_example_1</th>\n",
       "      <th>negative_example_1</th>\n",
       "      <th>positive_example_2</th>\n",
       "      <th>negative_example_2</th>\n",
       "      <th>test_comment</th>\n",
       "      <th>violates_rule</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hearthstone</td>\n",
       "      <td>\"No Piracy: Do not post requests for illegal s...</td>\n",
       "      <td>\"Hey everyone, does anyone have a link to a fr...</td>\n",
       "      <td>\"Just picked up the new Hearthstone expansion,...</td>\n",
       "      <td>\"Anyone know a good site to catch the Hearthst...</td>\n",
       "      <td>\"Anyone got tips for the new Arena meta? I'm s...</td>\n",
       "      <td>\"I heard the new Hearthstone expansion is amaz...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nosleep</td>\n",
       "      <td>\"No Abusive Circlejerks: Abusive, repetitive, ...</td>\n",
       "      <td>\"Dude, you're such a moron. Every time you pos...</td>\n",
       "      <td>\"I see what you mean about the tension buildin...</td>\n",
       "      <td>\" seriously, you need to shut up. You just kee...</td>\n",
       "      <td>\"That twist at the end was so unexpected! I di...</td>\n",
       "      <td>\"I think the ending was a bit rushed, but over...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit                                               rule  \\\n",
       "0  hearthstone  \"No Piracy: Do not post requests for illegal s...   \n",
       "1      nosleep  \"No Abusive Circlejerks: Abusive, repetitive, ...   \n",
       "\n",
       "                                  positive_example_1  \\\n",
       "0  \"Hey everyone, does anyone have a link to a fr...   \n",
       "1  \"Dude, you're such a moron. Every time you pos...   \n",
       "\n",
       "                                  negative_example_1  \\\n",
       "0  \"Just picked up the new Hearthstone expansion,...   \n",
       "1  \"I see what you mean about the tension buildin...   \n",
       "\n",
       "                                  positive_example_2  \\\n",
       "0  \"Anyone know a good site to catch the Hearthst...   \n",
       "1  \" seriously, you need to shut up. You just kee...   \n",
       "\n",
       "                                  negative_example_2  \\\n",
       "0  \"Anyone got tips for the new Arena meta? I'm s...   \n",
       "1  \"That twist at the end was so unexpected! I di...   \n",
       "\n",
       "                                        test_comment violates_rule  \n",
       "0  \"I heard the new Hearthstone expansion is amaz...            No  \n",
       "1  \"I think the ending was a bit rushed, but over...            No  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.to_csv(\"df_train_ds_fgkr_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2df4d722-f093-4737-9222-41fd01c88f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List to hold new rows\n",
    "new_rows = []\n",
    "\n",
    "# Loop through each row in the original dataframe\n",
    "for _, row in df_train.iterrows():\n",
    "    base = {\n",
    "        'subreddit': row['subreddit'],\n",
    "        'rule': row['rule'],\n",
    "        'violates_rule': row['violates_rule'],\n",
    "    }\n",
    "\n",
    "    if row['violates_rule'] == 'Yes':\n",
    "        # Swap with positive examples\n",
    "        if pd.notna(row['positive_example_1']):\n",
    "            new_rows.append({\n",
    "                **base,\n",
    "                'test_comment': row['positive_example_1'],\n",
    "                'positive_example_1': row['test_comment'],\n",
    "                'positive_example_2': row['positive_example_2'],\n",
    "                'negative_example_1': row['negative_example_1'],\n",
    "                'negative_example_2': row['negative_example_2'],\n",
    "            })\n",
    "        if pd.notna(row['positive_example_2']):\n",
    "            new_rows.append({\n",
    "                **base,\n",
    "                'test_comment': row['positive_example_2'],\n",
    "                'positive_example_1': row['positive_example_1'],\n",
    "                'positive_example_2': row['test_comment'],\n",
    "                'negative_example_1': row['negative_example_1'],\n",
    "                'negative_example_2': row['negative_example_2'],\n",
    "            })\n",
    "\n",
    "    elif row['violates_rule'] == 'No':\n",
    "        # Swap with negative examples\n",
    "        if pd.notna(row['negative_example_1']):\n",
    "            new_rows.append({\n",
    "                **base,\n",
    "                'test_comment': row['negative_example_1'],\n",
    "                'positive_example_1': row['positive_example_1'],\n",
    "                'positive_example_2': row['positive_example_2'],\n",
    "                'negative_example_1': row['test_comment'],\n",
    "                'negative_example_2': row['negative_example_2'],\n",
    "            })\n",
    "        if pd.notna(row['negative_example_2']):\n",
    "            new_rows.append({\n",
    "                **base,\n",
    "                'test_comment': row['negative_example_2'],\n",
    "                'positive_example_1': row['positive_example_1'],\n",
    "                'positive_example_2': row['positive_example_2'],\n",
    "                'negative_example_1': row['negative_example_1'],\n",
    "                'negative_example_2': row['test_comment'],\n",
    "            })\n",
    "\n",
    "# Convert list of new rows to DataFrame\n",
    "augmented_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# Optionally, combine with original data\n",
    "df_augmented_full = pd.concat([df_train, augmented_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ae40e4a-e575-433f-8172-beb73f7bad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df.to_csv(\"df_train_kdsr1_swap.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a35e64b8-6c78-4fc1-a011-e004e1f5bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Step 1: Keep only the main row\n",
    "# df_main = df_train[['rule', 'test_comment', 'violates_rule']].copy()\n",
    "\n",
    "# # Step 2: Positive examples\n",
    "# df_pos1 = df_train[['rule', 'positive_example_1']].copy()\n",
    "# df_pos1['violates_rule'] = 'Yes'\n",
    "# df_pos1.rename(columns={'positive_example_1': 'test_comment'}, inplace=True)\n",
    "\n",
    "# df_pos2 = df_train[['rule', 'positive_example_2']].copy()\n",
    "# df_pos2['violates_rule'] = 'Yes'\n",
    "# df_pos2.rename(columns={'positive_example_2': 'test_comment'}, inplace=True)\n",
    "\n",
    "# # Step 3: Negative examples\n",
    "# df_neg1 = df_train[['rule', 'negative_example_1']].copy()\n",
    "# df_neg1['violates_rule'] = 'No'\n",
    "# df_neg1.rename(columns={'negative_example_1': 'test_comment'}, inplace=True)\n",
    "\n",
    "# df_neg2 = df_train[['rule', 'negative_example_2']].copy()\n",
    "# df_neg2['violates_rule'] = 'No'\n",
    "# df_neg2.rename(columns={'negative_example_2': 'test_comment'}, inplace=True)\n",
    "\n",
    "# # Step 4: Concatenate all\n",
    "# df_final = pd.concat([df_main, df_pos1, df_pos2, df_neg1, df_neg2], ignore_index=True)\n",
    "\n",
    "# # Step 5: Convert rule_violation to binary value\n",
    "# df_final['value'] = df_final['violates_rule'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# # Done!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f059f042-25f6-4121-ba12-63e87c90b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Keep only the main row\n",
    "df_main = df_train[['rule', 'test_comment', 'violates_rule']].copy()\n",
    "\n",
    "# Step 2: Positive examples\n",
    "df_pos1 = df_train[['rule', 'positive_example_1']].copy()\n",
    "df_pos1['violates_rule'] = 'Yes'\n",
    "df_pos1.rename(columns={'positive_example_1': 'test_comment'}, inplace=True)\n",
    "\n",
    "df_pos2 = df_train[['rule', 'positive_example_2']].copy()\n",
    "df_pos2['violates_rule'] = 'Yes'\n",
    "df_pos2.rename(columns={'positive_example_2': 'test_comment'}, inplace=True)\n",
    "\n",
    "# Step 3: Negative examples\n",
    "df_neg1 = df_train[['rule', 'negative_example_1']].copy()\n",
    "df_neg1['violates_rule'] = 'No'\n",
    "df_neg1.rename(columns={'negative_example_1': 'test_comment'}, inplace=True)\n",
    "\n",
    "df_neg2 = df_train[['rule', 'negative_example_2']].copy()\n",
    "df_neg2['violates_rule'] = 'No'\n",
    "df_neg2.rename(columns={'negative_example_2': 'test_comment'}, inplace=True)\n",
    "\n",
    "# Step 4: Concatenate all\n",
    "df_final = pd.concat([df_main, df_pos1, df_pos2, df_neg1, df_neg2], ignore_index=True)\n",
    "\n",
    "# Step 5: Convert rule_violation to binary value\n",
    "df_final['value'] = df_final['violates_rule'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Done!\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11175052,
     "sourceId": 89659,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.756759,
   "end_time": "2025-02-26T02:09:27.363350",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-26T02:09:20.606591",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
