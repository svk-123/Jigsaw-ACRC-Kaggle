{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f506feee-98f0-4106-a53a-ccbff09b3ce3",
   "metadata": {},
   "source": [
    "# Jigsaw - Agile Community Rules Classification\n",
    "### https://www.kaggle.com/competitions/jigsaw-agile-community-rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fee25d6f-3e57-46f7-9d77-47f9a295a78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b22aad07-c794-49d5-b6ac-dc85f984187c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f62b246c-a5ee-407a-90ce-cbbac769c7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Virtual environment 'myvenv' already exists. Skipping creation.\n",
      "Activating virtual environment...\n",
      "Checking Python binary path:\n",
      "/workspace/myvenv/bin/python3\n",
      "Checking Python version:\n",
      "Python 3.11.13\n",
      "✅ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "VENV_DIR=\"myvenv\"\n",
    "\n",
    "if [ -d \"$VENV_DIR\" ]; then\n",
    "  echo \"✅ Virtual environment '$VENV_DIR' already exists. Skipping creation.\"\n",
    "else\n",
    "  echo \"Creating virtual environment...\"\n",
    "  uv venv \"$VENV_DIR\"\n",
    "fi\n",
    "\n",
    "echo \"Activating virtual environment...\"\n",
    "source \"$VENV_DIR/bin/activate\"\n",
    "\n",
    "echo \"Checking Python binary path:\"\n",
    "which python3\n",
    "\n",
    "echo \"Checking Python version:\"\n",
    "python3 --version\n",
    "\n",
    "echo \"✅ Setup complete!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3999429b-938c-415d-8628-1c4bc03ba026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping uv if needed...\n",
      "Requirement already satisfied: pip in ./myvenv/lib/python3.11/site-packages (25.2)\n",
      "Requirement already satisfied: setuptools in ./myvenv/lib/python3.11/site-packages (80.9.0)\n",
      "Requirement already satisfied: wheel in ./myvenv/lib/python3.11/site-packages (0.45.1)\n",
      "Requirement already satisfied: uv in ./myvenv/lib/python3.11/site-packages (0.8.17)\n",
      "Installing required libraries with uv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.13 environment at: myvenv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m8 packages\u001b[0m \u001b[2min 86ms\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done installing packages into myvenv using uv.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "PYTHON=./myvenv/bin/python\n",
    "\n",
    "echo \"Bootstrapping uv if needed...\"\n",
    "$PYTHON -m pip install --upgrade pip setuptools wheel\n",
    "$PYTHON -m pip install uv\n",
    "\n",
    "echo \"Installing required libraries with uv...\"\n",
    "$PYTHON -m uv pip install \\\n",
    "  trl \\\n",
    "  optimum \\\n",
    "  auto-gptq \\\n",
    "  bitsandbytes \\\n",
    "  peft \\\n",
    "  accelerate \\\n",
    "  deepspeed \\\n",
    "  kagglehub\n",
    "\n",
    "echo \"✅ Done installing packages into myvenv using uv.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffacf4e8-e1fa-4b50-9f70-8cc9c2f2f966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.py\n",
    "\n",
    "RESUME_TRAINING=False\n",
    "LOCAL_MODEL_PATH = \"Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4\"\n",
    "LORA_IN_PATH= \"./lora\"\n",
    "LORA_OUT_PATH = \"./\"\n",
    "DATA_PATH = \"./\"\n",
    "OUTPUT_PATH=\"./outputs/\"\n",
    "\n",
    "# Training parameters\n",
    "MAX_SEQ_LENGTH = 4096\n",
    "RANK = 64\n",
    "MAX_ITER_STEPS = 10\n",
    "EPOCHS = -1\n",
    "SAMPLE_LEN=\"25k\"\n",
    "\n",
    "# Kaggle upload configuration\n",
    "MODEL_SLUG = \"qwen25-7b-gptq-int4-jigsaw-acrc-lora\"\n",
    "VARIATION_SLUG = \"01\"\n",
    "\n",
    "###--------------------------------###\n",
    "DATASET_ID=\"1to9yn\"\n",
    "BASE_MODEL=LOCAL_MODEL_PATH.split(\"/\")[-1].replace(\".\", \"p\")\n",
    "TRAIN_DIR=f\"{BASE_MODEL}_lora_fp16_r{RANK}_s{SAMPLE_LEN}_e_{EPOCHS}_msl{MAX_SEQ_LENGTH}-{DATASET_ID}\"\n",
    "print(\"TRAIN_DIR\",TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "016b3fc1-46cb-4dd4-b7ba-f0140e144fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting get_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile get_dataset.py\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import kagglehub\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load Jigsaw ACRC dataset from Kaggle or local files\"\"\"\n",
    "    # Check if running on Kaggle\n",
    "    if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "        # Running on Kaggle\n",
    "        base_path = \"/kaggle/input/jigsaw-agile-community-rules/\"\n",
    "        df_train = pd.read_csv(f\"{base_path}*train*.csv\")\n",
    "        df_test = pd.read_csv(f\"{base_path}*test*.csv\")\n",
    "    else:\n",
    "        # Running locally\n",
    "        base_path = \"./\"\n",
    "        \n",
    "        # Find all train files\n",
    "        train_files = glob.glob(f\"{base_path}*train*.csv\")\n",
    "        if train_files:\n",
    "            train_dfs = [pd.read_csv(file) for file in train_files]\n",
    "            df_train = pd.concat(train_dfs, ignore_index=True)\n",
    "            print(f\"Concatenated {len(train_files)} train files: {train_files}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No train files found in {base_path}\")\n",
    "        \n",
    "        # Find all test files\n",
    "        test_files = glob.glob(f\"{base_path}*test*.csv\")\n",
    "        if test_files:\n",
    "            test_dfs = [pd.read_csv(file) for file in test_files]\n",
    "            df_test = pd.concat(test_dfs, ignore_index=True)\n",
    "            print(f\"Concatenated {len(test_files)} test files: {test_files}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No test files found in {base_path}\")\n",
    "\n",
    "    print(f\"Train shape: {df_train.shape}\")\n",
    "    print(f\"Test shape: {df_test.shape}\")\n",
    "    print(df_train.columns)\n",
    "            \n",
    "    req_cols=['subreddit', 'rule', 'positive_example_1', 'negative_example_1', 'positive_example_2',\n",
    "           'negative_example_2', 'test_comment', 'violates_rule']\n",
    "\n",
    "    df_train=df_train[req_cols]\n",
    "    df_test=df_test[req_cols]\n",
    "\n",
    "    # Normalize \"True\"/\"False\" -> \"Yes\"/\"No\" and drop anything else\n",
    "    for name, df in [(\"train\", df_train), (\"test\", df_test)]:\n",
    "        df[\"violates_rule\"] = (\n",
    "            df[\"violates_rule\"]\n",
    "            .astype(str).str.strip()\n",
    "            .map({\"True\": \"Yes\", \"False\": \"No\", \"Yes\": \"Yes\", \"No\": \"No\"})  # normalize\n",
    "        )\n",
    "        before = len(df)\n",
    "        df.dropna(subset=[\"violates_rule\"], inplace=True)  # drop rows with NaN (anything not Yes/No/True/False)\n",
    "        after = len(df)\n",
    "        print(f\"Dropped {before - after} rows from {name} due to invalid 'violates_rule'\")\n",
    "    \n",
    "    for col in req_cols:\n",
    "        dropped_rows = df_train[df_train[col].isna()].shape[0]\n",
    "        print(f\"{col}: {dropped_rows} rows would be dropped\")\n",
    "        \n",
    "    df_train = df_train[req_cols].dropna()\n",
    "    df_test = df_test[req_cols].dropna()\n",
    "\n",
    "    print(f\"Using path: {base_path}\")\n",
    "    print(\"\\n After dropping:\")\n",
    "    print(f\"Train shape: {df_train.shape}\")\n",
    "    print(f\"Test shape: {df_test.shape}\")\n",
    "\n",
    "    df_train[\"violates_rule\"] = df_train[\"violates_rule\"].astype(str)\n",
    "    df_test[\"violates_rule\"] = df_test[\"violates_rule\"].astype(str)\n",
    "\n",
    "    valid_values = {\"Yes\", \"No\"}\n",
    "    df_train = df_train[df_train[\"violates_rule\"].isin(valid_values)]\n",
    "    df_test  = df_test[df_test[\"violates_rule\"].isin(valid_values)]\n",
    "    print(\"\\n After checking Yes/No:\")\n",
    "    print(f\"Train shape: {df_train.shape}\")\n",
    "    print(f\"Test shape: {df_test.shape}\")\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"\n",
    "    Format Reddit moderation dataset for ChatML training - matches inference format exactly\n",
    "    \"\"\"\n",
    "    \n",
    "    texts = []\n",
    "    \n",
    "    for i in range(len(examples['subreddit'])):\n",
    "        # Create system message\n",
    "        system_msg = f\"You are a really experienced moderator for the subreddit /r/{examples['subreddit'][i]}. Your job is to determine if the following reported comment violates the given rule. Answer with only \\\"Yes\\\" or \\\"No\\\".\"\n",
    "        \n",
    "        # Create user message with the rule and examples\n",
    "        user_msg = f\"\"\"Rule: {examples['rule'][i]}\n",
    "Example 1:\n",
    "{examples['positive_example_1'][i]}\n",
    "Rule violation: Yes\n",
    "Example 2:\n",
    "{examples['negative_example_1'][i]}\n",
    "Rule violation: No\n",
    "Example 3:\n",
    "{examples['positive_example_2'][i]}\n",
    "Rule violation: Yes\n",
    "Example 4:\n",
    "{examples['negative_example_2'][i]}\n",
    "Rule violation: No\n",
    "Test sentence:\n",
    "{examples['test_comment'][i]}\"\"\"\n",
    "        \n",
    "        # Assistant response is \"Yes\" or \"No\"\n",
    "        assistant_msg = examples['violates_rule'][i]\n",
    "        \n",
    "        # Format as ChatML\n",
    "        chatml_text = f\"\"\"<|im_start|>system\n",
    "{system_msg}<|im_end|>\n",
    "<|im_start|>user\n",
    "{user_msg}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{assistant_msg}<|im_end|>\"\"\"\n",
    "        \n",
    "        texts.append(chatml_text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "def build_dataset():\n",
    "    \"\"\"\n",
    "    Build both train and test datasets using the new ChatML format\n",
    "    \"\"\"\n",
    "    df_train, df_test = load_data()\n",
    "    \n",
    "    train_dataset = Dataset.from_pandas(df_train)\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda examples: formatting_prompts_func(examples), \n",
    "        batched=True\n",
    "    )\n",
    "    \n",
    "    test_dataset = Dataset.from_pandas(df_test)\n",
    "    test_dataset = test_dataset.map(\n",
    "        lambda examples: formatting_prompts_func(examples), \n",
    "        batched=True\n",
    "    )\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd4ee819-dabb-406f-b404-17cd4b3bf0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers.utils import is_torch_bf16_gpu_available\n",
    "from get_dataset import build_dataset\n",
    "from config import LOCAL_MODEL_PATH, LORA_IN_PATH, LORA_OUT_PATH, RANK, MAX_SEQ_LENGTH, EPOCHS, TRAIN_DIR, MAX_ITER_STEPS, OUTPUT_PATH, RESUME_TRAINING\n",
    "import os\n",
    "\n",
    "# # ----------------------------\n",
    "# # Load model & tokenizer\n",
    "# # ----------------------------\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LOCAL_MODEL_PATH,\n",
    "    torch_dtype=\"auto\",       \n",
    ")\n",
    "\n",
    "if RESUME_TRAINING: \n",
    "    model = PeftModel.from_pretrained(model, LORA_IN_PATH, is_trainable=True)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.gradient_checkpointing_enable()  # reduce memory usage    \n",
    "\n",
    "# ----------------------------\n",
    "# Build datasets\n",
    "# ----------------------------\n",
    "train_dataset, test_dataset = build_dataset()\n",
    "\n",
    "# ----------------------------\n",
    "# LoRA config\n",
    "# ----------------------------\n",
    "if RESUME_TRAINING:    \n",
    "    lora_config = None       \n",
    "else:    \n",
    "    lora_config = LoraConfig(\n",
    "        r=RANK,\n",
    "        lora_alpha=64,\n",
    "        lora_dropout=0.0,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "        ],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    \n",
    "# ----------------------------\n",
    "# SFT config\n",
    "# ----------------------------\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=OUTPUT_PATH,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    max_steps=MAX_ITER_STEPS,\n",
    "    per_device_train_batch_size=4,       \n",
    "    gradient_accumulation_steps=4,       \n",
    "    max_length=min(MAX_SEQ_LENGTH, 4096),  \n",
    "\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "        \n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.05,\n",
    "        \n",
    "    bf16=is_torch_bf16_gpu_available(),\n",
    "    fp16=not is_torch_bf16_gpu_available(),\n",
    "    dataloader_pin_memory=True,\n",
    "        \n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "\n",
    "    warmup_steps=5,\n",
    "    logging_steps=10,\n",
    "    eval_steps=1000,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "\n",
    "    report_to=\"none\",        \n",
    "    packing=False,\n",
    "    remove_unused_columns=False,\n",
    "    dataset_text_field=\"text\",\n",
    "\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Trainer\n",
    "# ----------------------------\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=lora_config,\n",
    "    args=sft_config,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(os.path.join(LORA_OUT_PATH, TRAIN_DIR))\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1454531c-7a2d-47d9-aa14-43d75573c26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerate_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_config.yaml\n",
    "# \"\"\"\n",
    "# ZeRO Stage 1: Optimizer State Partitioning (2-4 GPU)\n",
    "# ZeRO Stage 2: + Gradient Partitioning (4-8 GPU)\n",
    "# ZeRO Stage 3: + Parameter Partitioning (8+ GPU)\n",
    "# \"\"\"\n",
    "\n",
    "compute_environment: LOCAL_MACHINE\n",
    "debug: false\n",
    "deepspeed_config:\n",
    "  gradient_accumulation_steps: 4\n",
    "  gradient_clipping: 1.0\n",
    "  train_batch_size: 16\n",
    "  train_micro_batch_size_per_gpu: 4\n",
    "  \n",
    "  zero_stage: 2\n",
    "  offload_optimizer_device: none\n",
    "  offload_param_device: none\n",
    "  zero3_init_flag: false\n",
    "  \n",
    "  stage3_gather_16bit_weights_on_model_save: false\n",
    "  stage3_max_live_parameters: 1e8\n",
    "  stage3_max_reuse_distance: 1e8\n",
    "  stage3_prefetch_bucket_size: 5e7\n",
    "  stage3_param_persistence_threshold: 1e5\n",
    "  \n",
    "  zero_allow_untested_optimizer: true\n",
    "  zero_force_ds_cpu_optimizer: false\n",
    "  \n",
    "  fp16:\n",
    "    enabled: true\n",
    "    loss_scale: 0\n",
    "    initial_scale_power: 16\n",
    "    loss_scale_window: 1000\n",
    "    hysteresis: 2\n",
    "    min_loss_scale: 1\n",
    "  \n",
    "distributed_type: DEEPSPEED\n",
    "downcast_bf16: 'no'\n",
    "dynamo_config:\n",
    "  dynamo_backend: INDUCTOR\n",
    "  dynamo_use_fullgraph: false\n",
    "  dynamo_use_dynamic: false\n",
    "enable_cpu_affinity: false\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "mixed_precision: fp16\n",
    "num_machines: 1\n",
    "num_processes: 1\n",
    "rdzv_backend: static\n",
    "same_network: true\n",
    "tpu_env: []\n",
    "tpu_use_cluster: false\n",
    "tpu_use_sudo: false\n",
    "use_cpu: false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d78e967-ce77-450c-b59d-0de3d002ed9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-17 08:21:37,935] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-09-17 08:21:41,591] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "TRAIN_DIR Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "/workspace/myvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/workspace/myvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/workspace/myvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "Found modules on cpu/disk. Using Exllama/Exllamav2 backend requires all the modules to be on GPU. Setting `disable_exllama=True`\n",
      "Concatenated 1 train files: ['./df_train.csv']\n",
      "Concatenated 1 test files: ['./df_test.csv']\n",
      "Train shape: (26899, 8)\n",
      "Test shape: (6449, 8)\n",
      "Index(['subreddit', 'rule', 'positive_example_1', 'negative_example_1',\n",
      "       'positive_example_2', 'negative_example_2', 'test_comment',\n",
      "       'violates_rule'],\n",
      "      dtype='object')\n",
      "Dropped 0 rows from train due to invalid 'violates_rule'\n",
      "Dropped 0 rows from test due to invalid 'violates_rule'\n",
      "subreddit: 0 rows would be dropped\n",
      "rule: 0 rows would be dropped\n",
      "positive_example_1: 0 rows would be dropped\n",
      "negative_example_1: 0 rows would be dropped\n",
      "positive_example_2: 0 rows would be dropped\n",
      "negative_example_2: 0 rows would be dropped\n",
      "test_comment: 0 rows would be dropped\n",
      "violates_rule: 0 rows would be dropped\n",
      "Using path: ./\n",
      "\n",
      " After dropping:\n",
      "Train shape: (26899, 8)\n",
      "Test shape: (6449, 8)\n",
      "\n",
      " After checking Yes/No:\n",
      "Train shape: (26899, 8)\n",
      "Test shape: (6449, 8)\n",
      "Map: 100%|██████████████████████| 26899/26899 [00:00<00:00, 84008.01 examples/s]\n",
      "Map: 100%|████████████████████████| 6449/6449 [00:00<00:00, 79712.46 examples/s]\n",
      "[2025-09-17 08:21:52,111] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-09-17 08:21:54,034] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "[2025-09-17 08:21:54,046] [INFO] [comm.py:821:init_distributed] cdb=None\n",
      "[2025-09-17 08:21:54,047] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "Adding EOS to train dataset: 100%|█| 26899/26899 [00:02<00:00, 10134.14 examples\n",
      "Tokenizing train dataset: 100%|██| 26899/26899 [00:24<00:00, 1108.74 examples/s]\n",
      "Truncating train dataset: 100%|█| 26899/26899 [00:00<00:00, 350540.72 examples/s\n",
      "Adding EOS to eval dataset: 100%|██| 6449/6449 [00:00<00:00, 9828.86 examples/s]\n",
      "Tokenizing eval dataset: 100%|█████| 6449/6449 [00:06<00:00, 1002.40 examples/s]\n",
      "Truncating eval dataset: 100%|███| 6449/6449 [00:00<00:00, 352651.38 examples/s]\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "[2025-09-17 08:22:32,232] [WARNING] [engine.py:1390:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n",
      "{'loss': 2.2934, 'grad_norm': 0.3836815357208252, 'learning_rate': 0.0001, 'entropy': 1.9364561885595322, 'num_tokens': 40943.0, 'mean_token_accuracy': 0.5657599844038487, 'epoch': 0.01}\n",
      "{'train_runtime': 39.2134, 'train_samples_per_second': 4.08, 'train_steps_per_second': 0.255, 'train_loss': 2.2934053421020506, 'epoch': 0.01}\n",
      "100%|███████████████████████████████████████████| 10/10 [00:39<00:00,  3.92s/it]\n",
      "success\n",
      "[rank0]:[W917 08:23:15.572364799 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "#!/workspace/myenev/bin/python3\n",
    "! ./myvenv/bin/accelerate launch --config_file accelerate_config.yaml train.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bbc3aac-c913-418d-b640-5a40b18935de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ~/.kaggle\n",
    "cp kaggle.json ~/.kaggle/\n",
    "chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64c6fd61-5a8f-45bf-afbe-4cfd8f476469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_DIR Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn\n",
      "Uploading Model https://www.kaggle.com/models/vinothkumarsekar89/qwen25-7b-gptq-int4-jigsaw-acrc-lora/transformers/01 ...\n",
      "Starting upload for file ./Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn/README.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████| 5.23k/5.23k [00:00<00:00, 5.68kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: ./Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn/README.md (5KB)\n",
      "Starting upload for file ./Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn/adapter_model.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████| 240M/240M [00:12<00:00, 18.8MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: ./Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn/adapter_model.safetensors (228MB)\n",
      "Starting upload for file ./Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn/adapter_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████| 945/945 [00:00<00:00, 1.03kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: ./Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn/adapter_config.json (945B)\n",
      "Starting upload for file ./Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn/chat_template.jinja\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████| 2.51k/2.51k [00:00<00:00, 2.54kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: ./Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn/chat_template.jinja (2KB)\n",
      "Starting upload for file ./Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████| 4.69k/4.69k [00:00<00:00, 5.08kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: ./Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn/tokenizer_config.json (5KB)\n",
      "Starting upload for file ./Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████| 613/613 [00:00<00:00, 633B/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: ./Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn/special_tokens_map.json (613B)\n",
      "Starting upload for file ./Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn/added_tokens.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████| 605/605 [00:01<00:00, 343B/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: ./Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn/added_tokens.json (605B)\n",
      "Starting upload for file ./Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn/vocab.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████| 2.78M/2.78M [00:02<00:00, 1.15MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: ./Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn/vocab.json (3MB)\n",
      "Starting upload for file ./Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn/merges.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████| 1.67M/1.67M [00:03<00:00, 556kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: ./Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn/merges.txt (2MB)\n",
      "Starting upload for file ./Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn/tokenizer.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████| 11.4M/11.4M [00:02<00:00, 3.95MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: ./Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn/tokenizer.json (11MB)\n",
      "Starting upload for file ./Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn/training_args.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████| 7.44k/7.44k [00:00<00:00, 7.71kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: ./Qwen2p5-3B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_-1_msl4096-1to9yn/training_args.bin (7KB)\n",
      "Your model instance has been created.\n",
      "Files are being processed...\n",
      "See at: https://www.kaggle.com/models/vinothkumarsekar89/qwen25-7b-gptq-int4-jigsaw-acrc-lora/transformers/01\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/workspace/myvenv/bin/python <<'EOF'\n",
    "import os\n",
    "import kagglehub\n",
    "from config import LORA_OUT_PATH, TRAIN_DIR, MODEL_SLUG, VARIATION_SLUG\n",
    "LOCAL_MODEL_DIR = os.path.join(LORA_OUT_PATH, TRAIN_DIR)\n",
    "\n",
    "kagglehub.model_upload(\n",
    "    handle=f\"vinothkumarsekar89/{MODEL_SLUG}/transformers/{VARIATION_SLUG}\",\n",
    "    local_model_dir=LOCAL_MODEL_DIR,\n",
    "    version_notes=\"LoRA Merged\"\n",
    ")\n",
    "EOF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf04ef5-2c69-46b9-8c9c-349328ee9408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11175052,
     "sourceId": 89659,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.756759,
   "end_time": "2025-02-26T02:09:27.363350",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-26T02:09:20.606591",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
