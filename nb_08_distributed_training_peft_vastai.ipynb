{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f506feee-98f0-4106-a53a-ccbff09b3ce3",
   "metadata": {},
   "source": [
    "# Jigsaw - Agile Community Rules Classification\n",
    "### https://www.kaggle.com/competitions/jigsaw-agile-community-rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fee25d6f-3e57-46f7-9d77-47f9a295a78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b22aad07-c794-49d5-b6ac-dc85f984187c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f62b246c-a5ee-407a-90ce-cbbac769c7a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ uv is already installed globally.\n",
      "✅ Virtual environment 'myvenv' already exists. Skipping creation.\n",
      "Activating virtual environment...\n",
      "Upgrading pip, setuptools, wheel, and uv inside the venv...\n",
      "Looking in links: /tmp/tmpea0z1jbk\n",
      "Requirement already satisfied: setuptools in ./myvenv/lib/python3.11/site-packages (80.9.0)\n",
      "Requirement already satisfied: pip in ./myvenv/lib/python3.11/site-packages (25.2)\n",
      "Requirement already satisfied: pip in ./myvenv/lib/python3.11/site-packages (25.2)\n",
      "Requirement already satisfied: setuptools in ./myvenv/lib/python3.11/site-packages (80.9.0)\n",
      "Requirement already satisfied: wheel in ./myvenv/lib/python3.11/site-packages (0.45.1)\n",
      "Requirement already satisfied: uv in ./myvenv/lib/python3.11/site-packages (0.8.18)\n",
      "Checking Python binary path:\n",
      "/data/myvenv/bin/python3\n",
      "Checking Python version:\n",
      "Python 3.11.13\n",
      "Checking uv version inside venv:\n",
      "uv 0.8.18\n",
      "✅ Virtual environment setup complete and up to date!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e  # exit immediately if a command fails\n",
    "\n",
    "VENV_DIR=\"myvenv\"\n",
    "\n",
    "# Step 1: Ensure uv is installed globally to create venv\n",
    "if ! command -v uv &> /dev/null; then\n",
    "  echo \"Installing uv globally to create venv...\"\n",
    "  python3 -m ensurepip --upgrade || true\n",
    "  python3 -m pip install --upgrade pip setuptools wheel\n",
    "  python3 -m pip install --upgrade uv\n",
    "else\n",
    "  echo \"✅ uv is already installed globally.\"\n",
    "fi\n",
    "\n",
    "# Step 2: Create virtual environment if it doesn't exist\n",
    "if [ -d \"$VENV_DIR\" ]; then\n",
    "  echo \"✅ Virtual environment '$VENV_DIR' already exists. Skipping creation.\"\n",
    "else\n",
    "  echo \"Creating virtual environment with uv...\"\n",
    "  uv venv \"$VENV_DIR\"\n",
    "fi\n",
    "\n",
    "# Step 3: Activate virtual environment\n",
    "echo \"Activating virtual environment...\"\n",
    "source \"$VENV_DIR/bin/activate\"\n",
    "\n",
    "# Step 4: Upgrade pip, setuptools, wheel, and uv *inside* the venv\n",
    "echo \"Upgrading pip, setuptools, wheel, and uv inside the venv...\"\n",
    "python3 -m ensurepip --upgrade || true\n",
    "python3 -m pip install --upgrade pip setuptools wheel --progress-bar=on\n",
    "python3 -m pip install --upgrade uv --progress-bar=on\n",
    "\n",
    "echo \"Checking Python binary path:\"\n",
    "which python3\n",
    "\n",
    "echo \"Checking Python version:\"\n",
    "python3 --version\n",
    "\n",
    "echo \"Checking uv version inside venv:\"\n",
    "uv --version\n",
    "\n",
    "echo \"✅ Virtual environment setup complete and up to date!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3999429b-938c-415d-8628-1c4bc03ba026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required libraries with uv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.13 environment at: myvenv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m8 packages\u001b[0m \u001b[2min 79ms\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done installing packages into myvenv using uv.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e  # Exit immediately if a command fails\n",
    "\n",
    "PYTHON=./myvenv/bin/python\n",
    "\n",
    "# echo \"Bootstrapping pip if needed...\"\n",
    "# $PYTHON -m ensurepip --upgrade || true\n",
    "# $PYTHON -m pip install --upgrade pip setuptools wheel\n",
    "\n",
    "# echo \"Installing uv...\"\n",
    "# $PYTHON -m pip install --upgrade uv\n",
    "\n",
    "echo \"Installing required libraries with uv...\"\n",
    "$PYTHON -m uv pip install \\\n",
    "  trl \\\n",
    "  optimum \\\n",
    "  auto-gptq \\\n",
    "  bitsandbytes \\\n",
    "  peft \\\n",
    "  accelerate \\\n",
    "  deepspeed \\\n",
    "  kagglehub\n",
    "\n",
    "echo \"✅ Done installing packages into myvenv using uv.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffacf4e8-e1fa-4b50-9f70-8cc9c2f2f966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.py\n",
    "\n",
    "RESUME_TRAINING=False\n",
    "#LOCAL_MODEL_PATH = \"Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4\"\n",
    "LOCAL_MODEL_PATH = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "LORA_IN_PATH= \"/workspace/lora\"\n",
    "LORA_OUT_PATH = \"/workspace\"\n",
    "DATA_PATH = \"./\"\n",
    "OUTPUT_PATH=\"/workspace\"\n",
    "\n",
    "# Training parameters\n",
    "MAX_SEQ_LENGTH = 3072\n",
    "RANK = 32\n",
    "LORA_ALPHA=32\n",
    "MAX_ITER_STEPS = -1\n",
    "EPOCHS = 2\n",
    "SAMPLE_LEN=\"35k\"\n",
    "\n",
    "# Kaggle upload configuration\n",
    "MODEL_SLUG = \"qwen25-3b-instruct-jigsaw-acrc-lora\"\n",
    "VARIATION_SLUG = \"17\"\n",
    "\n",
    "###--------------------------------###\n",
    "DATASET_ID=\"0-swap-cr12-kdsr1\"\n",
    "BASE_MODEL=LOCAL_MODEL_PATH.split(\"/\")[-1].replace(\".\", \"p\")\n",
    "TRAIN_DIR=f\"{BASE_MODEL}_lora_fp16_r{RANK}_s{SAMPLE_LEN}_e_{EPOCHS}_msl{MAX_SEQ_LENGTH}-{DATASET_ID}\"\n",
    "print(\"TRAIN_DIR\",TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b3fc1-46cb-4dd4-b7ba-f0140e144fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile get_dataset.py\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import kagglehub\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load Jigsaw ACRC dataset from Kaggle or local files\"\"\"\n",
    "    # Check if running on Kaggle\n",
    "    if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "        # Running on Kaggle\n",
    "        base_path = \"/kaggle/input/jigsaw-agile-community-rules/\"\n",
    "        df_train = pd.read_csv(f\"{base_path}*train*.csv\")\n",
    "        df_test = pd.read_csv(f\"{base_path}*test*.csv\")\n",
    "    else:\n",
    "        # Running locally\n",
    "        base_path = \"./\"\n",
    "        \n",
    "        # Find all train files\n",
    "        train_files = glob.glob(f\"{base_path}*train*.csv\")\n",
    "        if train_files:\n",
    "            train_dfs = [pd.read_csv(file) for file in train_files]\n",
    "            df_train = pd.concat(train_dfs, ignore_index=True)\n",
    "            print(f\"Concatenated {len(train_files)} train files: {train_files}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No train files found in {base_path}\")\n",
    "        \n",
    "        # Find all test files\n",
    "        test_files = glob.glob(f\"{base_path}*test*.csv\")\n",
    "        if test_files:\n",
    "            test_dfs = [pd.read_csv(file) for file in test_files]\n",
    "            df_test = pd.concat(test_dfs, ignore_index=True)\n",
    "            print(f\"Concatenated {len(test_files)} test files: {test_files}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No test files found in {base_path}\")\n",
    "\n",
    "    print(f\"Train shape: {df_train.shape}\")\n",
    "    print(f\"Test shape: {df_test.shape}\")\n",
    "    print(df_train.columns)\n",
    "            \n",
    "    req_cols=['subreddit', 'rule', 'positive_example_1', 'negative_example_1', 'positive_example_2',\n",
    "           'negative_example_2', 'test_comment', 'violates_rule']\n",
    "\n",
    "    df_train=df_train[req_cols]\n",
    "    df_test=df_test[req_cols]\n",
    "\n",
    "    # Normalize \"True\"/\"False\" -> \"Yes\"/\"No\" and drop anything else\n",
    "    for name, df in [(\"train\", df_train), (\"test\", df_test)]:\n",
    "        df[\"violates_rule\"] = (\n",
    "            df[\"violates_rule\"]\n",
    "            .astype(str).str.strip()\n",
    "            .map({\"True\": \"Yes\", \"False\": \"No\", \"Yes\": \"Yes\", \"No\": \"No\"})  # normalize\n",
    "        )\n",
    "        before = len(df)\n",
    "        df.dropna(subset=[\"violates_rule\"], inplace=True)  # drop rows with NaN (anything not Yes/No/True/False)\n",
    "        after = len(df)\n",
    "        print(f\"Dropped {before - after} rows from {name} due to invalid 'violates_rule'\")\n",
    "    \n",
    "    for col in req_cols:\n",
    "        dropped_rows = df_train[df_train[col].isna()].shape[0]\n",
    "        print(f\"{col}: {dropped_rows} rows would be dropped\")\n",
    "        \n",
    "    df_train = df_train[req_cols].dropna()\n",
    "    df_test = df_test[req_cols].dropna()\n",
    "\n",
    "\n",
    "    df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "    df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Using path: {base_path}\")\n",
    "    print(\"\\n After dropping:\")\n",
    "    print(f\"Train shape: {df_train.shape}\")\n",
    "    print(f\"Test shape: {df_test.shape}\")\n",
    "\n",
    "    df_train[\"violates_rule\"] = df_train[\"violates_rule\"].astype(str)\n",
    "    df_test[\"violates_rule\"] = df_test[\"violates_rule\"].astype(str)\n",
    "\n",
    "    valid_values = {\"Yes\", \"No\"}\n",
    "    df_train = df_train[df_train[\"violates_rule\"].isin(valid_values)]\n",
    "    df_test  = df_test[df_test[\"violates_rule\"].isin(valid_values)]\n",
    "    print(\"\\n After checking Yes/No:\")\n",
    "    print(f\"Train shape: {df_train.shape}\")\n",
    "    print(f\"Test shape: {df_test.shape}\")\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def formatting_prompts_func(examples, tokenizer):\n",
    "    \"\"\"\n",
    "    Format Reddit moderation dataset using tokenizer chat template\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for i in range(len(examples['subreddit'])):\n",
    "        # Create system message\n",
    "        system_msg = f\"\"\"You are a really experienced moderator for the subreddit /r/{examples['subreddit'][i]}. \n",
    "Your job is to determine if the following reported comment violates the given rule. Answer with only Yes or No.\"\"\"\n",
    "        \n",
    "        # Create user message with the rule and examples\n",
    "        user_msg = f\"\"\"<rule>\n",
    "{examples['rule'][i]}\n",
    "</rule>\n",
    "\n",
    "<examples>\n",
    "<example>\n",
    "<comment>{examples['positive_example_1'][i]}</comment>\n",
    "<rule_violation>Yes</rule_violation>\n",
    "</example>\n",
    "\n",
    "<example>\n",
    "<comment>{examples['positive_example_2'][i]}</comment>\n",
    "<rule_violation>Yes</rule_violation>\n",
    "</example>\n",
    "\n",
    "<example>\n",
    "<comment>{examples['negative_example_1'][i]}</comment>\n",
    "<rule_violation>No</rule_violation>\n",
    "</example>\n",
    "\n",
    "<example>\n",
    "<comment>{examples['negative_example_2'][i]}</comment>\n",
    "<rule_violation>No</rule_violation>\n",
    "</example>\n",
    "</examples>\n",
    "\n",
    "<test_comment>\n",
    "{examples['test_comment'][i]}\n",
    "</test_comment>\"\"\"\n",
    "        \n",
    "        # Assistant response is \"Yes\" or \"No\"\n",
    "        assistant_msg = examples['violates_rule'][i]\n",
    "\n",
    "        # Create messages list for chat template\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_msg}\n",
    "        ]\n",
    "\n",
    "\n",
    "        formatted_text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "\n",
    "        texts.append(formatted_text)\n",
    "\n",
    "    return {\"text\": texts}\n",
    "\n",
    "def build_dataset(tokenizer):\n",
    "    \"\"\"\n",
    "    Build both train and test datasets using tokenizer chat template\n",
    "    \"\"\"\n",
    "    df_train, df_test = load_data()\n",
    "    \n",
    "    train_dataset = Dataset.from_pandas(df_train)\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda examples: formatting_prompts_func(examples, tokenizer), \n",
    "        batched=True\n",
    "    )\n",
    "    \n",
    "    test_dataset = Dataset.from_pandas(df_test)\n",
    "    test_dataset = test_dataset.map(\n",
    "        lambda examples: formatting_prompts_func(examples, tokenizer), \n",
    "        batched=True\n",
    "    )\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ee819-dabb-406f-b404-17cd4b3bf0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers.utils import is_torch_bf16_gpu_available\n",
    "from get_dataset import build_dataset\n",
    "from config import LOCAL_MODEL_PATH, LORA_IN_PATH, LORA_OUT_PATH, RANK, MAX_SEQ_LENGTH, EPOCHS, TRAIN_DIR, MAX_ITER_STEPS, OUTPUT_PATH, RESUME_TRAINING, LORA_ALPHA\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# # ----------------------------\n",
    "# # Load model & tokenizer\n",
    "# # ----------------------------\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LOCAL_MODEL_PATH,\n",
    "    #torch_dtype=\"auto\",\n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,      # Reduce CPU memory during loading\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "if RESUME_TRAINING: \n",
    "    model = PeftModel.from_pretrained(model, LORA_IN_PATH, is_trainable=True)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.gradient_checkpointing_enable()  # reduce memory usage    \n",
    "\n",
    "# ----------------------------\n",
    "# Build datasets\n",
    "# ----------------------------\n",
    "train_dataset, test_dataset = build_dataset(tokenizer)\n",
    "\n",
    "# Add before training starts\n",
    "import gc\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# ----------------------------\n",
    "# LoRA config\n",
    "# ----------------------------\n",
    "if RESUME_TRAINING:    \n",
    "    lora_config = None       \n",
    "else:    \n",
    "    lora_config = LoraConfig(\n",
    "        r=RANK,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=0.0,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "        ],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    \n",
    "# ----------------------------\n",
    "# SFT config\n",
    "# ----------------------------\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=OUTPUT_PATH,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    max_steps=MAX_ITER_STEPS,\n",
    "    per_device_train_batch_size=4,       \n",
    "    gradient_accumulation_steps=4,       \n",
    "    max_length=min(MAX_SEQ_LENGTH, 4096),  \n",
    "\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "        \n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.05,\n",
    "        \n",
    "    bf16=is_torch_bf16_gpu_available(),\n",
    "    fp16=not is_torch_bf16_gpu_available(),\n",
    "    dataloader_pin_memory=True,\n",
    "        \n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "\n",
    "    warmup_steps=5,\n",
    "    logging_steps=10,\n",
    "    eval_steps=3000,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=1,\n",
    "\n",
    "    report_to=\"none\",        \n",
    "    packing=False,\n",
    "    remove_unused_columns=False,\n",
    "    dataset_text_field=\"text\",\n",
    "\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Trainer\n",
    "# ----------------------------\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=lora_config,\n",
    "    args=sft_config,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(os.path.join(LORA_OUT_PATH, TRAIN_DIR))\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1454531c-7a2d-47d9-aa14-43d75573c26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerate_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_config.yaml\n",
    "# \"\"\"\n",
    "# ZeRO Stage 1: Optimizer State Partitioning (2-4 GPU)\n",
    "# ZeRO Stage 2: + Gradient Partitioning (4-8 GPU)\n",
    "# ZeRO Stage 3: + Parameter Partitioning (8+ GPU)\n",
    "# \"\"\"\n",
    "\n",
    "compute_environment: LOCAL_MACHINE\n",
    "debug: false\n",
    "deepspeed_config:\n",
    "  gradient_accumulation_steps: 4\n",
    "  gradient_clipping: 1.0\n",
    "  train_batch_size: 16\n",
    "  train_micro_batch_size_per_gpu: 4\n",
    "  \n",
    "  zero_stage: 2\n",
    "  offload_optimizer_device: none\n",
    "  offload_param_device: none\n",
    "  zero3_init_flag: false\n",
    "  \n",
    "  stage3_gather_16bit_weights_on_model_save: false\n",
    "  stage3_max_live_parameters: 1e8\n",
    "  stage3_max_reuse_distance: 1e8\n",
    "  stage3_prefetch_bucket_size: 5e7\n",
    "  stage3_param_persistence_threshold: 1e5\n",
    "  \n",
    "  zero_allow_untested_optimizer: true\n",
    "  zero_force_ds_cpu_optimizer: false\n",
    "  \n",
    "  fp16:\n",
    "    enabled: true\n",
    "    loss_scale: 0\n",
    "    initial_scale_power: 16\n",
    "    loss_scale_window: 1000\n",
    "    hysteresis: 2\n",
    "    min_loss_scale: 1\n",
    "  \n",
    "distributed_type: DEEPSPEED\n",
    "downcast_bf16: 'no'\n",
    "dynamo_config:\n",
    "  dynamo_backend: INDUCTOR\n",
    "  dynamo_use_fullgraph: false\n",
    "  dynamo_use_dynamic: false\n",
    "enable_cpu_affinity: false\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "mixed_precision: fp16\n",
    "num_machines: 1\n",
    "num_processes: 1\n",
    "rdzv_backend: static\n",
    "same_network: true\n",
    "tpu_env: []\n",
    "tpu_use_cluster: false\n",
    "tpu_use_sudo: false\n",
    "use_cpu: false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bbc3aac-c913-418d-b640-5a40b18935de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ~/.kaggle\n",
    "cp kaggle.json ~/.kaggle/\n",
    "chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64c6fd61-5a8f-45bf-afbe-4cfd8f476469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing upload.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile upload.py\n",
    "import os\n",
    "import kagglehub\n",
    "from config import LORA_OUT_PATH, TRAIN_DIR, MODEL_SLUG, VARIATION_SLUG\n",
    "LOCAL_MODEL_DIR = os.path.join(LORA_OUT_PATH, TRAIN_DIR)\n",
    "\n",
    "kagglehub.model_upload(\n",
    "    handle=f\"vinothkumarsekar89/{MODEL_SLUG}/transformers/{VARIATION_SLUG}\",\n",
    "    local_model_dir=LOCAL_MODEL_DIR,\n",
    "    version_notes= TRAIN_DIR\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "baf04ef5-2c69-46b9-8c9c-349328ee9408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "./myvenv/bin/accelerate launch --config_file accelerate_config.yaml train.py\n",
    "./myvenv/bin/python upload.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab1a72a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "chmod +x run.sh\n",
    "nohup bash run.sh > log.log 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec7aeb34-a53f-49b2-8b2b-1d2fb5e28dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tail -f log.log"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11175052,
     "sourceId": 89659,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.756759,
   "end_time": "2025-02-26T02:09:27.363350",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-26T02:09:20.606591",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
