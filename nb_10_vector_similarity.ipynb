{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f506feee-98f0-4106-a53a-ccbff09b3ce3",
   "metadata": {},
   "source": [
    "# Jigsaw - Agile Community Rules Classification\n",
    "### https://www.kaggle.com/competitions/jigsaw-agile-community-rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9062a4b-55ec-493c-a31e-f03a3489a374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(251740, 4)\n",
      "(1000, 4)\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "base_path = \"./data/final/zothers/\"\n",
    "df_train = pd.read_csv(f\"{base_path}rule_comment.csv\")\n",
    "print(df_train.shape)\n",
    "df_train.head(1)\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "df_train = df_train.sample(n=1000)\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f059f042-25f6-4121-ba12-63e87c90b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"chunk\"] = \"rule: \" + df_train[\"rule\"] + \" comment: \" + df_train[\"test_comment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f3ef4ec-c8a0-4038-a4a2-8ff49e00e933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen3-Embedding-0.6B model...\n",
      "Encoding training data...\n",
      "Training vectors shape: (1000, 1024), dtype: float32\n",
      "Building FAISS index...\n",
      "Using GPU with 1 GPU(s)\n",
      "Loading and evaluating test data...\n",
      "      rule_violation  decision\n",
      "0                0.9         1\n",
      "1                0.5         0\n",
      "2                0.7         1\n",
      "3                0.8         1\n",
      "4                0.8         1\n",
      "...              ...       ...\n",
      "1995             0.3         0\n",
      "1996             0.2         0\n",
      "1997             0.0         0\n",
      "1998             0.3         0\n",
      "1999             0.1         0\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FastEmbedder:\n",
    "    \"\"\"Lightweight embedder using Qwen3-Embedding model\"\"\"\n",
    "    def __init__(self, model_name='Qwen/Qwen3-Embedding-0.6B', output_dim=512):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16  # Use FP16 for efficiency\n",
    "        )\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        self.output_dim = output_dim\n",
    "        self.max_length = 1024  # Qwen3 supports up to 32K, using 8K for efficiency\n",
    "    \n",
    "    def last_token_pool(self, last_hidden_states, attention_mask):\n",
    "        \"\"\"Pool using last token (EOS) as recommended for Qwen3\"\"\"\n",
    "        left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "        if left_padding:\n",
    "            return last_hidden_states[:, -1]\n",
    "        else:\n",
    "            sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "            batch_size = last_hidden_states.shape[0]\n",
    "            return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "    \n",
    "    def encode(self, texts, batch_size=32, convert_to_numpy=True, instruction=None):\n",
    "        \"\"\"\n",
    "        Encode texts to embeddings with optional instruction\n",
    "        \n",
    "        Args:\n",
    "            texts: List of texts to encode\n",
    "            batch_size: Batch size for encoding\n",
    "            convert_to_numpy: Whether to convert to numpy\n",
    "            instruction: Optional task instruction (e.g., \"Given a rule and comment, retrieve similar examples\")\n",
    "        \"\"\"\n",
    "        all_embeddings = []\n",
    "        \n",
    "        # Add instruction prefix if provided\n",
    "        if instruction:\n",
    "            texts = [f\"Instruct: {instruction}\\nQuery: {text}\" for text in texts]\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            encoded = self.tokenizer(\n",
    "                batch, \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                return_tensors='pt', \n",
    "                max_length=self.max_length\n",
    "            )\n",
    "            encoded = {k: v.to(self.device) for k, v in encoded.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**encoded)\n",
    "                \n",
    "                # Use last token pooling (recommended for Qwen3)\n",
    "                embeddings = self.last_token_pool(\n",
    "                    outputs.last_hidden_state, \n",
    "                    encoded['attention_mask']\n",
    "                )\n",
    "                \n",
    "                # Normalize embeddings\n",
    "                embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "                \n",
    "                # Truncate to desired dimension (Matryoshka)\n",
    "                if self.output_dim and self.output_dim < embeddings.shape[1]:\n",
    "                    embeddings = embeddings[:, :self.output_dim]\n",
    "                    # Renormalize after truncation\n",
    "                    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "                \n",
    "            # Convert to float32 for FAISS compatibility\n",
    "            all_embeddings.append(embeddings.cpu().float())\n",
    "        \n",
    "        embeddings = torch.cat(all_embeddings, dim=0)\n",
    "        \n",
    "        if convert_to_numpy:\n",
    "            return embeddings.numpy().astype(np.float32)\n",
    "        return embeddings\n",
    "\n",
    "def build_faiss_index(train_vectors, use_gpu=True):\n",
    "    \"\"\"Build FAISS index from training vectors\"\"\"\n",
    "    if not isinstance(train_vectors, np.ndarray):\n",
    "        train_vectors = train_vectors.cpu().numpy()\n",
    "    \n",
    "    # Ensure float32 dtype for FAISS\n",
    "    train_vectors = train_vectors.astype(np.float32)\n",
    "    \n",
    "    # Normalize for cosine similarity\n",
    "    faiss.normalize_L2(train_vectors)\n",
    "    \n",
    "    dimension = train_vectors.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    \n",
    "    if use_gpu and faiss.get_num_gpus() > 0:\n",
    "        print(f\"Using GPU with {faiss.get_num_gpus()} GPU(s)\")\n",
    "        res = faiss.StandardGpuResources()\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "    \n",
    "    index.add(train_vectors)\n",
    "    return index\n",
    "\n",
    "def evaluate_df_test(df_test, model, faiss_index, df_train_values, top_k=20, instruction=None):\n",
    "    \"\"\"Evaluate test dataframe using FAISS\"\"\"\n",
    "    # Create chunks\n",
    "    df_test[\"chunk\"] = \"rule: \" + df_test[\"rule\"] + \" comment: \" + df_test[\"test_comment\"]\n",
    "    \n",
    "    # Encode with instruction\n",
    "    new_vectors = model.encode(\n",
    "        df_test[\"chunk\"].tolist(), \n",
    "        convert_to_numpy=True,\n",
    "        instruction=instruction\n",
    "    )\n",
    "    \n",
    "    # Ensure float32 and normalize\n",
    "    new_vectors = new_vectors.astype(np.float32)\n",
    "    faiss.normalize_L2(new_vectors)\n",
    "    \n",
    "    # Search\n",
    "    similarities, indices = faiss_index.search(new_vectors, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for idx_row in indices:\n",
    "        top_values = df_train_values.iloc[idx_row]\n",
    "        avg_value = top_values.mean()\n",
    "        decision = 1 if avg_value > 0.5 else 0\n",
    "        results.append({\"rule_violation\": avg_value, \"decision\": decision})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading Qwen3-Embedding-0.6B model...\")\n",
    "    model = FastEmbedder('Qwen/Qwen3-Embedding-0.6B', output_dim=1024)\n",
    "    \n",
    "    # Define task instruction (recommended for Qwen3)\n",
    "    instruction = \"Given a rule and comment, retrieve similar training examples for classification\"\n",
    "    \n",
    "    print(\"Encoding training data...\")\n",
    "    df_train[\"chunk\"] = \"rule: \" + df_train[\"rule\"] + \" comment: \" + df_train[\"test_comment\"]\n",
    "    train_vectors = model.encode(\n",
    "        df_train[\"chunk\"].tolist(),\n",
    "        instruction=instruction\n",
    "    )\n",
    "    \n",
    "    print(f\"Training vectors shape: {train_vectors.shape}, dtype: {train_vectors.dtype}\")\n",
    "    \n",
    "    print(\"Building FAISS index...\")\n",
    "    faiss_index = build_faiss_index(train_vectors, use_gpu=True)\n",
    "    \n",
    "    print(\"Loading and evaluating test data...\")\n",
    "    df_test = pd.read_csv(\"./data/final/df_test_cr_12.csv\")\n",
    "    \n",
    "    result_df = evaluate_df_test(\n",
    "        df_test, \n",
    "        model, \n",
    "        faiss_index, \n",
    "        df_train[\"value\"], \n",
    "        top_k=10,\n",
    "        instruction=instruction\n",
    "    )\n",
    "    \n",
    "    print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51fc464c-8243-4441-8558-f309e3f36607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in violates_rule:\n",
      "['Yes' 'No']\n",
      "\n",
      "Unique values in decision:\n",
      "[1 0]\n",
      "\n",
      "After conversion:\n",
      "y_true unique: [1 0]\n",
      "y_pred unique: [1 0]\n",
      "\n",
      "y_true value counts:\n",
      "violates_rule\n",
      "1    1000\n",
      "0    1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "y_pred value counts:\n",
      "decision\n",
      "1    1087\n",
      "0     913\n",
      "Name: count, dtype: int64\n",
      "\n",
      "F1-Score: 0.8970\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Debug: Check what values actually exist\n",
    "print(\"Unique values in violates_rule:\")\n",
    "print(df_test[\"violates_rule\"].unique())\n",
    "print(\"\\nUnique values in decision:\")\n",
    "print(result_df[\"decision\"].unique())\n",
    "\n",
    "# More robust conversion\n",
    "y_true = df_test[\"violates_rule\"].astype(str).str.strip().str.strip('\"').str.strip(\"'\").str.lower().map({\"yes\": 1, \"no\": 0})\n",
    "y_pred = result_df[\"decision\"].astype(int)\n",
    "\n",
    "# Check after conversion\n",
    "print(\"\\nAfter conversion:\")\n",
    "print(f\"y_true unique: {y_true.unique()}\")\n",
    "print(f\"y_pred unique: {y_pred.unique()}\")\n",
    "print(f\"\\ny_true value counts:\\n{y_true.value_counts()}\")\n",
    "print(f\"\\ny_pred value counts:\\n{y_pred.value_counts()}\")\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f\"\\nF1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbd59a01-a9cf-4c98-b74e-3f56a1c6423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write to submissions.csv\n",
    "# df_test[\"rule_violation\"]=result_df[\"rule_violation\"].copy()\n",
    "# df_test[[\"row_id\",\"rule_violation\"]].to_csv(\"submission.csv\",index=False)\n",
    "# print(\"wrote results to submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11175052,
     "sourceId": 89659,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.756759,
   "end_time": "2025-02-26T02:09:27.363350",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-26T02:09:20.606591",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
