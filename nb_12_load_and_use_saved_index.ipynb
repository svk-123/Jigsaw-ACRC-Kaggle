{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Use Saved Index for Inference\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load a previously saved rule-filtered retrieval index\n",
    "2. Use it for fast inference on new data\n",
    "3. Evaluate performance without rebuilding the index\n",
    "\n",
    "**Note**: The index must be built first (see nb_11_rule_filtered_retrieval.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting rule_filtered_retrieval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile rule_filtered_retrieval.py\n",
    "\n",
    "\"\"\"\n",
    "Rule-Filtered Vector Similarity Retrieval System\n",
    "\n",
    "This system implements a two-stage retrieval:\n",
    "1. Filter by matching rules first\n",
    "2. Retrieve top K similar examples from filtered chunks using vector similarity\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class FastEmbedder:\n",
    "    \"\"\"Lightweight embedder using Qwen3-Embedding model\"\"\"\n",
    "    def __init__(self, model_name='Qwen/Qwen3-Embedding-0.6B', output_dim=512):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        self.output_dim = output_dim\n",
    "        self.max_length = 1024\n",
    "\n",
    "    def last_token_pool(self, last_hidden_states, attention_mask):\n",
    "        \"\"\"Pool using last token (EOS) as recommended for Qwen3\"\"\"\n",
    "        left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "        if left_padding:\n",
    "            return last_hidden_states[:, -1]\n",
    "        else:\n",
    "            sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "            batch_size = last_hidden_states.shape[0]\n",
    "            return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "    def encode(self, texts, batch_size=32, convert_to_numpy=True, instruction=None):\n",
    "        \"\"\"Encode texts to embeddings with optional instruction\"\"\"\n",
    "        all_embeddings = []\n",
    "\n",
    "        if instruction:\n",
    "            texts = [f\"Instruct: {instruction}\\nQuery: {text}\" for text in texts]\n",
    "\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            encoded = self.tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors='pt',\n",
    "                max_length=self.max_length\n",
    "            )\n",
    "            encoded = {k: v.to(self.device) for k, v in encoded.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**encoded)\n",
    "                embeddings = self.last_token_pool(\n",
    "                    outputs.last_hidden_state,\n",
    "                    encoded['attention_mask']\n",
    "                )\n",
    "                embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "                if self.output_dim and self.output_dim < embeddings.shape[1]:\n",
    "                    embeddings = embeddings[:, :self.output_dim]\n",
    "                    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "            all_embeddings.append(embeddings.cpu().float())\n",
    "\n",
    "        embeddings = torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "        if convert_to_numpy:\n",
    "            return embeddings.numpy().astype(np.float32)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class RuleFilteredRetriever:\n",
    "    \"\"\"\n",
    "    Two-stage retrieval system:\n",
    "    1. Filter by rule category\n",
    "    2. Retrieve top K similar examples from filtered set\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedder: FastEmbedder, use_gpu: bool = True):\n",
    "        self.embedder = embedder\n",
    "        self.use_gpu = use_gpu and faiss.get_num_gpus() > 0\n",
    "\n",
    "        # Store rule categories and their indices\n",
    "        self.rule_to_indices = defaultdict(list)\n",
    "        self.rule_to_faiss_index = {}\n",
    "        self.rule_to_data = {}\n",
    "\n",
    "        # Global storage\n",
    "        self.all_data = None\n",
    "        self.all_vectors = None\n",
    "\n",
    "    def build_index(self, df: pd.DataFrame, rule_col: str = 'rule',\n",
    "                   comment_col: str = 'test_comment',\n",
    "                   value_col: str = 'value',\n",
    "                   instruction: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Build FAISS indices per rule category\n",
    "        Uses ONLY comment embeddings for similarity search within each rule\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame with rules, comments, and values\n",
    "            rule_col: Column name for rules\n",
    "            comment_col: Column name for comments\n",
    "            value_col: Column name for target values\n",
    "            instruction: Optional instruction for embeddings\n",
    "        \"\"\"\n",
    "        print(f\"Building rule-filtered index from {len(df)} examples...\")\n",
    "        print(\"Strategy: Filter by rule FIRST, then search by COMMENT similarity only\")\n",
    "\n",
    "        # Store data\n",
    "        df = df.copy()\n",
    "        self.all_data = df\n",
    "\n",
    "        # Group by rule\n",
    "        rule_groups = df.groupby(rule_col)\n",
    "        print(f\"Found {len(rule_groups)} unique rules\")\n",
    "\n",
    "        # Encode ONLY comments for similarity search (not rule+comment combined)\n",
    "        print(\"Encoding comments only (not combined with rules)...\")\n",
    "        all_vectors = self.embedder.encode(\n",
    "            df[comment_col].tolist(),\n",
    "            instruction=instruction\n",
    "        )\n",
    "        faiss.normalize_L2(all_vectors)\n",
    "        self.all_vectors = all_vectors\n",
    "\n",
    "        # Build separate FAISS index for each rule\n",
    "        print(\"Building per-rule FAISS indices...\")\n",
    "        for rule, group_df in rule_groups:\n",
    "            indices = group_df.index.tolist()\n",
    "            rule_vectors = all_vectors[indices]\n",
    "\n",
    "            # Store mapping\n",
    "            self.rule_to_indices[rule] = indices\n",
    "            self.rule_to_data[rule] = group_df\n",
    "\n",
    "            # Build FAISS index for this rule using ONLY comment vectors\n",
    "            dimension = rule_vectors.shape[1]\n",
    "            index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "            if self.use_gpu:\n",
    "                res = faiss.StandardGpuResources()\n",
    "                index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "\n",
    "            index.add(rule_vectors)\n",
    "            self.rule_to_faiss_index[rule] = index\n",
    "\n",
    "            print(f\"  Rule '{rule[:50]}...' : {len(indices)} examples\")\n",
    "\n",
    "        print(f\"\\nIndex building complete!\")\n",
    "        print(f\"Total rules: {len(self.rule_to_faiss_index)}\")\n",
    "        print(f\"Total examples: {len(df)}\")\n",
    "\n",
    "    def retrieve(self, query_rule: str, query_comment: str,\n",
    "                top_k: int = 10,\n",
    "                instruction: Optional[str] = None,\n",
    "                fallback_to_similar_rules: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Two-stage retrieval:\n",
    "        1. Filter by rule (exact or similar)\n",
    "        2. Search by comment similarity ONLY within filtered rule\n",
    "\n",
    "        Args:\n",
    "            query_rule: The rule to match\n",
    "            query_comment: The comment to find similar examples for\n",
    "            top_k: Number of similar examples to retrieve\n",
    "            instruction: Optional instruction for encoding\n",
    "            fallback_to_similar_rules: If exact rule not found, use rule similarity\n",
    "\n",
    "        Returns:\n",
    "            Dict with retrieved examples and metadata\n",
    "        \"\"\"\n",
    "        # STAGE 1: Filter by rule first\n",
    "        # Check if exact rule exists\n",
    "        if query_rule in self.rule_to_faiss_index:\n",
    "            matched_rule = query_rule\n",
    "            rule_match_type = \"exact\"\n",
    "        elif fallback_to_similar_rules:\n",
    "            # Find most similar rule using rule text similarity\n",
    "            matched_rule, rule_match_type = self._find_similar_rule(query_rule, instruction)\n",
    "        else:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': f'Rule not found: {query_rule}',\n",
    "                'matched_rule': None,\n",
    "                'examples': []\n",
    "            }\n",
    "\n",
    "        # STAGE 2: Encode ONLY the comment for similarity search\n",
    "        query_vector = self.embedder.encode(\n",
    "            [query_comment],  # Only comment, no rule!\n",
    "            instruction=instruction\n",
    "        )\n",
    "        faiss.normalize_L2(query_vector)\n",
    "\n",
    "        # Get FAISS index for matched rule (contains only comment embeddings)\n",
    "        rule_index = self.rule_to_faiss_index[matched_rule]\n",
    "        rule_data = self.rule_to_data[matched_rule]\n",
    "\n",
    "        # Search in filtered index using comment similarity\n",
    "        similarities, indices = rule_index.search(query_vector, min(top_k, len(rule_data)))\n",
    "\n",
    "        # Get actual dataframe indices\n",
    "        actual_indices = [self.rule_to_indices[matched_rule][i] for i in indices[0]]\n",
    "        retrieved_examples = self.all_data.iloc[actual_indices]\n",
    "\n",
    "        return {\n",
    "            'success': True,\n",
    "            'matched_rule': matched_rule,\n",
    "            'rule_match_type': rule_match_type,\n",
    "            'query_rule': query_rule,\n",
    "            'query_comment': query_comment,\n",
    "            'similarities': similarities[0].tolist(),\n",
    "            'examples': retrieved_examples.to_dict('records'),\n",
    "            'num_examples_in_rule': len(rule_data)\n",
    "        }\n",
    "\n",
    "    def _find_similar_rule(self, query_rule: str, instruction: Optional[str] = None) -> Tuple[str, str]:\n",
    "        \"\"\"Find the most similar rule to query_rule using embedding similarity\"\"\"\n",
    "        # Encode query rule\n",
    "        query_rule_vector = self.embedder.encode(\n",
    "            [f\"rule: {query_rule}\"],\n",
    "            instruction=instruction\n",
    "        )\n",
    "        faiss.normalize_L2(query_rule_vector)\n",
    "\n",
    "        # Encode all unique rules\n",
    "        unique_rules = list(self.rule_to_faiss_index.keys())\n",
    "        rule_vectors = self.embedder.encode(\n",
    "            [f\"rule: {r}\" for r in unique_rules],\n",
    "            instruction=instruction\n",
    "        )\n",
    "        faiss.normalize_L2(rule_vectors)\n",
    "\n",
    "        # Find most similar\n",
    "        similarities = np.dot(query_rule_vector, rule_vectors.T)[0]\n",
    "        best_idx = np.argmax(similarities)\n",
    "        best_rule = unique_rules[best_idx]\n",
    "\n",
    "        return best_rule, f\"similar (similarity: {similarities[best_idx]:.3f})\"\n",
    "\n",
    "    def batch_retrieve(self, df_query: pd.DataFrame,\n",
    "                      rule_col: str = 'rule',\n",
    "                      comment_col: str = 'test_comment',\n",
    "                      top_k: int = 10,\n",
    "                      instruction: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Batch retrieval for multiple queries\n",
    "\n",
    "        Args:\n",
    "            df_query: DataFrame with query rules and comments\n",
    "            rule_col: Column name for rules\n",
    "            comment_col: Column name for comments\n",
    "            top_k: Number of examples to retrieve per query\n",
    "            instruction: Optional instruction for encoding\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with predictions\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        for idx, row in df_query.iterrows():\n",
    "            query_rule = row[rule_col]\n",
    "            query_comment = row[comment_col]\n",
    "\n",
    "            # Retrieve examples\n",
    "            retrieval_result = self.retrieve(\n",
    "                query_rule=query_rule,\n",
    "                query_comment=query_comment,\n",
    "                top_k=top_k,\n",
    "                instruction=instruction\n",
    "            )\n",
    "\n",
    "            if retrieval_result['success']:\n",
    "                # Calculate weighted sum of similarity scores with values\n",
    "                examples = retrieval_result['examples']\n",
    "                similarities = retrieval_result['similarities']\n",
    "                values = [ex['value'] for ex in examples]\n",
    "\n",
    "                # Weighted sum: multiply each similarity by its value and sum\n",
    "                weighted_sum = sum(sim * val for sim, val in zip(similarities, values))\n",
    "\n",
    "                # Decision based on sign of weighted sum\n",
    "                decision = 1 if weighted_sum >= 0 else 0\n",
    "                avg_violation = weighted_sum  # Store weighted sum for inspection\n",
    "\n",
    "                results.append({\n",
    "                    'row_id': idx,\n",
    "                    'rule_violation': avg_violation,\n",
    "                    'decision': decision,\n",
    "                    'matched_rule': retrieval_result['matched_rule'],\n",
    "                    'rule_match_type': retrieval_result['rule_match_type'],\n",
    "                    'num_retrieved': len(examples)\n",
    "                })\n",
    "            else:\n",
    "                # Fallback: return neutral prediction\n",
    "                results.append({\n",
    "                    'row_id': idx,\n",
    "                    'rule_violation': 0.5,\n",
    "                    'decision': 0,\n",
    "                    'matched_rule': None,\n",
    "                    'rule_match_type': 'not_found',\n",
    "                    'num_retrieved': 0\n",
    "                })\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def save_index(self, save_dir: str):\n",
    "        \"\"\"\n",
    "        Save the retriever indices and data to disk\n",
    "\n",
    "        Args:\n",
    "            save_dir: Directory to save the index files\n",
    "        \"\"\"\n",
    "        save_path = Path(save_dir)\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(f\"Saving index to {save_dir}...\")\n",
    "\n",
    "        # Save FAISS indices (convert GPU to CPU first if needed)\n",
    "        faiss_dir = save_path / \"faiss_indices\"\n",
    "        faiss_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        for rule, index in self.rule_to_faiss_index.items():\n",
    "            # Convert to CPU index if it's a GPU index\n",
    "            if self.use_gpu:\n",
    "                cpu_index = faiss.index_gpu_to_cpu(index)\n",
    "            else:\n",
    "                cpu_index = index\n",
    "\n",
    "            # Create safe filename from rule\n",
    "            safe_filename = f\"index_{hash(rule) % 10**8}.faiss\"\n",
    "            faiss.write_index(cpu_index, str(faiss_dir / safe_filename))\n",
    "\n",
    "        # Save metadata (rule mappings)\n",
    "        metadata = {\n",
    "            'rule_to_indices': dict(self.rule_to_indices),\n",
    "            'rule_to_filename': {rule: f\"index_{hash(rule) % 10**8}.faiss\"\n",
    "                                 for rule in self.rule_to_faiss_index.keys()},\n",
    "            'use_gpu': self.use_gpu\n",
    "        }\n",
    "\n",
    "        with open(save_path / \"metadata.pkl\", 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "\n",
    "        # Save training data\n",
    "        if self.all_data is not None:\n",
    "            self.all_data.to_parquet(save_path / \"training_data.parquet\", index=True)\n",
    "\n",
    "        # Save embeddings\n",
    "        if self.all_vectors is not None:\n",
    "            np.save(save_path / \"embeddings.npy\", self.all_vectors)\n",
    "\n",
    "        # Save rule_to_data mapping\n",
    "        rule_data_mapping = {rule: df.index.tolist() for rule, df in self.rule_to_data.items()}\n",
    "        with open(save_path / \"rule_data_mapping.pkl\", 'wb') as f:\n",
    "            pickle.dump(rule_data_mapping, f)\n",
    "\n",
    "        print(f\"âœ“ Index saved successfully!\")\n",
    "        print(f\"  - {len(self.rule_to_faiss_index)} FAISS indices\")\n",
    "        print(f\"  - {len(self.all_data)} training examples\")\n",
    "        print(f\"  - Embeddings shape: {self.all_vectors.shape}\")\n",
    "\n",
    "    def load_index(self, load_dir: str):\n",
    "        \"\"\"\n",
    "        Load a previously saved index from disk\n",
    "\n",
    "        Args:\n",
    "            load_dir: Directory containing the saved index files\n",
    "        \"\"\"\n",
    "        load_path = Path(load_dir)\n",
    "\n",
    "        if not load_path.exists():\n",
    "            raise FileNotFoundError(f\"Index directory not found: {load_dir}\")\n",
    "\n",
    "        print(f\"Loading index from {load_dir}...\")\n",
    "\n",
    "        # Load metadata\n",
    "        with open(load_path / \"metadata.pkl\", 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "\n",
    "        self.rule_to_indices = defaultdict(list, metadata['rule_to_indices'])\n",
    "        rule_to_filename = metadata['rule_to_filename']\n",
    "        saved_use_gpu = metadata['use_gpu']\n",
    "\n",
    "        # Load training data\n",
    "        if (load_path / \"training_data.parquet\").exists():\n",
    "            self.all_data = pd.read_parquet(load_path / \"training_data.parquet\")\n",
    "\n",
    "        # Load embeddings\n",
    "        if (load_path / \"embeddings.npy\").exists():\n",
    "            self.all_vectors = np.load(load_path / \"embeddings.npy\")\n",
    "\n",
    "        # Load rule_to_data mapping\n",
    "        with open(load_path / \"rule_data_mapping.pkl\", 'rb') as f:\n",
    "            rule_data_mapping = pickle.load(f)\n",
    "\n",
    "        # Reconstruct rule_to_data from mapping\n",
    "        self.rule_to_data = {}\n",
    "        for rule, indices in rule_data_mapping.items():\n",
    "            self.rule_to_data[rule] = self.all_data.loc[indices]\n",
    "\n",
    "        # Load FAISS indices\n",
    "        faiss_dir = load_path / \"faiss_indices\"\n",
    "        self.rule_to_faiss_index = {}\n",
    "\n",
    "        for rule, filename in rule_to_filename.items():\n",
    "            # Load CPU index\n",
    "            cpu_index = faiss.read_index(str(faiss_dir / filename))\n",
    "\n",
    "            # Convert to GPU if requested and available\n",
    "            if self.use_gpu and faiss.get_num_gpus() > 0:\n",
    "                res = faiss.StandardGpuResources()\n",
    "                gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n",
    "                self.rule_to_faiss_index[rule] = gpu_index\n",
    "            else:\n",
    "                self.rule_to_faiss_index[rule] = cpu_index\n",
    "\n",
    "        print(f\"âœ“ Index loaded successfully!\")\n",
    "        print(f\"  - {len(self.rule_to_faiss_index)} FAISS indices\")\n",
    "        print(f\"  - {len(self.all_data)} training examples\")\n",
    "        print(f\"  - Embeddings shape: {self.all_vectors.shape}\")\n",
    "        print(f\"  - Using GPU: {self.use_gpu}\")\n",
    "\n",
    "\n",
    "def main_example():\n",
    "    \"\"\"Example usage of RuleFilteredRetriever\"\"\"\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    base_path = \"./data/final/zothers/\"\n",
    "    df_train = pd.read_csv(f\"{base_path}rule_comment.csv\")\n",
    "    df_train = df_train.sample(n=5000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Training data shape: {df_train.shape}\")\n",
    "    print(f\"Unique rules: {df_train['rule'].nunique()}\")\n",
    "\n",
    "    # Initialize embedder\n",
    "    print(\"\\nInitializing embedder...\")\n",
    "    embedder = FastEmbedder('Qwen/Qwen3-Embedding-0.6B', output_dim=1024)\n",
    "\n",
    "    # Initialize retriever\n",
    "    print(\"\\nInitializing retriever...\")\n",
    "    retriever = RuleFilteredRetriever(embedder, use_gpu=True)\n",
    "\n",
    "    # Build index\n",
    "    instruction = \"Given a rule and comment, retrieve similar training examples\"\n",
    "    retriever.build_index(\n",
    "        df_train,\n",
    "        rule_col='rule',\n",
    "        comment_col='test_comment',\n",
    "        value_col='value',\n",
    "        instruction=instruction\n",
    "    )\n",
    "\n",
    "    # Test single query\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Testing single query retrieval...\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    test_row = df_train.iloc[0]\n",
    "    result = retriever.retrieve(\n",
    "        query_rule=test_row['rule'],\n",
    "        query_comment=test_row['test_comment'],\n",
    "        top_k=5,\n",
    "        instruction=instruction\n",
    "    )\n",
    "\n",
    "    print(f\"\\nQuery Rule: {result['query_rule'][:100]}...\")\n",
    "    print(f\"Query Comment: {result['query_comment'][:100]}...\")\n",
    "    print(f\"\\nMatched Rule: {result['matched_rule'][:100]}...\")\n",
    "    print(f\"Rule Match Type: {result['rule_match_type']}\")\n",
    "    print(f\"Retrieved {len(result['examples'])} examples\")\n",
    "    print(f\"\\nTop 3 similarities: {result['similarities'][:3]}\")\n",
    "\n",
    "    # Test batch retrieval\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Testing batch retrieval...\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    df_test = pd.read_csv(\"./data/final/df_test_cr_12.csv\")\n",
    "    df_test_sample = df_test.sample(n=100, random_state=42)\n",
    "\n",
    "    results_df = retriever.batch_retrieve(\n",
    "        df_test_sample,\n",
    "        rule_col='rule',\n",
    "        comment_col='test_comment',\n",
    "        top_k=10,\n",
    "        instruction=instruction\n",
    "    )\n",
    "\n",
    "    print(f\"\\nBatch retrieval results:\")\n",
    "    print(results_df.head(10))\n",
    "\n",
    "    # Evaluate\n",
    "    if 'violates_rule' in df_test_sample.columns:\n",
    "        from sklearn.metrics import f1_score\n",
    "\n",
    "        y_true = df_test_sample[\"violates_rule\"].astype(str).str.strip().str.strip('\"').str.strip(\"'\").str.lower().map({\"yes\": 1, \"no\": 0})\n",
    "        y_pred = results_df[\"decision\"].values\n",
    "\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        print(f\"\\nF1-Score: {f1:.4f}\")\n",
    "\n",
    "        # Match statistics\n",
    "        print(f\"\\nRule matching statistics:\")\n",
    "        print(results_df['rule_match_type'].value_counts())\n",
    "\n",
    "    return retriever, results_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    retriever, results = main_example()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rule_filtered_retrieval import FastEmbedder, RuleFilteredRetriever\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Embedder and Retriever\n",
    "\n",
    "**IMPORTANT**: Use the SAME model and output_dim as when building the index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen3-Embedding model...\n",
      "Embedder initialized on device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedder (must match the one used during index building)\n",
    "print(\"Loading Qwen3-Embedding model...\")\n",
    "embedder = FastEmbedder(\n",
    "    model_name='Qwen/Qwen3-Embedding-0.6B',\n",
    "    output_dim=1024  # MUST match what was used during build\n",
    ")\n",
    "\n",
    "print(f\"Embedder initialized on device: {embedder.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing retriever...\n",
      "âœ“ Retriever initialized (index not loaded yet)\n"
     ]
    }
   ],
   "source": [
    "# Initialize retriever\n",
    "print(\"Initializing retriever...\")\n",
    "retriever = RuleFilteredRetriever(embedder, use_gpu=True)\n",
    "\n",
    "print(\"âœ“ Retriever initialized (index not loaded yet)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Saved Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index from ./saved_indices/rule_filtered_sample...\n",
      "Loading index from ./saved_indices/rule_filtered_sample...\n",
      "âœ“ Index loaded successfully!\n",
      "  - 6 FAISS indices\n",
      "  - 251739 training examples\n",
      "  - Embeddings shape: (251739, 1024)\n",
      "  - Using GPU: True\n",
      "\n",
      "âœ“ Index loaded in 1.32 seconds\n",
      "  Total rules: 6\n",
      "  Total examples: 251739\n"
     ]
    }
   ],
   "source": [
    "# Path to saved index (choose one)\n",
    "INDEX_DIR = \"./saved_indices/rule_filtered_sample\"  # Sample index (10K examples)\n",
    "# INDEX_DIR = \"./saved_indices/rule_filtered_full\"  # Full index (250K examples)\n",
    "\n",
    "print(f\"Loading index from {INDEX_DIR}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "retriever.load_index(INDEX_DIR)\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"\\nâœ“ Index loaded in {load_time:.2f} seconds\")\n",
    "print(f\"  Total rules: {len(retriever.rule_to_faiss_index)}\")\n",
    "print(f\"  Total examples: {len(retriever.all_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Single Query Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Rule: No medical advice: do not offer or request specific medical advice.\n",
      "\n",
      "Query Comment: You should take 500mg of ibuprofen every 4 hours for that headache.\n",
      "\n",
      "================================================================================\n",
      "RETRIEVAL RESULTS\n",
      "================================================================================\n",
      "Matched Rule Type: similar (similarity: 0.964)\n",
      "Number of examples in this rule category: 45965\n",
      "Retrieved: 10 examples\n",
      "\n",
      "Top 5 Similarities:\n",
      "  1. Similarity: 0.8947, Value:  1, Comment: You should definitely take 500mg of ibuprofen every 6 hours for that TV-induced ...\n",
      "  2. Similarity: 0.8942, Value:  1, Comment: You should definitely try taking 500mg of ibuprofen every 6 hours for that persi...\n",
      "  3. Similarity: 0.8820, Value:  1, Comment: You should definitely start taking 500mg of ibuprofen every 6 hours for that hea...\n",
      "  4. Similarity: 0.8729, Value:  1, Comment: You should definitely try taking 800mg of ibuprofen every 6 hours for that heada...\n",
      "  5. Similarity: 0.8723, Value:  1, Comment: Dude you should definitely take 800mg of ibuprofen every 6 hours for that headac...\n",
      "\n",
      "Predicted Value: 1.000\n",
      "Predicted Decision: Violation\n"
     ]
    }
   ],
   "source": [
    "# Define instruction for retrieval\n",
    "instruction = \"Given a rule and comment, retrieve similar training examples for classification\"\n",
    "\n",
    "# Test with a custom example\n",
    "test_rule = \"No medical advice: do not offer or request specific medical advice.\"\n",
    "test_comment = \"You should take 500mg of ibuprofen every 4 hours for that headache.\"\n",
    "\n",
    "print(f\"Query Rule: {test_rule}\")\n",
    "print(f\"\\nQuery Comment: {test_comment}\")\n",
    "\n",
    "# Retrieve similar examples\n",
    "result = retriever.retrieve(\n",
    "    query_rule=test_rule,\n",
    "    query_comment=test_comment,\n",
    "    top_k=10,\n",
    "    instruction=instruction\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RETRIEVAL RESULTS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Matched Rule Type: {result['rule_match_type']}\")\n",
    "print(f\"Number of examples in this rule category: {result['num_examples_in_rule']}\")\n",
    "print(f\"Retrieved: {len(result['examples'])} examples\")\n",
    "\n",
    "print(f\"\\nTop 5 Similarities:\")\n",
    "for i, (sim, ex) in enumerate(zip(result['similarities'][:5], result['examples'][:5])):\n",
    "    print(f\"  {i+1}. Similarity: {sim:.4f}, Value: {ex['value']:>2}, Comment: {ex['test_comment'][:80]}...\")\n",
    "\n",
    "# Calculate prediction\n",
    "retrieved_values = [ex['value'] for ex in result['examples']]\n",
    "predicted_value = np.mean(retrieved_values)\n",
    "predicted_decision = 1 if predicted_value > 0 else 0\n",
    "\n",
    "print(f\"\\nPredicted Value: {predicted_value:.3f}\")\n",
    "print(f\"Predicted Decision: {'Violation' if predicted_decision == 1 else 'No Violation'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 2,029\n",
      "Unique rules in test: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>body</th>\n",
       "      <th>rule</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>positive_example_1</th>\n",
       "      <th>positive_example_2</th>\n",
       "      <th>negative_example_1</th>\n",
       "      <th>negative_example_2</th>\n",
       "      <th>rule_violation</th>\n",
       "      <th>violates_rule</th>\n",
       "      <th>test_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Banks don't want you to know this! Click here ...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>Futurology</td>\n",
       "      <td>If you could tell your younger self something ...</td>\n",
       "      <td>hunt for lady for jack off in neighbourhood ht...</td>\n",
       "      <td>Watch Golden Globe Awards 2017 Live Online in ...</td>\n",
       "      <td>DOUBLE CEE x BANDS EPPS - \"BIRDS\"\\n\\nDOWNLOAD/...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Banks don't want you to know this! Click here ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>SD Stream [ ENG Link 1] (http://www.sportsstre...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>soccerstreams</td>\n",
       "      <td>[I wanna kiss you all over! Stunning!](http://...</td>\n",
       "      <td>LOLGA.COM is One of the First Professional Onl...</td>\n",
       "      <td>#Rapper \\nðŸš¨Straight Outta Cross Keys SC ðŸš¨YouTu...</td>\n",
       "      <td>[15 Amazing Hidden Features Of Google Search Y...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>SD Stream [ ENG Link 1] (http://www.sportsstre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Lol. Try appealing the ban and say you won't d...</td>\n",
       "      <td>No legal advice: Do not offer or request legal...</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>Don't break up with him or call the cops.  If ...</td>\n",
       "      <td>It'll be dismissed: https://en.wikipedia.org/w...</td>\n",
       "      <td>Where is there a site that still works where y...</td>\n",
       "      <td>Because this statement of his is true. It isn'...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>Lol. Try appealing the ban and say you won't d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>she will come your home open her legs with  an...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>sex</td>\n",
       "      <td>Selling Tyrande codes for 3â‚¬ to paypal. PM. \\n...</td>\n",
       "      <td>tight pussy watch for your cock get her at thi...</td>\n",
       "      <td>NSFW(obviously) http://spankbang.com/iy3u/vide...</td>\n",
       "      <td>Good News ::Download WhatsApp 2.16.230 APK for...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>she will come your home open her legs with  an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>code free tyrande ---&gt;&gt;&gt; [Imgur](http://i.imgu...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>hearthstone</td>\n",
       "      <td>wow!! amazing reminds me of the old days.Well...</td>\n",
       "      <td>seek for lady for sex in around http://p77.pl/...</td>\n",
       "      <td>must be watch movie https://sites.google.com/s...</td>\n",
       "      <td>We're streaming Pokemon Veitnamese Crystal RIG...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>code free tyrande ---&gt;&gt;&gt; [Imgur](http://i.imgu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id                                               body  \\\n",
       "0       0  Banks don't want you to know this! Click here ...   \n",
       "1       1  SD Stream [ ENG Link 1] (http://www.sportsstre...   \n",
       "2       2  Lol. Try appealing the ban and say you won't d...   \n",
       "3       3  she will come your home open her legs with  an...   \n",
       "4       4  code free tyrande --->>> [Imgur](http://i.imgu...   \n",
       "\n",
       "                                                rule      subreddit  \\\n",
       "0  No Advertising: Spam, referral links, unsolici...     Futurology   \n",
       "1  No Advertising: Spam, referral links, unsolici...  soccerstreams   \n",
       "2  No legal advice: Do not offer or request legal...   pcmasterrace   \n",
       "3  No Advertising: Spam, referral links, unsolici...            sex   \n",
       "4  No Advertising: Spam, referral links, unsolici...    hearthstone   \n",
       "\n",
       "                                  positive_example_1  \\\n",
       "0  If you could tell your younger self something ...   \n",
       "1  [I wanna kiss you all over! Stunning!](http://...   \n",
       "2  Don't break up with him or call the cops.  If ...   \n",
       "3  Selling Tyrande codes for 3â‚¬ to paypal. PM. \\n...   \n",
       "4   wow!! amazing reminds me of the old days.Well...   \n",
       "\n",
       "                                  positive_example_2  \\\n",
       "0  hunt for lady for jack off in neighbourhood ht...   \n",
       "1  LOLGA.COM is One of the First Professional Onl...   \n",
       "2  It'll be dismissed: https://en.wikipedia.org/w...   \n",
       "3  tight pussy watch for your cock get her at thi...   \n",
       "4  seek for lady for sex in around http://p77.pl/...   \n",
       "\n",
       "                                  negative_example_1  \\\n",
       "0  Watch Golden Globe Awards 2017 Live Online in ...   \n",
       "1  #Rapper \\nðŸš¨Straight Outta Cross Keys SC ðŸš¨YouTu...   \n",
       "2  Where is there a site that still works where y...   \n",
       "3  NSFW(obviously) http://spankbang.com/iy3u/vide...   \n",
       "4  must be watch movie https://sites.google.com/s...   \n",
       "\n",
       "                                  negative_example_2  rule_violation  \\\n",
       "0  DOUBLE CEE x BANDS EPPS - \"BIRDS\"\\n\\nDOWNLOAD/...               0   \n",
       "1  [15 Amazing Hidden Features Of Google Search Y...               0   \n",
       "2  Because this statement of his is true. It isn'...               1   \n",
       "3  Good News ::Download WhatsApp 2.16.230 APK for...               1   \n",
       "4  We're streaming Pokemon Veitnamese Crystal RIG...               1   \n",
       "\n",
       "   violates_rule                                       test_comment  \n",
       "0          False  Banks don't want you to know this! Click here ...  \n",
       "1          False  SD Stream [ ENG Link 1] (http://www.sportsstre...  \n",
       "2           True  Lol. Try appealing the ban and say you won't d...  \n",
       "3           True  she will come your home open her legs with  an...  \n",
       "4           True  code free tyrande --->>> [Imgur](http://i.imgu...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test data\n",
    "df_test = pd.read_csv(\"./data/final/df_train_0.csv\")\n",
    "\n",
    "print(f\"Test set size: {len(df_test):,}\")\n",
    "print(f\"Unique rules in test: {df_test['rule'].nunique():,}\")\n",
    "\n",
    "# Show sample\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Inference on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batch retrieval with top_k=4...\n",
      "\n",
      "âœ“ Batch inference complete in 56.13 seconds\n",
      "  Average time per query: 27.66 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>rule_violation</th>\n",
       "      <th>decision</th>\n",
       "      <th>matched_rule</th>\n",
       "      <th>rule_match_type</th>\n",
       "      <th>num_retrieved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-3.999986</td>\n",
       "      <td>0</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>exact</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-3.999986</td>\n",
       "      <td>0</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>exact</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.210768</td>\n",
       "      <td>0</td>\n",
       "      <td>No legal advice: Do not offer or request legal...</td>\n",
       "      <td>exact</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.999984</td>\n",
       "      <td>1</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>exact</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.999987</td>\n",
       "      <td>1</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>exact</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  rule_violation  decision  \\\n",
       "0       0       -3.999986         0   \n",
       "1       1       -3.999986         0   \n",
       "2       2       -1.210768         0   \n",
       "3       3        3.999984         1   \n",
       "4       4        3.999987         1   \n",
       "\n",
       "                                        matched_rule rule_match_type  \\\n",
       "0  No Advertising: Spam, referral links, unsolici...           exact   \n",
       "1  No Advertising: Spam, referral links, unsolici...           exact   \n",
       "2  No legal advice: Do not offer or request legal...           exact   \n",
       "3  No Advertising: Spam, referral links, unsolici...           exact   \n",
       "4  No Advertising: Spam, referral links, unsolici...           exact   \n",
       "\n",
       "   num_retrieved  \n",
       "0              4  \n",
       "1              4  \n",
       "2              4  \n",
       "3              4  \n",
       "4              4  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run batch retrieval\n",
    "TOP_K = 2\n",
    "\n",
    "print(f\"Running batch retrieval with top_k={TOP_K}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "results_df = retriever.batch_retrieve(\n",
    "    df_test,\n",
    "    rule_col='rule',\n",
    "    comment_col='test_comment',\n",
    "    top_k=TOP_K,\n",
    "    instruction=instruction\n",
    ")\n",
    "\n",
    "inference_time = time.time() - start_time\n",
    "print(f\"\\nâœ“ Batch inference complete in {inference_time:.2f} seconds\")\n",
    "print(f\"  Average time per query: {inference_time/len(df_test)*1000:.2f} ms\")\n",
    "\n",
    "# Show sample results\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_df[\"rule_violation\"]=results_df[\"rule_violation\"]/TOP_K\n",
    "#results_df[\"rule_violation\"]=results_df[\"rule_violation\"]-results_df[\"rule_violation\"].mean()\n",
    "#results_df[\"decision\"] = np.where(results_df[\"rule_violation\"] > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATION METRICS\n",
      "================================================================================\n",
      "\n",
      "Accuracy:  0.9019\n",
      "Precision: 0.8925\n",
      "Recall:    0.9176\n",
      "F1-Score:  0.9048\n",
      "\n",
      "================================================================================\n",
      "CLASSIFICATION REPORT\n",
      "================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "No Violation       0.91      0.89      0.90       998\n",
      "   Violation       0.89      0.92      0.90      1031\n",
      "\n",
      "    accuracy                           0.90      2029\n",
      "   macro avg       0.90      0.90      0.90      2029\n",
      "weighted avg       0.90      0.90      0.90      2029\n",
      "\n",
      "================================================================================\n",
      "CONFUSION MATRIX\n",
      "================================================================================\n",
      "\n",
      "                 Predicted\n",
      "                 No  Yes\n",
      "Actual No       884  114\n",
      "       Yes       85  946\n",
      "\n",
      "================================================================================\n",
      "RULE MATCH STATISTICS\n",
      "================================================================================\n",
      "rule_match_type\n",
      "exact    2029\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare ground truth\n",
    "#y_true = df_test[\"violates_rule\"].astype(str).str.strip().str.strip('\"').str.strip(\"'\").str.lower().map({\"yes\": 1, \"no\": 0})\n",
    "y_true = df_test[\"rule_violation\"].values\n",
    "y_pred = results_df[\"decision\"].values\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"\\nAccuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(f\"{'='*80}\")\n",
    "print(classification_report(y_true, y_pred, target_names=['No Violation', 'Violation']))\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(f\"{'='*80}\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(f\"\\n                 Predicted\")\n",
    "print(f\"                 No  Yes\")\n",
    "print(f\"Actual No      {cm[0,0]:4d} {cm[0,1]:4d}\")\n",
    "print(f\"       Yes     {cm[1,0]:4d} {cm[1,1]:4d}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RULE MATCH STATISTICS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(results_df['rule_match_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
