{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f506feee-98f0-4106-a53a-ccbff09b3ce3",
   "metadata": {},
   "source": [
    "# Jigsaw - Agile Community Rules Classification\n",
    "### https://www.kaggle.com/competitions/jigsaw-agile-community-rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fee25d6f-3e57-46f7-9d77-47f9a295a78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b22aad07-c794-49d5-b6ac-dc85f984187c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f62b246c-a5ee-407a-90ce-cbbac769c7a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ uv is already installed globally.\n",
      "✅ Virtual environment 'myvenv' already exists. Skipping creation.\n",
      "Activating virtual environment...\n",
      "Upgrading pip, setuptools, wheel, and uv inside the venv...\n",
      "Looking in links: /tmp/tmpea0z1jbk\n",
      "Requirement already satisfied: setuptools in ./myvenv/lib/python3.11/site-packages (80.9.0)\n",
      "Requirement already satisfied: pip in ./myvenv/lib/python3.11/site-packages (25.2)\n",
      "Requirement already satisfied: pip in ./myvenv/lib/python3.11/site-packages (25.2)\n",
      "Requirement already satisfied: setuptools in ./myvenv/lib/python3.11/site-packages (80.9.0)\n",
      "Requirement already satisfied: wheel in ./myvenv/lib/python3.11/site-packages (0.45.1)\n",
      "Requirement already satisfied: uv in ./myvenv/lib/python3.11/site-packages (0.8.18)\n",
      "Checking Python binary path:\n",
      "/data/myvenv/bin/python3\n",
      "Checking Python version:\n",
      "Python 3.11.13\n",
      "Checking uv version inside venv:\n",
      "uv 0.8.18\n",
      "✅ Virtual environment setup complete and up to date!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e  # exit immediately if a command fails\n",
    "\n",
    "VENV_DIR=\"myvenv\"\n",
    "\n",
    "# Step 1: Ensure uv is installed globally to create venv\n",
    "if ! command -v uv &> /dev/null; then\n",
    "  echo \"Installing uv globally to create venv...\"\n",
    "  python3 -m ensurepip --upgrade || true\n",
    "  python3 -m pip install --upgrade pip setuptools wheel\n",
    "  python3 -m pip install --upgrade uv\n",
    "else\n",
    "  echo \"✅ uv is already installed globally.\"\n",
    "fi\n",
    "\n",
    "# Step 2: Create virtual environment if it doesn't exist\n",
    "if [ -d \"$VENV_DIR\" ]; then\n",
    "  echo \"✅ Virtual environment '$VENV_DIR' already exists. Skipping creation.\"\n",
    "else\n",
    "  echo \"Creating virtual environment with uv...\"\n",
    "  uv venv \"$VENV_DIR\"\n",
    "fi\n",
    "\n",
    "# Step 3: Activate virtual environment\n",
    "echo \"Activating virtual environment...\"\n",
    "source \"$VENV_DIR/bin/activate\"\n",
    "\n",
    "# Step 4: Upgrade pip, setuptools, wheel, and uv *inside* the venv\n",
    "echo \"Upgrading pip, setuptools, wheel, and uv inside the venv...\"\n",
    "python3 -m ensurepip --upgrade || true\n",
    "python3 -m pip install --upgrade pip setuptools wheel --progress-bar=on\n",
    "python3 -m pip install --upgrade uv --progress-bar=on\n",
    "\n",
    "echo \"Checking Python binary path:\"\n",
    "which python3\n",
    "\n",
    "echo \"Checking Python version:\"\n",
    "python3 --version\n",
    "\n",
    "echo \"Checking uv version inside venv:\"\n",
    "uv --version\n",
    "\n",
    "echo \"✅ Virtual environment setup complete and up to date!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3999429b-938c-415d-8628-1c4bc03ba026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required libraries with uv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.13 environment at: myvenv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m8 packages\u001b[0m \u001b[2min 79ms\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done installing packages into myvenv using uv.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e  # Exit immediately if a command fails\n",
    "\n",
    "PYTHON=./myvenv/bin/python\n",
    "\n",
    "# echo \"Bootstrapping pip if needed...\"\n",
    "# $PYTHON -m ensurepip --upgrade || true\n",
    "# $PYTHON -m pip install --upgrade pip setuptools wheel\n",
    "\n",
    "# echo \"Installing uv...\"\n",
    "# $PYTHON -m pip install --upgrade uv\n",
    "\n",
    "echo \"Installing required libraries with uv...\"\n",
    "$PYTHON -m uv pip install \\\n",
    "  trl \\\n",
    "  optimum \\\n",
    "  auto-gptq \\\n",
    "  bitsandbytes \\\n",
    "  peft \\\n",
    "  accelerate \\\n",
    "  deepspeed \\\n",
    "  kagglehub\n",
    "\n",
    "echo \"✅ Done installing packages into myvenv using uv.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffacf4e8-e1fa-4b50-9f70-8cc9c2f2f966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.py\n",
    "\n",
    "RESUME_TRAINING=False\n",
    "#LOCAL_MODEL_PATH = \"Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4\"\n",
    "LOCAL_MODEL_PATH = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "LORA_IN_PATH= \"/workspace/lora\"\n",
    "LORA_OUT_PATH = \"/workspace\"\n",
    "DATA_PATH = \"./\"\n",
    "OUTPUT_PATH=\"/workspace\"\n",
    "\n",
    "# Training parameters\n",
    "MAX_SEQ_LENGTH = 3600\n",
    "RANK = 64\n",
    "LORA_ALPHA=64\n",
    "MAX_ITER_STEPS = -1\n",
    "EPOCHS = 2\n",
    "SAMPLE_LEN=\"35k\"\n",
    "\n",
    "# Kaggle upload configuration\n",
    "MODEL_SLUG = \"qwen25-3b-instruct-jigsaw-acrc-lora\"\n",
    "VARIATION_SLUG = \"15\"\n",
    "\n",
    "###--------------------------------###\n",
    "DATASET_ID=\"0-cr2\"\n",
    "BASE_MODEL=LOCAL_MODEL_PATH.split(\"/\")[-1].replace(\".\", \"p\")\n",
    "TRAIN_DIR=f\"{BASE_MODEL}_lora_fp16_r{RANK}_s{SAMPLE_LEN}_e_{EPOCHS}_msl{MAX_SEQ_LENGTH}-{DATASET_ID}\"\n",
    "print(\"TRAIN_DIR\",TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b3fc1-46cb-4dd4-b7ba-f0140e144fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile get_dataset.py\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import kagglehub\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load Jigsaw ACRC dataset from Kaggle or local files\"\"\"\n",
    "    # Check if running on Kaggle\n",
    "    if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "        # Running on Kaggle\n",
    "        base_path = \"/kaggle/input/jigsaw-agile-community-rules/\"\n",
    "        df_train = pd.read_csv(f\"{base_path}*train*.csv\")\n",
    "        df_test = pd.read_csv(f\"{base_path}*test*.csv\")\n",
    "    else:\n",
    "        # Running locally\n",
    "        base_path = \"./\"\n",
    "        \n",
    "        # Find all train files\n",
    "        train_files = glob.glob(f\"{base_path}*train*.csv\")\n",
    "        if train_files:\n",
    "            train_dfs = [pd.read_csv(file) for file in train_files]\n",
    "            df_train = pd.concat(train_dfs, ignore_index=True)\n",
    "            print(f\"Concatenated {len(train_files)} train files: {train_files}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No train files found in {base_path}\")\n",
    "        \n",
    "        # Find all test files\n",
    "        test_files = glob.glob(f\"{base_path}*test*.csv\")\n",
    "        if test_files:\n",
    "            test_dfs = [pd.read_csv(file) for file in test_files]\n",
    "            df_test = pd.concat(test_dfs, ignore_index=True)\n",
    "            print(f\"Concatenated {len(test_files)} test files: {test_files}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No test files found in {base_path}\")\n",
    "\n",
    "    print(f\"Train shape: {df_train.shape}\")\n",
    "    print(f\"Test shape: {df_test.shape}\")\n",
    "    print(df_train.columns)\n",
    "            \n",
    "    req_cols=['subreddit', 'rule', 'positive_example_1', 'negative_example_1', 'positive_example_2',\n",
    "           'negative_example_2', 'test_comment', 'violates_rule']\n",
    "\n",
    "    df_train=df_train[req_cols]\n",
    "    df_test=df_test[req_cols]\n",
    "\n",
    "    # Normalize \"True\"/\"False\" -> \"Yes\"/\"No\" and drop anything else\n",
    "    for name, df in [(\"train\", df_train), (\"test\", df_test)]:\n",
    "        df[\"violates_rule\"] = (\n",
    "            df[\"violates_rule\"]\n",
    "            .astype(str).str.strip()\n",
    "            .map({\"True\": \"Yes\", \"False\": \"No\", \"Yes\": \"Yes\", \"No\": \"No\"})  # normalize\n",
    "        )\n",
    "        before = len(df)\n",
    "        df.dropna(subset=[\"violates_rule\"], inplace=True)  # drop rows with NaN (anything not Yes/No/True/False)\n",
    "        after = len(df)\n",
    "        print(f\"Dropped {before - after} rows from {name} due to invalid 'violates_rule'\")\n",
    "    \n",
    "    for col in req_cols:\n",
    "        dropped_rows = df_train[df_train[col].isna()].shape[0]\n",
    "        print(f\"{col}: {dropped_rows} rows would be dropped\")\n",
    "        \n",
    "    df_train = df_train[req_cols].dropna()\n",
    "    df_test = df_test[req_cols].dropna()\n",
    "\n",
    "    print(f\"Using path: {base_path}\")\n",
    "    print(\"\\n After dropping:\")\n",
    "    print(f\"Train shape: {df_train.shape}\")\n",
    "    print(f\"Test shape: {df_test.shape}\")\n",
    "\n",
    "    df_train[\"violates_rule\"] = df_train[\"violates_rule\"].astype(str)\n",
    "    df_test[\"violates_rule\"] = df_test[\"violates_rule\"].astype(str)\n",
    "\n",
    "    valid_values = {\"Yes\", \"No\"}\n",
    "    df_train = df_train[df_train[\"violates_rule\"].isin(valid_values)]\n",
    "    df_test  = df_test[df_test[\"violates_rule\"].isin(valid_values)]\n",
    "    print(\"\\n After checking Yes/No:\")\n",
    "    print(f\"Train shape: {df_train.shape}\")\n",
    "    print(f\"Test shape: {df_test.shape}\")\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def formatting_prompts_func(examples, tokenizer):\n",
    "    \"\"\"\n",
    "    Format Reddit moderation dataset using tokenizer chat template\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for i in range(len(examples['subreddit'])):\n",
    "        # Create system message\n",
    "        system_msg = f\"\"\"You are a really experienced moderator for the subreddit /r/{examples['subreddit'][i]}. \n",
    "Your job is to determine if the following reported comment violates the given rule. Answer with only Yes or No.\"\"\"\n",
    "        \n",
    "        # Create user message with the rule and examples\n",
    "        user_msg = f\"\"\"<rule>\n",
    "{examples['rule'][i]}\n",
    "</rule>\n",
    "\n",
    "<examples>\n",
    "<example>\n",
    "<comment>{examples['positive_example_1'][i]}</comment>\n",
    "<rule_violation>Yes</rule_violation>\n",
    "</example>\n",
    "\n",
    "<example>\n",
    "<comment>{examples['positive_example_2'][i]}</comment>\n",
    "<rule_violation>Yes</rule_violation>\n",
    "</example>\n",
    "\n",
    "<example>\n",
    "<comment>{examples['negative_example_1'][i]}</comment>\n",
    "<rule_violation>No</rule_violation>\n",
    "</example>\n",
    "\n",
    "<example>\n",
    "<comment>{examples['negative_example_2'][i]}</comment>\n",
    "<rule_violation>No</rule_violation>\n",
    "</example>\n",
    "</examples>\n",
    "\n",
    "<test_comment>\n",
    "{examples['test_comment'][i]}\n",
    "</test_comment>\"\"\"\n",
    "        \n",
    "        # Assistant response is \"Yes\" or \"No\"\n",
    "        assistant_msg = examples['violates_rule'][i]\n",
    "\n",
    "        # Create messages list for chat template\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_msg}\n",
    "        ]\n",
    "\n",
    "\n",
    "        formatted_text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "\n",
    "        texts.append(formatted_text)\n",
    "\n",
    "    return {\"text\": texts}\n",
    "\n",
    "def build_dataset(tokenizer):\n",
    "    \"\"\"\n",
    "    Build both train and test datasets using tokenizer chat template\n",
    "    \"\"\"\n",
    "    df_train, df_test = load_data()\n",
    "    \n",
    "    train_dataset = Dataset.from_pandas(df_train)\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda examples: formatting_prompts_func(examples, tokenizer), \n",
    "        batched=True\n",
    "    )\n",
    "    \n",
    "    test_dataset = Dataset.from_pandas(df_test)\n",
    "    test_dataset = test_dataset.map(\n",
    "        lambda examples: formatting_prompts_func(examples, tokenizer), \n",
    "        batched=True\n",
    "    )\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ee819-dabb-406f-b404-17cd4b3bf0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "\n",
    "\n",
    "#load model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_path,\n",
    "    #model_name=\"./local-model-name\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=( None ),\n",
    "    load_in_4bit=False,\n",
    "    load_in_8bit=False\n",
    ")\n",
    "\n",
    "#set peft config\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = RANK, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = LORA_ALPHA,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 123,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "#load dataset\n",
    "dataset_train, dataset_test = build_dataset(tokenizer)\n",
    "\n",
    "\n",
    "#SFT trainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset_train,\n",
    "    eval_dataset = dataset_test,  # Add test dataset here\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = EPOCHS, \n",
    "        max_steps = max_iter_steps,\n",
    "        learning_rate = 5e-4,\n",
    "        fp16 = False,  # Disable FP16 to avoid gradient unscaling issues\n",
    "        bf16 = is_bfloat16_supported(),  # Use BF16 instead if supported\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_torch\",  # Use adamw_torch instead of adamw_8bit for better compatibility\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 123,\n",
    "        eval_strategy = \"steps\", \n",
    "        eval_steps = 2000,\n",
    "        max_grad_norm = 1.0,  # Add explicit gradient clipping\n",
    "        # Save settings - save after each epoch\n",
    "        save_strategy = \"epoch\",\n",
    "        save_total_limit = 3,  # Keep only last 3 checkpoints to save space\n",
    "        #load_best_model_at_end = True,\n",
    "        #metric_for_best_model = \"eval_loss\",\n",
    "        output_dir = \"outputs\",\n",
    "        #report_to = \"mlflow\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "\n",
    "#train model\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "#save lora weights\n",
    "os.makedirs(os.path.join(LORA_OUT_PATH, TRAIN_DIR), exist_ok=True)\n",
    "model.save_pretrained(os.path.join(LORA_OUT_PATH, TRAIN_DIR))  # Local saving\n",
    "tokenizer.save_pretrained(os.path.join(LORA_OUT_PATH, TRAIN_DIR))\n",
    "\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bbc3aac-c913-418d-b640-5a40b18935de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ~/.kaggle\n",
    "cp kaggle.json ~/.kaggle/\n",
    "chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64c6fd61-5a8f-45bf-afbe-4cfd8f476469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing upload.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile upload.py\n",
    "import os\n",
    "import kagglehub\n",
    "from config import LORA_OUT_PATH, TRAIN_DIR, MODEL_SLUG, VARIATION_SLUG\n",
    "LOCAL_MODEL_DIR = os.path.join(LORA_OUT_PATH, TRAIN_DIR)\n",
    "\n",
    "kagglehub.model_upload(\n",
    "    handle=f\"vinothkumarsekar89/{MODEL_SLUG}/transformers/{VARIATION_SLUG}\",\n",
    "    local_model_dir=LOCAL_MODEL_DIR,\n",
    "    version_notes= TRAIN_DIR\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "baf04ef5-2c69-46b9-8c9c-349328ee9408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "./myvenv/bin/python  train.py\n",
    "./myvenv/bin/python launch upload.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab1a72a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "chmod +x run.sh\n",
    "nohup bash run.sh > log.log 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec7aeb34-a53f-49b2-8b2b-1d2fb5e28dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tail -f log.log"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11175052,
     "sourceId": 89659,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.756759,
   "end_time": "2025-02-26T02:09:27.363350",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-26T02:09:20.606591",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
