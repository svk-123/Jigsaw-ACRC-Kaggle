{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f506feee-98f0-4106-a53a-ccbff09b3ce3",
   "metadata": {},
   "source": [
    "# Jigsaw - Agile Community Rules Classification\n",
    "### https://www.kaggle.com/competitions/jigsaw-agile-community-rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "935e1ef4-5a36-4c0c-b90d-59bb8c446f34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install packages\n",
    "# !pip install --upgrade pip\n",
    "# !pip install trl\n",
    "# !pip install optimum\n",
    "# !pip install auto-gptq\n",
    "# !pip install bitsandbytes\n",
    "# !pip install peft accelerate deepspeed\n",
    "# !pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b7b6a783-3fa0-4d11-a1b2-1b83d930bd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a12dff2735249dcb6b7b67a229ed5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kagglehub\n",
    "kagglehub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ffacf4e8-e1fa-4b50-9f70-8cc9c2f2f966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.py\n",
    "\n",
    "RESUME_TRAINING=True\n",
    "LOCAL_MODEL_PATH = \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\"\n",
    "LORA_IN_PATH= \"lora_repo\"\n",
    "LORA_OUT_PATH = \"./\"\n",
    "DATA_PATH = \"./\"\n",
    "OUTPUT_PATH=\"./outputs/\"\n",
    "\n",
    "# Training parameters\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "RANK = 64\n",
    "MAX_ITER_STEPS = 1\n",
    "EPOCHS = 0\n",
    "SAMPLE_LEN=\"25k\"\n",
    "\n",
    "# Kaggle upload configuration\n",
    "MODEL_SLUG = \"qwen25-32b-gptq-int4-jigsaw-acrc-lora---\"\n",
    "VARIATION_SLUG = \"01\"\n",
    "\n",
    "###--------------------------------###\n",
    "DATASET_ID=\"1to9\"\n",
    "BASE_MODEL=LOCAL_MODEL_PATH.split(\"/\")[-1].replace(\".\", \"p\")\n",
    "TRAIN_DIR=f\"{BASE_MODEL}_lora_fp16_r{RANK}_s{SAMPLE_LEN}_e_{EPOCHS}_msl{MAX_SEQ_LENGTH}-{DATASET_ID}\"\n",
    "print(\"TRAIN_DIR\",TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "016b3fc1-46cb-4dd4-b7ba-f0140e144fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting get_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile get_dataset.py\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import kagglehub\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load Jigsaw ACRC dataset from Kaggle or local files\"\"\"\n",
    "    # Check if running on Kaggle\n",
    "    if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "        # Running on Kaggle\n",
    "        base_path = \"/kaggle/input/jigsaw-agile-community-rules/\"\n",
    "        df_train = pd.read_csv(f\"{base_path}*train*.csv\")\n",
    "        df_test = pd.read_csv(f\"{base_path}*test*.csv\")\n",
    "    else:\n",
    "        # Running locally\n",
    "        base_path = \"./\"\n",
    "        \n",
    "        # Find all train files\n",
    "        train_files = glob.glob(f\"{base_path}*train*.csv\")\n",
    "        if train_files:\n",
    "            train_dfs = [pd.read_csv(file) for file in train_files]\n",
    "            df_train = pd.concat(train_dfs, ignore_index=True)\n",
    "            print(f\"Concatenated {len(train_files)} train files: {train_files}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No train files found in {base_path}\")\n",
    "        \n",
    "        # Find all test files\n",
    "        test_files = glob.glob(f\"{base_path}*test*.csv\")\n",
    "        if test_files:\n",
    "            test_dfs = [pd.read_csv(file) for file in test_files]\n",
    "            df_test = pd.concat(test_dfs, ignore_index=True)\n",
    "            print(f\"Concatenated {len(test_files)} test files: {test_files}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No test files found in {base_path}\")\n",
    "\n",
    "    print(f\"Train shape: {df_train.shape}\")\n",
    "    print(f\"Test shape: {df_test.shape}\")\n",
    "    print(df_train.columns)\n",
    "            \n",
    "    req_cols=['subreddit', 'rule', 'positive_example_1', 'negative_example_1', 'positive_example_2',\n",
    "           'negative_example_2', 'test_comment', 'violates_rule']\n",
    "\n",
    "    df_train=df_train[req_cols]\n",
    "    df_test=df_test[req_cols]\n",
    "\n",
    "    for col in req_cols:\n",
    "        dropped_rows = df_train[df_train[col].isna()].shape[0]\n",
    "        print(f\"{col}: {dropped_rows} rows would be dropped\")\n",
    "        \n",
    "    df_train = df_train[req_cols].dropna()\n",
    "    df_test = df_test[req_cols].dropna()\n",
    "\n",
    "    print(f\"Using path: {base_path}\")\n",
    "    print(\"\\n After dropping:\")\n",
    "    print(f\"Train shape: {df_train.shape}\")\n",
    "    print(f\"Test shape: {df_test.shape}\")\n",
    "\n",
    "    df_train[\"violates_rule\"] = df_train[\"violates_rule\"].astype(str)\n",
    "    df_test[\"violates_rule\"] = df_test[\"violates_rule\"].astype(str)\n",
    "\n",
    "    valid_values = {\"True\", \"False\"}\n",
    "    df_train = df_train[df_train[\"violates_rule\"].isin(valid_values)]\n",
    "    df_test  = df_test[df_test[\"violates_rule\"].isin(valid_values)]\n",
    "    print(\"\\n After checking True/False:\")\n",
    "    print(f\"Train shape: {df_train.shape}\")\n",
    "    print(f\"Test shape: {df_test.shape}\")\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"\n",
    "    Format Reddit moderation dataset for Alpaca training - matches inference format exactly\n",
    "    \"\"\"\n",
    "    alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{}\n",
    "### Input:\n",
    "{}\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "    \n",
    "    texts = []\n",
    "    \n",
    "    for i in range(len(examples['subreddit'])):\n",
    "        # Create instruction - exactly as in inference\n",
    "        instruction = f\"\"\"You are a really experienced moderator for the subreddit /r/{examples['subreddit'][i]}. \n",
    "Your job is to determine if the following reported comment violates the given rule.\n",
    "Answer with only \"True\" or \"False\".\"\"\"\n",
    "        \n",
    "        # Create input - exactly as in inference\n",
    "        input_text = f\"\"\"Rule: {examples['rule'][i]}\n",
    "Example 1:\n",
    "{examples['positive_example_1'][i]}\n",
    "Rule violation: True\n",
    "Example 2:\n",
    "{examples['negative_example_1'][i]}\n",
    "Rule violation: False\n",
    "Example 3:\n",
    "{examples['positive_example_2'][i]}\n",
    "Rule violation: True\n",
    "Example 4:\n",
    "{examples['negative_example_2'][i]}\n",
    "Rule violation: False\n",
    "Test sentence:\n",
    "{examples['test_comment'][i]}\"\"\"\n",
    "        \n",
    "        # Response is already \"True\" or \"False\" string\n",
    "        response = examples['violates_rule'][i]\n",
    "                \n",
    "        # Format the complete prompt\n",
    "        text = alpaca_prompt.format(instruction, input_text, response)\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "def build_dataset():\n",
    "    \"\"\"\n",
    "    Build both train and test datasets using the new Alpaca format\n",
    "    \"\"\"\n",
    "    df_train, df_test = load_data()\n",
    "    \n",
    "    train_dataset = Dataset.from_pandas(df_train)\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda examples: formatting_prompts_func(examples), \n",
    "        batched=True\n",
    "    )\n",
    "    \n",
    "    test_dataset = Dataset.from_pandas(df_test)\n",
    "    test_dataset = test_dataset.map(\n",
    "        lambda examples: formatting_prompts_func(examples), \n",
    "        batched=True\n",
    "    )\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dd4ee819-dabb-406f-b404-17cd4b3bf0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers.utils import is_torch_bf16_gpu_available\n",
    "from get_dataset import build_dataset\n",
    "from config import LOCAL_MODEL_PATH, LORA_IN_PATH, LORA_OUT_PATH, RANK, MAX_SEQ_LENGTH, EPOCHS, TRAIN_DIR, MAX_ITER_STEPS, OUTPUT_PATH, RESUME_TRAINING\n",
    "import os\n",
    "\n",
    "# # ----------------------------\n",
    "# # Load model & tokenizer\n",
    "# # ----------------------------\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LOCAL_MODEL_PATH,\n",
    "    torch_dtype=\"auto\",       \n",
    ")\n",
    "\n",
    "if RESUME_TRAINING: \n",
    "    model = PeftModel.from_pretrained(model, LORA_IN_PATH, is_trainable=True)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.gradient_checkpointing_enable()  # reduce memory usage    \n",
    "\n",
    "# ----------------------------\n",
    "# Build datasets\n",
    "# ----------------------------\n",
    "train_dataset, test_dataset = build_dataset()\n",
    "\n",
    "# ----------------------------\n",
    "# LoRA config\n",
    "# ----------------------------\n",
    "if RESUME_TRAINING:    \n",
    "    lora_config = None       \n",
    "else:    \n",
    "    lora_config = LoraConfig(\n",
    "        r=RANK,\n",
    "        lora_alpha=64,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "        ],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    \n",
    "# ----------------------------\n",
    "# SFT config\n",
    "# ----------------------------\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=OUTPUT_PATH,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    max_steps=MAX_ITER_STEPS,\n",
    "    per_device_train_batch_size=4,       \n",
    "    gradient_accumulation_steps=4,       \n",
    "    max_length=min(MAX_SEQ_LENGTH, 2048),  \n",
    "\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "        \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "        \n",
    "    bf16=is_torch_bf16_gpu_available(),\n",
    "    fp16=not is_torch_bf16_gpu_available(),\n",
    "    dataloader_pin_memory=True,\n",
    "        \n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "\n",
    "    warmup_steps=5,\n",
    "    logging_steps=10,\n",
    "    eval_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "\n",
    "    report_to=\"none\",        \n",
    "    packing=False,\n",
    "    remove_unused_columns=False,\n",
    "    dataset_text_field=\"text\",\n",
    "\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Trainer\n",
    "# ----------------------------\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=lora_config,\n",
    "    args=sft_config,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(os.path.join(LORA_OUT_PATH, TRAIN_DIR))\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1454531c-7a2d-47d9-aa14-43d75573c26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerate_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_config.yaml\n",
    "# \"\"\"\n",
    "# ZeRO Stage 1: Optimizer State Partitioning (2-4 GPU)\n",
    "# ZeRO Stage 2: + Gradient Partitioning (4-8 GPU)\n",
    "# ZeRO Stage 3: + Parameter Partitioning (8+ GPU)\n",
    "# \"\"\"\n",
    "\n",
    "compute_environment: LOCAL_MACHINE\n",
    "debug: false\n",
    "deepspeed_config:\n",
    "  gradient_accumulation_steps: 4\n",
    "  gradient_clipping: 1.0\n",
    "  train_batch_size: 16\n",
    "  train_micro_batch_size_per_gpu: 4\n",
    "  \n",
    "  zero_stage: 2\n",
    "  offload_optimizer_device: none\n",
    "  offload_param_device: none\n",
    "  zero3_init_flag: false\n",
    "  \n",
    "  stage3_gather_16bit_weights_on_model_save: false\n",
    "  stage3_max_live_parameters: 1e8\n",
    "  stage3_max_reuse_distance: 1e8\n",
    "  stage3_prefetch_bucket_size: 5e7\n",
    "  stage3_param_persistence_threshold: 1e5\n",
    "  \n",
    "  zero_allow_untested_optimizer: true\n",
    "  zero_force_ds_cpu_optimizer: false\n",
    "  \n",
    "  fp16:\n",
    "    enabled: true\n",
    "    loss_scale: 0\n",
    "    initial_scale_power: 16\n",
    "    loss_scale_window: 1000\n",
    "    hysteresis: 2\n",
    "    min_loss_scale: 1\n",
    "  \n",
    "distributed_type: DEEPSPEED\n",
    "downcast_bf16: 'no'\n",
    "dynamo_config:\n",
    "  dynamo_backend: INDUCTOR\n",
    "  dynamo_use_fullgraph: false\n",
    "  dynamo_use_dynamic: false\n",
    "enable_cpu_affinity: false\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "mixed_precision: fp16\n",
    "num_machines: 1\n",
    "num_processes: 1\n",
    "rdzv_backend: static\n",
    "same_network: true\n",
    "tpu_env: []\n",
    "tpu_use_cluster: false\n",
    "tpu_use_sudo: false\n",
    "use_cpu: false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9d78e967-ce77-450c-b59d-0de3d002ed9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-20 22:47:32,764] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-08-20 22:47:34,417] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "TRAIN_DIR Qwen2p5-7B-Instruct-GPTQ-Int4_lora_fp16_r64_s25k_e_0_msl2048-1to9\n",
      "/home/vino/anaconda3/envs/kaggle/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/vino/anaconda3/envs/kaggle/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/vino/anaconda3/envs/kaggle/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  7.48it/s]\n",
      "Found modules on cpu/disk. Using Exllama/Exllamav2 backend requires all the modules to be on GPU. Setting `disable_exllama=True`\n",
      "Concatenated 1 train files: ['./df_train.csv']\n",
      "Concatenated 1 test files: ['./df_test.csv']\n",
      "Train shape: (26899, 8)\n",
      "Test shape: (6449, 8)\n",
      "Index(['subreddit', 'rule', 'positive_example_1', 'negative_example_1',\n",
      "       'positive_example_2', 'negative_example_2', 'test_comment',\n",
      "       'violates_rule'],\n",
      "      dtype='object')\n",
      "subreddit: 0 rows would be dropped\n",
      "rule: 0 rows would be dropped\n",
      "positive_example_1: 0 rows would be dropped\n",
      "negative_example_1: 0 rows would be dropped\n",
      "positive_example_2: 0 rows would be dropped\n",
      "negative_example_2: 0 rows would be dropped\n",
      "test_comment: 0 rows would be dropped\n",
      "violates_rule: 0 rows would be dropped\n",
      "Using path: ./\n",
      "\n",
      " After dropping:\n",
      "Train shape: (26899, 8)\n",
      "Test shape: (6449, 8)\n",
      "\n",
      " After checking True/False:\n",
      "Train shape: (26899, 8)\n",
      "Test shape: (6449, 8)\n",
      "Map: 100%|██████████████████████| 26899/26899 [00:00<00:00, 94384.09 examples/s]\n",
      "Map: 100%|████████████████████████| 6449/6449 [00:00<00:00, 95942.83 examples/s]\n",
      "[2025-08-20 22:47:43,948] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-08-20 22:47:45,739] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "[2025-08-20 22:47:45,747] [INFO] [comm.py:821:init_distributed] cdb=None\n",
      "[2025-08-20 22:47:45,747] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "Adding EOS to train dataset: 100%|█| 26899/26899 [00:02<00:00, 12902.65 examples\n",
      "Tokenizing train dataset: 100%|██| 26899/26899 [00:17<00:00, 1502.85 examples/s]\n",
      "Truncating train dataset: 100%|█| 26899/26899 [00:00<00:00, 352634.49 examples/s\n",
      "Adding EOS to eval dataset: 100%|█| 6449/6449 [00:00<00:00, 13113.29 examples/s]\n",
      "Tokenizing eval dataset: 100%|█████| 6449/6449 [00:04<00:00, 1378.95 examples/s]\n",
      "Truncating eval dataset: 100%|███| 6449/6449 [00:00<00:00, 413638.56 examples/s]\n",
      "[2025-08-20 22:48:12,918] [WARNING] [engine.py:1373:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "{'train_runtime': 15.8408, 'train_samples_per_second': 1.01, 'train_steps_per_second': 0.063, 'train_loss': 2.856719970703125, 'num_tokens': 4864.0, 'mean_token_accuracy': 0.49155566096305847, 'epoch': 0.0}\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:15<00:00, 15.84s/it]\n",
      "success\n",
      "[rank0]:[W820 22:48:34.723161556 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch --config_file accelerate_config.yaml train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2d97436d-4648-44a7-8695-535a467044ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replace with path to directory containing model files.\n",
    "# from config import LORA_OUT_PATH, TRAIN_DIR, MODEL_SLUG, VARIATION_SLUG\n",
    "# LOCAL_MODEL_DIR = LORA_OUT_PATH + TRAIN_DIR\n",
    "\n",
    "# MODEL_SLUG = MODEL_SLUG # Replace with model slug.\n",
    "\n",
    "# # Learn more about naming model variations at\n",
    "# # https://www.kaggle.com/docs/models#name-model.\n",
    "# VARIATION_SLUG = VARIATION_SLUG # Replace with variation slug.\n",
    "\n",
    "# kagglehub.model_upload(\n",
    "#   handle = f\"vinothkumarsekar89/{MODEL_SLUG}/transformers/{VARIATION_SLUG}\",\n",
    "#   local_model_dir = LOCAL_MODEL_DIR,\n",
    "#   version_notes = 'LoRA Merged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c6fd61-5a8f-45bf-afbe-4cfd8f476469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11175052,
     "sourceId": 89659,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.756759,
   "end_time": "2025-02-26T02:09:27.363350",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-26T02:09:20.606591",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
