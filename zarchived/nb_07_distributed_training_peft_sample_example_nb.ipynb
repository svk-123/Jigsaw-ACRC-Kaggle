{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f506feee-98f0-4106-a53a-ccbff09b3ce3",
   "metadata": {},
   "source": [
    "# Jigsaw - Agile Community Rules Classification\n",
    "### https://www.kaggle.com/competitions/jigsaw-agile-community-rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "638d1fcb-e6f2-4240-8735-b581c7758034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic training\n",
    "#python train.py\n",
    "\n",
    "# Training with custom config\n",
    "#python train.py --config config.yaml --use-wandb\n",
    "\n",
    "# Training with CLI overrides\n",
    "#python train.py --epochs 5 --batch-size 4 --learning-rate 1e-4\n",
    "\n",
    "# Evaluation\n",
    "#python evaluate.py --model-path output/ --config config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bedf533-7536-4459-b5a4-8bb33402b0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.py\n",
    "\"\"\"Simple configuration management.\"\"\"\n",
    "\n",
    "import yaml\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Model paths\n",
    "    base_model_path: str = \"/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1\"\n",
    "    lora_output_path: str = \"output/\"\n",
    "    data_path: str = \"/kaggle/input/jigsaw-agile-community-rules/\"\n",
    "    \n",
    "    # Prompt settings\n",
    "    positive_answer: str = \"Yes\"\n",
    "    negative_answer: str = \"No\"\n",
    "    complete_phrase: str = \"Answer:\"\n",
    "    \n",
    "    # LoRA settings\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.1\n",
    "    target_modules: List[str] = field(default_factory=lambda: [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ])\n",
    "    \n",
    "    # Training settings\n",
    "    num_epochs: int = 1\n",
    "    batch_size: int = 16\n",
    "    gradient_accumulation_steps: int = 2\n",
    "    learning_rate: float = 5e-5\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_ratio: float = 0.05\n",
    "    max_seq_length: int = 2048\n",
    "    \n",
    "    # System settings\n",
    "    seed: int = 42\n",
    "    output_dir: str = \"outputs\"\n",
    "    \n",
    "    @property\n",
    "    def base_prompt(self) -> str:\n",
    "        return f\"Reddit moderation: Does the comment violate the rule? Answer '{self.positive_answer}' or '{self.negative_answer}' only.\"\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, config_path: str) -> 'Config':\n",
    "        \"\"\"Load config from YAML file.\"\"\"\n",
    "        with open(config_path, 'r') as f:\n",
    "            data = yaml.safe_load(f)\n",
    "        return cls(**data)\n",
    "    \n",
    "    def save(self, config_path: str):\n",
    "        \"\"\"Save config to YAML file.\"\"\"\n",
    "        with open(config_path, 'w') as f:\n",
    "            yaml.dump(self.__dict__, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c607cf99-5564-4447-944c-7dcd316c78fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "\"\"\"Data processing utilities.\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def build_prompt(row, config):\n",
    "    \"\"\"Build training prompt from data row.\"\"\"\n",
    "    return f\"\"\"\n",
    "{config.base_prompt}\n",
    "r/{row[\"subreddit\"]} rule: {row[\"rule\"]}\n",
    "Comment: {row[\"body\"]}\n",
    "---\n",
    "{config.complete_phrase}\"\"\"\n",
    "\n",
    "def load_and_combine_data(config) -> pd.DataFrame:\n",
    "    \"\"\"Load and combine training data.\"\"\"\n",
    "    # Load datasets\n",
    "    train_df = pd.read_csv(f\"{config.data_path}/train.csv\")\n",
    "    test_df = pd.read_csv(f\"{config.data_path}/test.csv\")\n",
    "    \n",
    "    dataframes = []\n",
    "    \n",
    "    # Add original training data\n",
    "    train_subset = train_df[[\"body\", \"rule\", \"subreddit\", \"rule_violation\"]].copy()\n",
    "    dataframes.append(train_subset)\n",
    "    \n",
    "    # Add positive and negative examples from test set\n",
    "    for violation_type in [\"positive\", \"negative\"]:\n",
    "        for i in range(1, 3):\n",
    "            column_name = f\"{violation_type}_example_{i}\"\n",
    "            if column_name in test_df.columns:\n",
    "                sub_df = test_df[[\"rule\", \"subreddit\", column_name]].copy()\n",
    "                sub_df = sub_df.rename(columns={column_name: \"body\"})\n",
    "                sub_df[\"rule_violation\"] = 1 if violation_type == \"positive\" else 0\n",
    "                \n",
    "                # Remove empty entries\n",
    "                sub_df = sub_df.dropna(subset=[\"body\"])\n",
    "                sub_df = sub_df[sub_df[\"body\"].str.strip() != \"\"]\n",
    "                dataframes.append(sub_df)\n",
    "    \n",
    "    # Combine and deduplicate\n",
    "    combined_df = pd.concat(dataframes, axis=0, ignore_index=True)\n",
    "    combined_df = combined_df.drop_duplicates(subset=[\"body\", \"rule\", \"subreddit\"], ignore_index=True)\n",
    "    \n",
    "    print(f\"Combined dataset: {len(combined_df)} samples\")\n",
    "    print(f\"Positive: {sum(combined_df['rule_violation'] == 1)}\")\n",
    "    print(f\"Negative: {sum(combined_df['rule_violation'] == 0)}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def create_dataset(config) -> Dataset:\n",
    "    \"\"\"Create training dataset.\"\"\"\n",
    "    df = load_and_combine_data(config)\n",
    "    \n",
    "    # Build prompts and completions\n",
    "    df[\"prompt\"] = df.apply(lambda row: build_prompt(row, config), axis=1)\n",
    "    df[\"completion\"] = df[\"rule_violation\"].map({\n",
    "        1: config.positive_answer,\n",
    "        0: config.negative_answer,\n",
    "    })\n",
    "    \n",
    "    # Create final dataset\n",
    "    final_df = df[[\"prompt\", \"completion\"]].copy()\n",
    "    dataset = Dataset.from_pandas(final_df, preserve_index=False)\n",
    "    \n",
    "    print(f\"Training dataset: {len(dataset)} samples\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b0f4306-5f8a-4586-9a96-43f0343f95f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\"\"\"Main training script with distributed support.\"\"\"\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_torch_bf16_gpu_available\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "from config import Config\n",
    "from utils import create_dataset\n",
    "\n",
    "def get_training_args(config, accelerator) -> SFTConfig:\n",
    "    \"\"\"Create training arguments.\"\"\"\n",
    "    # Auto-detect mixed precision\n",
    "    use_bf16 = is_torch_bf16_gpu_available()\n",
    "    use_fp16 = not use_bf16\n",
    "    \n",
    "    # Create output directory\n",
    "    Path(config.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    return SFTConfig(\n",
    "        output_dir=config.output_dir,\n",
    "        \n",
    "        # Training params\n",
    "        num_train_epochs=config.num_epochs,\n",
    "        per_device_train_batch_size=config.batch_size,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        \n",
    "        # Optimization\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        learning_rate=config.learning_rate,\n",
    "        weight_decay=config.weight_decay,\n",
    "        max_grad_norm=1.0,\n",
    "        \n",
    "        # Scheduler\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=config.warmup_ratio,\n",
    "        \n",
    "        # Mixed precision\n",
    "        bf16=use_bf16,\n",
    "        fp16=use_fp16,\n",
    "        \n",
    "        # Memory optimization\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        dataloader_pin_memory=True,\n",
    "        \n",
    "        # Logging\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        \n",
    "        # SFT specific\n",
    "        completion_only_loss=True,\n",
    "        packing=False,\n",
    "        max_seq_length=config.max_seq_length,\n",
    "        remove_unused_columns=False,\n",
    "        \n",
    "        # Simple reporting\n",
    "        report_to=\"none\",\n",
    "        seed=config.seed,\n",
    "    )\n",
    "\n",
    "def get_lora_config(config) -> LoraConfig:\n",
    "    \"\"\"Create LoRA configuration.\"\"\"\n",
    "    return LoraConfig(\n",
    "        r=config.lora_r,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        lora_dropout=config.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        target_modules=config.target_modules,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--config\", type=str, help=\"Config file path\")\n",
    "    parser.add_argument(\"--epochs\", type=int, help=\"Number of epochs\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, help=\"Batch size\")\n",
    "    parser.add_argument(\"--learning-rate\", type=float, help=\"Learning rate\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Initialize accelerator for distributed training\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # Load config\n",
    "    if args.config:\n",
    "        config = Config.load(args.config)\n",
    "    else:\n",
    "        config = Config()\n",
    "    \n",
    "    # Override with CLI args\n",
    "    if args.epochs:\n",
    "        config.num_epochs = args.epochs\n",
    "    if args.batch_size:\n",
    "        config.batch_size = args.batch_size\n",
    "    if args.learning_rate:\n",
    "        config.learning_rate = args.learning_rate\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    set_seed(config.seed)\n",
    "    \n",
    "    # Setup logging\n",
    "    if accelerator.is_main_process:\n",
    "        print(f\"Training on {accelerator.num_processes} process(es)\")\n",
    "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "        print(f\"Config: {config}\")\n",
    "        \n",
    "        # Save config\n",
    "        config.save(f\"{config.output_dir}/config.yaml\")\n",
    "    \n",
    "    # Create dataset\n",
    "    train_dataset = create_dataset(config)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config.base_model_path,\n",
    "        trust_remote_code=True,\n",
    "        padding_side=\"right\"\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Get configs\n",
    "    training_args = get_training_args(config, accelerator)\n",
    "    lora_config = get_lora_config(config)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=config.base_model_path,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        peft_config=lora_config,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    if accelerator.is_main_process:\n",
    "        print(\"Starting training...\")\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Save model\n",
    "    output_path = Path(config.lora_output_path)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    trainer.save_model(str(output_path))\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        print(f\"Training completed! Model saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23919b5f-be38-42ff-85ec-055ee46547fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing evaluate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile evaluate.py\n",
    "\"\"\"Simple evaluation script.\"\"\"\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import argparse\n",
    "\n",
    "from config import Config\n",
    "from utils import build_prompt\n",
    "\n",
    "def load_model(config, model_path):\n",
    "    \"\"\"Load trained model.\"\"\"\n",
    "    print(f\"Loading model from {model_path}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config.base_model_path,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.base_model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def predict_single(model, tokenizer, prompt, config):\n",
    "    \"\"\"Make prediction for single prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    new_tokens = outputs[0][len(inputs.input_ids[0]):]\n",
    "    response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "    \n",
    "    # Convert to binary prediction\n",
    "    if config.positive_answer.lower() in response.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def evaluate_model(config, model_path, test_data):\n",
    "    \"\"\"Evaluate model on test data.\"\"\"\n",
    "    model, tokenizer = load_model(config, model_path)\n",
    "    \n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    print(f\"Evaluating on {len(test_data)} samples...\")\n",
    "    \n",
    "    for idx, row in test_data.iterrows():\n",
    "        prompt = build_prompt(row, config)\n",
    "        pred = predict_single(model, tokenizer, prompt, config)\n",
    "        \n",
    "        predictions.append(pred)\n",
    "        true_labels.append(row['rule_violation'])\n",
    "        \n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"Processed {idx + 1}/{len(test_data)}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    report = classification_report(true_labels, predictions)\n",
    "    \n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    return accuracy, predictions, true_labels\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model-path\", required=True, help=\"Path to trained model\")\n",
    "    parser.add_argument(\"--config\", help=\"Config file path\")\n",
    "    parser.add_argument(\"--test-file\", help=\"Test data file\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load config\n",
    "    if args.config:\n",
    "        config = Config.load(args.config)\n",
    "    else:\n",
    "        config = Config()\n",
    "    \n",
    "    # Load test data\n",
    "    if args.test_file:\n",
    "        test_data = pd.read_csv(args.test_file)\n",
    "    else:\n",
    "        test_data = pd.read_csv(f\"{config.data_path}/test.csv\")\n",
    "    \n",
    "    # Add rule_violation column for test data (adjust based on your data)\n",
    "    if 'rule_violation' not in test_data.columns:\n",
    "        print(\"Warning: No rule_violation column found. Using dummy labels for demo.\")\n",
    "        test_data['rule_violation'] = 0  # or load from somewhere else\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluate_model(config, args.model_path, test_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9707f9fa-c7ff-40fe-ac0d-d822caaba3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "# Simple configuration file\n",
    "base_model_path: \"/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1\"\n",
    "lora_output_path: \"output/\"\n",
    "data_path: \"/kaggle/input/jigsaw-agile-community-rules/\"\n",
    "\n",
    "# Prompt settings\n",
    "positive_answer: \"Yes\"\n",
    "negative_answer: \"No\"\n",
    "complete_phrase: \"Answer:\"\n",
    "\n",
    "# LoRA settings\n",
    "lora_r: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.1\n",
    "target_modules: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# Training settings\n",
    "num_epochs: 3\n",
    "batch_size: 8\n",
    "gradient_accumulation_steps: 4\n",
    "learning_rate: 5e-5\n",
    "weight_decay: 0.01\n",
    "warmup_ratio: 0.05\n",
    "max_seq_length: 2048\n",
    "\n",
    "# System\n",
    "seed: 42\n",
    "output_dir: \"outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e734e84-456d-4b7b-bf4a-0bc7ffd1a0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing accelerate_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_config.yaml\n",
    "compute_environment: LOCAL_MACHINE\n",
    "debug: false\n",
    "deepspeed_config:\n",
    "  gradient_accumulation_steps: 4\n",
    "  gradient_clipping: 1.0\n",
    "  train_batch_size: 32\n",
    "  train_micro_batch_size_per_gpu: 8\n",
    "  \n",
    "  zero_stage: 2\n",
    "  offload_optimizer_device: none\n",
    "  offload_param_device: none\n",
    "  zero3_init_flag: false\n",
    "  \n",
    "  stage3_gather_16bit_weights_on_model_save: false\n",
    "  stage3_max_live_parameters: 1e8\n",
    "  stage3_max_reuse_distance: 1e8\n",
    "  stage3_prefetch_bucket_size: 5e7\n",
    "  stage3_param_persistence_threshold: 1e5\n",
    "  \n",
    "  zero_allow_untested_optimizer: true\n",
    "  zero_force_ds_cpu_optimizer: false\n",
    "  \n",
    "  fp16:\n",
    "    enabled: true\n",
    "    loss_scale: 0\n",
    "    initial_scale_power: 16\n",
    "    loss_scale_window: 1000\n",
    "    hysteresis: 2\n",
    "    min_loss_scale: 1\n",
    "  \n",
    "distributed_type: DEEPSPEED\n",
    "downcast_bf16: 'no'\n",
    "dynamo_config:\n",
    "  dynamo_backend: INDUCTOR\n",
    "  dynamo_use_fullgraph: false\n",
    "  dynamo_use_dynamic: false\n",
    "enable_cpu_affinity: false\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "mixed_precision: fp16\n",
    "num_machines: 1\n",
    "num_processes: 2\n",
    "rdzv_backend: static\n",
    "same_network: true\n",
    "tpu_env: []\n",
    "tpu_use_cluster: false\n",
    "tpu_use_sudo: false\n",
    "use_cpu: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e50d81d6-5cd9-45e2-8002-dd101044fc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_training.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_training.sh\n",
    "#!/bin/bash\n",
    "# Simple training script with distributed support\n",
    "\n",
    "# Configure accelerate (run once)\n",
    "# accelerate config --config_file accelerate_config.yaml\n",
    "\n",
    "echo \"=== Reddit Moderation Training ===\"\n",
    "\n",
    "# Single GPU training\n",
    "echo \"Single GPU training:\"\n",
    "python train.py --config config.yaml\n",
    "\n",
    "# Multi-GPU training with accelerate and DeepSpeed\n",
    "echo \"Multi-GPU training with accelerate:\"\n",
    "# accelerate launch --config_file accelerate_config.yaml train.py --config config.yaml\n",
    "\n",
    "# With custom parameters\n",
    "echo \"Training with custom parameters:\"\n",
    "# python train.py --epochs 5 --batch-size 4 --learning-rate 1e-4\n",
    "\n",
    "# Evaluation\n",
    "echo \"Evaluation:\"\n",
    "# python evaluate.py --model-path output/ --config config.yaml\n",
    "\n",
    "echo \"Training script ready!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ffab791-cb0b-4997-bfe0-488c1e37c220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup accelerate (first time only)\n",
    "# accelerate config --config_file accelerate_config.yaml\n",
    "\n",
    "# # Single GPU\n",
    "# python train.py --config config.yaml\n",
    "\n",
    "# # Multi-GPU with DeepSpeed\n",
    "# accelerate launch --config_file accelerate_config.yaml train.py --config config.yaml\n",
    "\n",
    "# # Custom parameters\n",
    "# python train.py --epochs 5 --batch-size 4 --learning-rate 1e-4\n",
    "\n",
    "# # Evaluation\n",
    "# python evaluate.py --model-path output/ --config config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935e1ef4-5a36-4c0c-b90d-59bb8c446f34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11175052,
     "sourceId": 89659,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.756759,
   "end_time": "2025-02-26T02:09:27.363350",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-26T02:09:20.606591",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
