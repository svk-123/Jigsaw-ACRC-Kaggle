{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f506feee-98f0-4106-a53a-ccbff09b3ce3",
   "metadata": {},
   "source": [
    "# Jigsaw - Agile Community Rules Classification\n",
    "### https://www.kaggle.com/competitions/jigsaw-agile-community-rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76154601-e79c-499f-b931-0b3452cc0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Speed up in progress .. not seen it yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffacf4e8-e1fa-4b50-9f70-8cc9c2f2f966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.py\n",
    "\n",
    "RESUME_TRAINING=False\n",
    "LOCAL_MODEL_PATH = \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\"\n",
    "LORA_IN_PATH= \"./\"\n",
    "LORA_OUT_PATH = \"./\"\n",
    "DATA_PATH = \"./\"\n",
    "OUTPUT_PATH=\"./\"\n",
    "\n",
    "# Training parameters\n",
    "MAX_SEQ_LENGTH = 3600\n",
    "RANK = 32\n",
    "LORA_ALPHA=64\n",
    "MAX_ITER_STEPS = 20\n",
    "EPOCHS = -1\n",
    "SAMPLE_LEN=\"24k\"\n",
    "\n",
    "# Kaggle upload configuration\n",
    "MODEL_SLUG = \"qwen25-7b-gptq-int4-jigsaw-acrc-lora-cml-0==\"\n",
    "VARIATION_SLUG = \"01\"\n",
    "\n",
    "###--------------------------------###\n",
    "DATASET_ID=\"1to9\"\n",
    "BASE_MODEL=LOCAL_MODEL_PATH.split(\"/\")[-1].replace(\".\", \"p\")\n",
    "TRAIN_DIR=f\"{BASE_MODEL}_lora_fp16_r{RANK}_s{SAMPLE_LEN}_e_{EPOCHS}_msl{MAX_SEQ_LENGTH}-{DATASET_ID}\"\n",
    "print(\"TRAIN_DIR\",TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "016b3fc1-46cb-4dd4-b7ba-f0140e144fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting get_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile get_dataset.py\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import kagglehub\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load Jigsaw ACRC dataset from Kaggle or local files\"\"\"\n",
    "    # Check if running on Kaggle\n",
    "    if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "        # Running on Kaggle\n",
    "        base_path = \"/kaggle/input/jigsaw-agile-community-rules/\"\n",
    "        df_train = pd.read_csv(f\"{base_path}*train*.csv\")\n",
    "        df_test = pd.read_csv(f\"{base_path}*test*.csv\")\n",
    "    else:\n",
    "        # Running locally\n",
    "        base_path = \"./\"\n",
    "        \n",
    "        # Find all train files\n",
    "        train_files = glob.glob(f\"{base_path}*train*.csv\")\n",
    "        if train_files:\n",
    "            train_dfs = [pd.read_csv(file) for file in train_files]\n",
    "            df_train = pd.concat(train_dfs, ignore_index=True)\n",
    "            print(f\"Concatenated {len(train_files)} train files: {train_files}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No train files found in {base_path}\")\n",
    "        \n",
    "        # Find all test files\n",
    "        test_files = glob.glob(f\"{base_path}*test*.csv\")\n",
    "        if test_files:\n",
    "            test_dfs = [pd.read_csv(file) for file in test_files]\n",
    "            df_test = pd.concat(test_dfs, ignore_index=True)\n",
    "            print(f\"Concatenated {len(test_files)} test files: {test_files}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No test files found in {base_path}\")\n",
    "\n",
    "    print(f\"Train shape: {df_train.shape}\")\n",
    "    print(f\"Test shape: {df_test.shape}\")\n",
    "    print(df_train.columns)\n",
    "            \n",
    "    req_cols=['subreddit', 'rule', 'positive_example_1', 'negative_example_1', 'positive_example_2',\n",
    "           'negative_example_2', 'test_comment', 'violates_rule']\n",
    "\n",
    "    df_train=df_train[req_cols]\n",
    "    df_test=df_test[req_cols]\n",
    "\n",
    "    # Normalize \"True\"/\"False\" -> \"Yes\"/\"No\" and drop anything else\n",
    "    for name, df in [(\"train\", df_train), (\"test\", df_test)]:\n",
    "        df[\"violates_rule\"] = (\n",
    "            df[\"violates_rule\"]\n",
    "            .astype(str).str.strip()\n",
    "            .map({\"True\": \"Yes\", \"False\": \"No\", \"Yes\": \"Yes\", \"No\": \"No\"})  # normalize\n",
    "        )\n",
    "        before = len(df)\n",
    "        df.dropna(subset=[\"violates_rule\"], inplace=True)  # drop rows with NaN (anything not Yes/No/True/False)\n",
    "        after = len(df)\n",
    "        print(f\"Dropped {before - after} rows from {name} due to invalid 'violates_rule'\")\n",
    "    \n",
    "    for col in req_cols:\n",
    "        dropped_rows = df_train[df_train[col].isna()].shape[0]\n",
    "        print(f\"{col}: {dropped_rows} rows would be dropped\")\n",
    "        \n",
    "    df_train = df_train[req_cols].dropna()\n",
    "    df_test = df_test[req_cols].dropna()\n",
    "\n",
    "    print(f\"Using path: {base_path}\")\n",
    "    print(\"\\n After dropping:\")\n",
    "    print(f\"Train shape: {df_train.shape}\")\n",
    "    print(f\"Test shape: {df_test.shape}\")\n",
    "\n",
    "    df_train[\"violates_rule\"] = df_train[\"violates_rule\"].astype(str)\n",
    "    df_test[\"violates_rule\"] = df_test[\"violates_rule\"].astype(str)\n",
    "\n",
    "    valid_values = {\"Yes\", \"No\"}\n",
    "    df_train = df_train[df_train[\"violates_rule\"].isin(valid_values)]\n",
    "    df_test  = df_test[df_test[\"violates_rule\"].isin(valid_values)]\n",
    "    print(\"\\n After checking Yes/No:\")\n",
    "    print(f\"Train shape: {df_train.shape}\")\n",
    "    print(f\"Test shape: {df_test.shape}\")\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def formatting_prompts_func(examples, tokenizer):\n",
    "    \"\"\"\n",
    "    Format Reddit moderation dataset using tokenizer chat template\n",
    "    \"\"\"\n",
    "\n",
    "    texts = []\n",
    "\n",
    "    for i in range(len(examples['subreddit'])):\n",
    "        # Create system message\n",
    "        system_msg = f\"You are a really experienced moderator for the subreddit /r/{examples['subreddit'][i]}. Your job is to determine if the following reported comment violates the given rule. Answer with only \\\"Yes\\\" or \\\"No\\\".\"\n",
    "\n",
    "        # Create user message with the rule and examples\n",
    "        user_msg = f\"\"\"Rule: {examples['rule'][i]}\n",
    "Example 1:\n",
    "{examples['positive_example_1'][i]}\n",
    "Rule violation: Yes\n",
    "Example 2:\n",
    "{examples['negative_example_1'][i]}\n",
    "Rule violation: No\n",
    "Example 3:\n",
    "{examples['positive_example_2'][i]}\n",
    "Rule violation: Yes\n",
    "Example 4:\n",
    "{examples['negative_example_2'][i]}\n",
    "Rule violation: No\n",
    "Test sentence:\n",
    "{examples['test_comment'][i]}\"\"\"\n",
    "\n",
    "        # Assistant response is \"Yes\" or \"No\"\n",
    "        assistant_msg = examples['violates_rule'][i]\n",
    "\n",
    "        # Create messages list for chat template\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_msg}\n",
    "        ]\n",
    "\n",
    "\n",
    "        formatted_text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "\n",
    "        texts.append(formatted_text)\n",
    "\n",
    "    return {\"text\": texts}\n",
    "\n",
    "def build_dataset(tokenizer):\n",
    "    \"\"\"\n",
    "    Build both train and test datasets using tokenizer chat template\n",
    "    \"\"\"\n",
    "    df_train, df_test = load_data()\n",
    "    \n",
    "    train_dataset = Dataset.from_pandas(df_train)\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda examples: formatting_prompts_func(examples, tokenizer), \n",
    "        batched=True\n",
    "    )\n",
    "    \n",
    "    test_dataset = Dataset.from_pandas(df_test)\n",
    "    test_dataset = test_dataset.map(\n",
    "        lambda examples: formatting_prompts_func(examples, tokenizer), \n",
    "        batched=True\n",
    "    )\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd4ee819-dabb-406f-b404-17cd4b3bf0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers.utils import is_torch_bf16_gpu_available\n",
    "from get_dataset import build_dataset\n",
    "from config import LOCAL_MODEL_PATH, LORA_IN_PATH, LORA_OUT_PATH, RANK, MAX_SEQ_LENGTH, EPOCHS, TRAIN_DIR, MAX_ITER_STEPS, OUTPUT_PATH, RESUME_TRAINING, LORA_ALPHA\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Minimal environment optimizations that definitely help\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" \n",
    "os.environ[\"TORCH_CUDNN_ALLOW_TF32\"] = \"1\"  # This one is almost always beneficial\n",
    "\n",
    "# Check for Flash Attention 2 (this is the biggest potential win)\n",
    "try:\n",
    "    import flash_attn\n",
    "    use_flash_attn = True\n",
    "    print(\"Flash Attention 2 available - will enable\")\n",
    "except ImportError:\n",
    "    use_flash_attn = False\n",
    "    print(\"Flash Attention 2 not available\")\n",
    "\n",
    "# ----------------------------\n",
    "# Load model & tokenizer  \n",
    "# ----------------------------\n",
    "print(\"Loading model and tokenizer...\")\n",
    "\n",
    "# Only add optimizations that are proven to help\n",
    "model_kwargs = {\n",
    "    \"device_map\": \"auto\", \n",
    "    \"torch_dtype\": torch.bfloat16,  # Keep this - often better than fp16\n",
    "    \"low_cpu_mem_usage\": True,\n",
    "    \"trust_remote_code\": True,\n",
    "    \"use_cache\": False,  # This saves memory without hurting speed\n",
    "}\n",
    "\n",
    "# Only add Flash Attention if available (this can be 2-4x speedup)\n",
    "if use_flash_attn:\n",
    "    model_kwargs[\"attn_implementation\"] = \"flash_attention_2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(LOCAL_MODEL_PATH, **model_kwargs)\n",
    "\n",
    "if RESUME_TRAINING: \n",
    "    model = PeftModel.from_pretrained(model, LORA_IN_PATH, is_trainable=True)\n",
    "\n",
    "# Use fast tokenizer (minimal overhead, good benefit)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    LOCAL_MODEL_PATH,\n",
    "    use_fast=True,\n",
    "    padding_side=\"right\"\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Keep your original gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# ----------------------------\n",
    "# Build datasets\n",
    "# ----------------------------\n",
    "print(\"Building datasets...\")\n",
    "train_dataset, test_dataset = build_dataset(tokenizer)\n",
    "\n",
    "# Basic memory cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# ----------------------------\n",
    "# LoRA config (exactly as you had)\n",
    "# ----------------------------\n",
    "if RESUME_TRAINING:    \n",
    "    lora_config = None       \n",
    "else:    \n",
    "    lora_config = LoraConfig(\n",
    "        r=RANK,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=0.0,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "        ],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    \n",
    "# ----------------------------\n",
    "# SFT config (EXACTLY your original settings)\n",
    "# ----------------------------\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=OUTPUT_PATH,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    max_steps=MAX_ITER_STEPS,\n",
    "    per_device_train_batch_size=2,       \n",
    "    gradient_accumulation_steps=4,       \n",
    "    max_length=min(MAX_SEQ_LENGTH, 4096),  \n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.05,\n",
    "    \n",
    "    # Keep your original precision settings\n",
    "    bf16=is_torch_bf16_gpu_available(),\n",
    "    fp16=not is_torch_bf16_gpu_available(),\n",
    "    dataloader_pin_memory=True,\n",
    "    \n",
    "    # Your exact logging/saving settings\n",
    "    warmup_steps=5,\n",
    "    logging_steps=10,\n",
    "    eval_steps=3000,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",        \n",
    "    packing=False,\n",
    "    remove_unused_columns=False,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Trainer (keep it simple)\n",
    "# ----------------------------\n",
    "print(\"Initializing trainer...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=lora_config,\n",
    "    args=sft_config,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(os.path.join(LORA_OUT_PATH, TRAIN_DIR))\n",
    "torch.cuda.empty_cache()\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1454531c-7a2d-47d9-aa14-43d75573c26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerate_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_config.yaml\n",
    "compute_environment: LOCAL_MACHINE\n",
    "debug: false\n",
    "deepspeed_config:\n",
    "  # Your exact settings that were working\n",
    "  gradient_accumulation_steps: 4\n",
    "  gradient_clipping: 1.0\n",
    "  train_batch_size: 8\n",
    "  train_micro_batch_size_per_gpu: 2\n",
    "  \n",
    "  # Keep ZeRO Stage 2 (it was working well)\n",
    "  zero_stage: 2\n",
    "  offload_optimizer_device: none\n",
    "  offload_param_device: none\n",
    "  zero3_init_flag: false\n",
    "  \n",
    "  # Conservative memory settings (don't over-optimize)\n",
    "  stage3_gather_16bit_weights_on_model_save: false\n",
    "  stage3_max_live_parameters: 1e8\n",
    "  stage3_max_reuse_distance: 1e8\n",
    "  stage3_prefetch_bucket_size: 5e7\n",
    "  stage3_param_persistence_threshold: 1e5\n",
    "  \n",
    "  zero_allow_untested_optimizer: true\n",
    "  zero_force_ds_cpu_optimizer: false\n",
    "  \n",
    "  # Only enable proven optimizations\n",
    "  contiguous_gradients: true\n",
    "  \n",
    "  # Your working precision settings\n",
    "  fp16:\n",
    "    enabled: true\n",
    "    loss_scale: 0\n",
    "    initial_scale_power: 16\n",
    "    loss_scale_window: 1000\n",
    "    hysteresis: 2\n",
    "    min_loss_scale: 1\n",
    "  \n",
    "distributed_type: DEEPSPEED\n",
    "downcast_bf16: 'no'\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "mixed_precision: fp16\n",
    "num_machines: 1\n",
    "num_processes: 1\n",
    "use_cpu: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38f44949-3161-4bfb-87e6-54f1c6698184",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 20 iter in 1.17-1.20 with 3.9s/iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94d9a542-7f3d-4a7e-95d0-d3a2c950249a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-19 18:25:49,106] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-09-19 18:25:50,715] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "TRAIN_DIR Qwen2p5-7B-Instruct-GPTQ-Int4_lora_fp16_r32_s24k_e_-1_msl3600-1to9\n",
      "âœ“ Flash Attention 2 available - will enable\n",
      "Loading model and tokenizer...\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "/home/vino/anaconda3/envs/kaggle/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/vino/anaconda3/envs/kaggle/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/vino/anaconda3/envs/kaggle/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.60it/s]\n",
      "Building datasets...\n",
      "Concatenated 1 train files: ['./df_train.csv']\n",
      "Concatenated 1 test files: ['./df_test.csv']\n",
      "Train shape: (2029, 11)\n",
      "Test shape: (2029, 11)\n",
      "Index(['row_id', 'body', 'rule', 'subreddit', 'positive_example_1',\n",
      "       'positive_example_2', 'negative_example_1', 'negative_example_2',\n",
      "       'rule_violation', 'violates_rule', 'test_comment'],\n",
      "      dtype='object')\n",
      "Dropped 0 rows from train due to invalid 'violates_rule'\n",
      "Dropped 0 rows from test due to invalid 'violates_rule'\n",
      "subreddit: 0 rows would be dropped\n",
      "rule: 0 rows would be dropped\n",
      "positive_example_1: 0 rows would be dropped\n",
      "negative_example_1: 0 rows would be dropped\n",
      "positive_example_2: 0 rows would be dropped\n",
      "negative_example_2: 0 rows would be dropped\n",
      "test_comment: 0 rows would be dropped\n",
      "violates_rule: 0 rows would be dropped\n",
      "Using path: ./\n",
      "\n",
      " After dropping:\n",
      "Train shape: (2029, 8)\n",
      "Test shape: (2029, 8)\n",
      "\n",
      " After checking Yes/No:\n",
      "Train shape: (2029, 8)\n",
      "Test shape: (2029, 8)\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2029/2029 [00:00<00:00, 18503.46 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2029/2029 [00:00<00:00, 20483.02 examples/s]\n",
      "[2025-09-19 18:25:58,837] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-09-19 18:26:00,455] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "[2025-09-19 18:26:00,464] [INFO] [comm.py:821:init_distributed] cdb=None\n",
      "[2025-09-19 18:26:00,464] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "Initializing trainer...\n",
      "Adding EOS to train dataset: 100%|â–ˆ| 2029/2029 [00:00<00:00, 12450.25 examples/s\n",
      "Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 2029/2029 [00:01<00:00, 1417.82 examples/s]\n",
      "Truncating train dataset: 100%|â–ˆâ–ˆ| 2029/2029 [00:00<00:00, 328492.02 examples/s]\n",
      "Adding EOS to eval dataset: 100%|â–ˆ| 2029/2029 [00:00<00:00, 12546.39 examples/s]\n",
      "Tokenizing eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2029/2029 [00:01<00:00, 1411.18 examples/s]\n",
      "Truncating eval dataset: 100%|â–ˆâ–ˆâ–ˆ| 2029/2029 [00:00<00:00, 444120.80 examples/s]\n",
      "ðŸš€ Starting training...\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "[2025-09-19 18:26:04,950] [WARNING] [engine.py:1373:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n",
      "{'loss': 2.905, 'grad_norm': 0.5951701998710632, 'learning_rate': 0.00036666666666666667, 'entropy': 2.3989210397005083, 'num_tokens': 27660.0, 'mean_token_accuracy': 0.486764944344759, 'epoch': 0.04}\n",
      "{'loss': 2.1386, 'grad_norm': 0.5112891793251038, 'learning_rate': 3.3333333333333335e-05, 'entropy': 2.1580403238534926, 'num_tokens': 55763.0, 'mean_token_accuracy': 0.5873469322919845, 'epoch': 0.08}\n",
      "{'train_runtime': 78.8451, 'train_samples_per_second': 2.029, 'train_steps_per_second': 0.254, 'train_loss': 2.5218217849731444, 'epoch': 0.08}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [01:18<00:00,  3.94s/it]\n",
      "success\n",
      "[rank0]:[W919 18:27:29.598604962 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch --config_file accelerate_config.yaml train.py"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11175052,
     "sourceId": 89659,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.756759,
   "end_time": "2025-02-26T02:09:27.363350",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-26T02:09:20.606591",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
